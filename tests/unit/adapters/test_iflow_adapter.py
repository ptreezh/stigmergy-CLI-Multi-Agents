"""
iFlow CLI Workflow Pipelineé€‚é…å™¨å•å…ƒæµ‹è¯•

éµå¾ªæµ‹è¯•é©±åŠ¨å¼€å‘(TDD)åŸåˆ™ï¼Œå…ˆåˆ›å»ºæµ‹è¯•ç”¨ä¾‹ï¼Œå†å®ç°å…·ä½“åŠŸèƒ½ã€‚
æµ‹è¯•iFlow CLIçš„Workflow Pipelineé›†æˆæœºåˆ¶ã€‚
"""

import pytest
import asyncio
import json
import datetime
from unittest.mock import Mock, AsyncMock, patch, MagicMock
from typing import Dict, Any, Optional, List
from pathlib import Path

# å¯¼å…¥è¢«æµ‹è¯•çš„æ¨¡å—
from src.core.base_adapter import BaseCrossCLIAdapter
from src.core.parser import NaturalLanguageParser, IntentResult


class MockIFlowPipelineContext:
    """æ¨¡æ‹ŸiFlow CLI Pipelineä¸Šä¸‹æ–‡"""
    def __init__(self, workflow_id: str = "", stage: str = "", data: Dict = None):
        self.workflow_id = workflow_id or "test-workflow"
        self.stage = stage or "input"
        self.data = data or {}
        self.metadata = {
            'user_id': 'test_user',
            'session_id': 'test_session',
            'pipeline_config': {}
        }
        self.pipeline_name = "cross-cli-integration"
        self.version = "1.0.0"


class TestIFlowWorkflowAdapterTDD:
    """iFlow CLI Workflow Pipelineé€‚é…å™¨TDDæµ‹è¯• - éµå¾ªæµ‹è¯•å…ˆè¡Œçš„åŸåˆ™"""

    @pytest.fixture
    def mock_adapter_class(self):
        """Mocké€‚é…å™¨ç±»ç”¨äºTDD"""
        class IFlowWorkflowAdapter(BaseCrossCLIAdapter):
            def __init__(self, cli_name: str):
                super().__init__(cli_name)
                self.pipeline_stages = []
                self.processed_workflows = []
                self.workflow_executions = []
                self.pipeline_hooks = {}
                self.stages_processed = 0
                self.cross_cli_calls_count = 0

            async def initialize(self) -> bool:
                """åˆå§‹åŒ–Workflow Pipelineç³»ç»Ÿ"""
                try:
                    # 1. åŠ è½½Pipelineé…ç½®
                    await self._load_pipeline_config()

                    # 2. æ³¨å†ŒPipelineå¤„ç†å™¨
                    await self._register_pipeline_stages()

                    # 3. è®¾ç½®Workflow Hooks
                    await self._setup_workflow_hooks()

                    # 4. åˆå§‹åŒ–ä»»åŠ¡é˜Ÿåˆ—
                    await self._initialize_task_queue()

                    return True
                except Exception as e:
                    print(f"åˆå§‹åŒ–å¤±è´¥: {e}")
                    return False

            async def _load_pipeline_config(self) -> bool:
                """åŠ è½½Pipelineé…ç½®"""
                config_path = Path("src/adapters/iflow/config.json")
                if config_path.exists():
                    with open(config_path, 'r', encoding='utf-8') as f:
                        self.pipeline_config = json.load(f)
                    return True
                return False

            async def _register_pipeline_stages(self) -> bool:
                """æ³¨å†ŒPipelineé˜¶æ®µå¤„ç†å™¨"""
                self.pipeline_stages = [
                    'input_validation',
                    'cross_cli_detection',
                    'target_execution',
                    'result_processing',
                    'output_formatting'
                ]
                return True

            async def _setup_workflow_hooks(self) -> bool:
                """è®¾ç½®Workflow Hooks"""
                self.pipeline_hooks = {
                    'on_workflow_start': self.on_workflow_start,
                    'on_stage_complete': self.on_stage_complete,
                    'on_workflow_success': self.on_workflow_success,
                    'on_workflow_error': self.on_workflow_error,
                    'on_pipeline_ready': self.on_pipeline_ready
                }
                return True

            async def _initialize_task_queue(self) -> bool:
                """åˆå§‹åŒ–ä»»åŠ¡é˜Ÿåˆ—"""
                self.task_queue = asyncio.Queue()
                return True

            def is_available(self) -> bool:
                """æ£€æŸ¥Pipelineç³»ç»Ÿæ˜¯å¦å¯ç”¨"""
                return len(self.pipeline_stages) > 0

            async def execute_task(self, task: str, context: Dict[str, Any]) -> str:
                """æ‰§è¡Œå·¥ä½œæµä»»åŠ¡"""
                return f"iFlowå¤„ç†ç»“æœ: {task}"

            # Pipeline Hookå¤„ç†å™¨
            async def on_workflow_start(self, context: MockIFlowPipelineContext) -> Optional[str]:
                """å·¥ä½œæµå¼€å§‹Hook"""
                try:
                    self.stages_processed += 1
                    workflow_data = {
                        'workflow_id': context.workflow_id,
                        'stage': context.stage,
                        'data': context.data,
                        'metadata': context.metadata
                    }
                    self.processed_workflows.append(workflow_data)

                    # æ£€æµ‹è·¨CLIè°ƒç”¨æ„å›¾
                    if self._detect_cross_cli_intent(context):
                        target_cli, task = self._parse_cross_cli_task(context)
                        if target_cli and target_cli != 'iflow':
                            result = await self._execute_cross_cli_workflow(target_cli, task, context)
                            return result

                    return None  # ç»§ç»­æ­£å¸¸Pipelineæµç¨‹

                except Exception as e:
                    print(f"å·¥ä½œæµå¼€å§‹Hooké”™è¯¯: {e}")
                    return None

            async def on_stage_complete(self, context: MockIFlowPipelineContext, stage_result: Any) -> Optional[str]:
                """é˜¶æ®µå®ŒæˆHook"""
                self.stages_processed += 1
                return None

            async def on_workflow_success(self, context: MockIFlowPipelineContext, final_result: Any) -> Optional[str]:
                """å·¥ä½œæµæˆåŠŸHook"""
                self.processed_workflows.append({
                    'type': 'workflow_success',
                    'workflow_id': context.workflow_id,
                    'result': final_result
                })
                return None

            async def on_workflow_error(self, context: MockIFlowPipelineContext, error: Exception) -> Optional[str]:
                """å·¥ä½œæµé”™è¯¯Hook"""
                self.processed_workflows.append({
                    'type': 'workflow_error',
                    'workflow_id': context.workflow_id,
                    'error': str(error)
                })
                return None

            async def on_pipeline_ready(self, pipeline_config: Dict) -> Optional[str]:
                """Pipelineå°±ç»ªHook"""
                return None

            # è·¨CLIåŠŸèƒ½
            def _detect_cross_cli_intent(self, context: MockIFlowPipelineContext) -> bool:
                """æ£€æµ‹è·¨CLIè°ƒç”¨æ„å›¾"""
                import re
                user_input = context.data.get('prompt', '')
                patterns = [
                    r'è°ƒç”¨\s*(\w+)\s*æ¥',
                    r'ä½¿ç”¨\s*(\w+)\s*æ‰§è¡Œ',
                    r'è®©\s*(\w+)\s*å¸®æˆ‘',
                    r'use\s+(\w+)\s+to',
                    r'call\s+(\w+)\s+to',
                    r'ask\s+(\w+)\s+for'
                ]

                for pattern in patterns:
                    if re.search(pattern, user_input, re.IGNORECASE):
                        return True
                return False

            def _parse_cross_cli_task(self, context: MockIFlowPipelineContext) -> tuple:
                """è§£æè·¨CLIä»»åŠ¡"""
                user_input = context.data.get('prompt', '')

                # ä½¿ç”¨å·²æœ‰çš„è§£æå™¨
                parser = NaturalLanguageParser()
                intent = parser.parse_intent(user_input, "iflow")

                if intent.is_cross_cli:
                    return intent.target_cli, intent.task

                return None, None

            async def _execute_cross_cli_workflow(self, target_cli: str, task: str, context: MockIFlowPipelineContext) -> str:
                """æ‰§è¡Œè·¨CLIå·¥ä½œæµ"""
                # è®°å½•è·¨CLIè°ƒç”¨
                self.cross_cli_calls_count += 1
                workflow_execution = {
                    'workflow_id': context.workflow_id,
                    'target_cli': target_cli,
                    'task': task,
                    'timestamp': datetime.datetime.now().isoformat()
                }
                self.workflow_executions.append(workflow_execution)

                # æ¨¡æ‹Ÿç›®æ ‡CLIè°ƒç”¨
                mock_result = await self._mock_target_cli_workflow(target_cli, task, context)
                return self._format_workflow_result(target_cli, mock_result, context)

            async def _mock_target_cli_workflow(self, target_cli: str, task: str, context: MockIFlowPipelineContext) -> str:
                """æ¨¡æ‹Ÿç›®æ ‡CLIå·¥ä½œæµæ‰§è¡Œ"""
                workflow_results = {
                    'claude': f"Claudeå·¥ä½œæµç»“æœ: æˆåŠŸæ‰§è¡Œ '{task}' çš„æ™ºèƒ½åˆ†æ",
                    'gemini': f"Geminiå·¥ä½œæµç»“æœ: å®Œæˆäº† '{task}' çš„AIå¤„ç†æµç¨‹",
                    'qwencode': f"QwenCodeå·¥ä½œæµç»“æœ: ä»£ç ç”Ÿæˆ '{task}' å·²å®Œæˆ",
                    'qoder': f"Qoderå·¥ä½œæµç»“æœ: å¼€å‘ä»»åŠ¡ '{task}' æ‰§è¡Œå®Œæ¯•",
                    'codebuddy': f"CodeBuddyå·¥ä½œæµç»“æœ: ç¼–ç¨‹è¾…åŠ© '{task}' å®Œæˆ",
                    'codex': f"Codexå·¥ä½œæµç»“æœ: ä»£ç æ‰§è¡Œ '{task}' ç»“æŸ"
                }
                return workflow_results.get(target_cli, f"{target_cli}å·¥ä½œæµç»“æœ: {task}")

            def _format_workflow_result(self, target_cli: str, result: str, context: MockIFlowPipelineContext) -> str:
                """æ ¼å¼åŒ–å·¥ä½œæµç»“æœ"""
                return f"""## ğŸ”„ è·¨CLIå·¥ä½œæµç»“æœ

**æºå·¥ä½œæµ**: iFlow Pipeline ({context.workflow_id})
**ç›®æ ‡CLI**: {target_cli.upper()}
**å·¥ä½œæµé˜¶æ®µ**: {context.stage}
**æ‰§è¡Œæ—¶é—´**: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

{result}

---

*æ­¤ç»“æœç”±iFlowè·¨CLIé›†æˆç³»ç»Ÿé€šè¿‡Workflow Pipelineæä¾›*"""

        return IFlowWorkflowAdapter

    @pytest.fixture
    def adapter(self, mock_adapter_class):
        """åˆ›å»ºé€‚é…å™¨å®ä¾‹"""
        return mock_adapter_class("iflow")

    @pytest.fixture
    def mock_context(self):
        """åˆ›å»ºæ¨¡æ‹ŸPipelineä¸Šä¸‹æ–‡"""
        return MockIFlowPipelineContext(
            workflow_id="test-workflow-001",
            stage="input_validation",
            data={"prompt": "è¯·ç”¨claudeå¸®æˆ‘åˆ†æè¿™ä¸ªç®—æ³•"}
        )

    # ==================== TDDæµ‹è¯•ç”¨ä¾‹ ====================

    @pytest.mark.unit
    async def test_adapter_initialization(self, adapter):
        """æµ‹è¯•é€‚é…å™¨åˆå§‹åŒ–"""
        result = await adapter.initialize()

        assert result is True, "é€‚é…å™¨åˆå§‹åŒ–åº”è¯¥æˆåŠŸ"
        assert adapter.is_available() is True, "åˆå§‹åŒ–åé€‚é…å™¨åº”è¯¥å¯ç”¨"
        assert len(adapter.pipeline_stages) > 0, "åº”è¯¥åŠ è½½Pipelineé˜¶æ®µ"
        assert len(adapter.pipeline_hooks) > 0, "åº”è¯¥æ³¨å†ŒPipeline Hooks"

    @pytest.mark.unit
    async def test_pipeline_configuration_loading(self, adapter):
        """æµ‹è¯•Pipelineé…ç½®åŠ è½½"""
        await adapter.initialize()

        assert hasattr(adapter, 'pipeline_config'), "åº”è¯¥åŠ è½½Pipelineé…ç½®"

        # éªŒè¯é…ç½®ç»“æ„
        if hasattr(adapter, 'pipeline_config'):
            config = adapter.pipeline_config
            assert 'adapter_name' in config, "é…ç½®åº”åŒ…å«é€‚é…å™¨åç§°"
            assert 'pipeline_mechanism' in config, "é…ç½®åº”åŒ…å«Pipelineæœºåˆ¶"

    @pytest.mark.unit
    async def test_pipeline_stages_registration(self, adapter):
        """æµ‹è¯•Pipelineé˜¶æ®µæ³¨å†Œ"""
        await adapter.initialize()

        expected_stages = [
            'input_validation',
            'cross_cli_detection',
            'target_execution',
            'result_processing',
            'output_formatting'
        ]

        assert adapter.pipeline_stages == expected_stages, "åº”è¯¥æ³¨å†Œæ­£ç¡®çš„Pipelineé˜¶æ®µ"

    @pytest.mark.unit
    async def test_workflow_hooks_setup(self, adapter):
        """æµ‹è¯•Workflow Hooksè®¾ç½®"""
        await adapter.initialize()

        expected_hooks = [
            'on_workflow_start',
            'on_stage_complete',
            'on_workflow_success',
            'on_workflow_error',
            'on_pipeline_ready'
        ]

        assert set(adapter.pipeline_hooks.keys()) == set(expected_hooks), "åº”è¯¥è®¾ç½®æ­£ç¡®çš„Workflow Hooks"

    @pytest.mark.unit
    async def test_cross_cli_detection_in_workflow(self, adapter, mock_context):
        """æµ‹è¯•å·¥ä½œæµä¸­çš„è·¨CLIæ£€æµ‹"""
        await adapter.initialize()

        # æµ‹è¯•è·¨CLIè°ƒç”¨æ£€æµ‹
        mock_context.data = {"prompt": "è¯·ç”¨claudeå¸®æˆ‘åˆ†ææ•°æ®"}
        is_cross_cli = adapter._detect_cross_cli_intent(mock_context)

        assert is_cross_cli is True, "åº”è¯¥æ£€æµ‹åˆ°è·¨CLIè°ƒç”¨æ„å›¾"

    @pytest.mark.unit
    async def test_workflow_stage_processing_with_cross_cli(self, adapter, mock_context):
        """æµ‹è¯•åŒ…å«è·¨CLIçš„å·¥ä½œæµé˜¶æ®µå¤„ç†"""
        await adapter.initialize()

        mock_context.data = {"prompt": "ä½¿ç”¨geminiå¤„ç†è¿™ä¸ªæ–‡æœ¬"}
        result = await adapter.on_workflow_start(mock_context)

        assert result is not None, "è·¨CLIè°ƒç”¨åº”è¯¥è¿”å›ç»“æœ"
        assert "gemini" in result.lower(), "ç»“æœåº”è¯¥åŒ…å«ç›®æ ‡CLIåç§°"
        assert "å·¥ä½œæµç»“æœ" in result, "ç»“æœåº”è¯¥æ˜¯å·¥ä½œæµæ ¼å¼"

    @pytest.mark.unit
    async def test_workflow_stage_processing_normal_task(self, adapter, mock_context):
        """æµ‹è¯•æ™®é€šä»»åŠ¡çš„å·¥ä½œæµé˜¶æ®µå¤„ç†"""
        await adapter.initialize()

        mock_context.data = {"prompt": "æ­£å¸¸çš„æ•°æ®å¤„ç†ä»»åŠ¡"}
        result = await adapter.on_workflow_start(mock_context)

        assert result is None, "æ™®é€šä»»åŠ¡åº”è¯¥è¿”å›Noneï¼Œç»§ç»­æ­£å¸¸æµç¨‹"
        assert adapter.stages_processed > 0, "åº”è¯¥å¤„ç†äº†å·¥ä½œæµé˜¶æ®µ"

    @pytest.mark.unit
    async def test_workflow_self_reference_handling(self, adapter, mock_context):
        """æµ‹è¯•å·¥ä½œæµè‡ªæˆ‘å¼•ç”¨å¤„ç†"""
        await adapter.initialize()

        mock_context.data = {"prompt": "ä½¿ç”¨iflowå¤„ç†è¿™ä¸ªå·¥ä½œæµ"}
        target_cli, task = adapter._parse_cross_cli_task(mock_context)

        # åº”è¯¥ä¸è§¦å‘è·¨CLIè°ƒç”¨ï¼ˆé¿å…è‡ªæˆ‘å¼•ç”¨ï¼‰
        result = await adapter.on_workflow_start(mock_context)
        assert result is None, "è‡ªæˆ‘å¼•ç”¨åº”è¯¥è¿”å›None"

    @pytest.mark.unit
    async def test_multiple_target_cli_support(self, adapter, mock_context):
        """æµ‹è¯•å¤šç›®æ ‡CLIæ”¯æŒ"""
        await adapter.initialize()

        supported_clis = ['claude', 'gemini', 'qwencode', 'qoder', 'codebuddy', 'codex']

        for cli in supported_clis:
            mock_context.data = {"prompt": f"è¯·ç”¨{cli}å¸®æˆ‘å¤„ç†ä»»åŠ¡"}
            result = await adapter.on_workflow_start(mock_context)

            assert result is not None, f"åº”è¯¥æ”¯æŒ{cli}çš„è·¨CLIè°ƒç”¨"
            assert cli.upper() in result, f"ç»“æœåº”è¯¥åŒ…å«{cli}"

    @pytest.mark.unit
    async def test_pipeline_hooks_configuration(self, adapter):
        """æµ‹è¯•Pipeline Hooksé…ç½®"""
        await adapter.initialize()

        # éªŒè¯æ¯ä¸ªHookéƒ½æ˜¯å¯è°ƒç”¨çš„
        for hook_name, hook_func in adapter.pipeline_hooks.items():
            assert callable(hook_func), f"Hook {hook_name} åº”è¯¥æ˜¯å¯è°ƒç”¨çš„"

    @pytest.mark.unit
    async def test_workflow_result_formatting_consistency(self, adapter, mock_context):
        """æµ‹è¯•å·¥ä½œæµç»“æœæ ¼å¼åŒ–ä¸€è‡´æ€§"""
        await adapter.initialize()

        mock_context.data = {"prompt": "è¯·ç”¨claudeåˆ†æè¿™ä¸ªç®—æ³•"}
        result = await adapter.on_workflow_start(mock_context)

        # éªŒè¯æ ¼å¼åŒ–ç»“æ„
        required_elements = [
            "ğŸ”„ è·¨CLIå·¥ä½œæµç»“æœ",
            "æºå·¥ä½œæµ**: iFlow Pipeline",
            "ç›®æ ‡CLI**: CLAUDE",
            "å·¥ä½œæµé˜¶æ®µ",
            "æ‰§è¡Œæ—¶é—´",
            "Claudeå·¥ä½œæµç»“æœ",
            "é€šè¿‡Workflow Pipelineæä¾›"
        ]

        for element in required_elements:
            assert element in result, f"å·¥ä½œæµç»“æœæ ¼å¼åº”è¯¥åŒ…å«: {element}"

    @pytest.mark.unit
    async def test_workflow_error_handling(self, adapter, mock_context):
        """æµ‹è¯•å·¥ä½œæµé”™è¯¯å¤„ç†"""
        await adapter.initialize()

        # æ¨¡æ‹Ÿå·¥ä½œæµé”™è¯¯
        test_error = Exception("å·¥ä½œæµæ‰§è¡Œé”™è¯¯")
        result = await adapter.on_workflow_error(mock_context, test_error)

        assert result is None, "é”™è¯¯å¤„ç†åº”è¯¥è¿”å›None"

        # éªŒè¯é”™è¯¯è¢«è®°å½•
        error_records = [w for w in adapter.processed_workflows if w.get('type') == 'workflow_error']
        assert len(error_records) > 0, "åº”è¯¥è®°å½•å·¥ä½œæµé”™è¯¯"

    @pytest.mark.unit
    async def test_concurrent_workflow_processing(self, adapter, mock_context):
        """æµ‹è¯•å¹¶å‘å·¥ä½œæµå¤„ç†"""
        await adapter.initialize()

        # åˆ›å»ºå¤šä¸ªå¹¶å‘å·¥ä½œæµ
        workflows = []
        for i in range(3):
            workflow_context = MockIFlowPipelineContext(
                workflow_id=f"concurrent-workflow-{i}",
                stage="input_validation",
                data={"prompt": f"è¯·ç”¨geminiå¤„ç†ä»»åŠ¡{i}"}
            )
            workflows.append(adapter.on_workflow_start(workflow_context))

        # å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*workflows)

        # éªŒè¯ç»“æœ
        for i, result in enumerate(results):
            assert result is not None, f"å¹¶å‘å·¥ä½œæµ{i}åº”è¯¥è¿”å›ç»“æœ"
            assert "gemini" in result.lower(), f"ç»“æœ{i}åº”è¯¥åŒ…å«ç›®æ ‡CLI"

    @pytest.mark.unit
    async def test_workflow_statistics_tracking(self, adapter, mock_context):
        """æµ‹è¯•å·¥ä½œæµç»Ÿè®¡è·Ÿè¸ª"""
        await adapter.initialize()

        # æ‰§è¡Œå‡ ä¸ªå·¥ä½œæµ
        await adapter.on_workflow_start(mock_context)
        await adapter.on_stage_complete(mock_context, "stage_result")
        await adapter.on_workflow_success(mock_context, "final_result")

        # éªŒè¯ç»Ÿè®¡
        assert adapter.stages_processed > 0, "åº”è¯¥è®°å½•å¤„ç†çš„é˜¶æ®µæ•°"
        assert len(adapter.processed_workflows) > 0, "åº”è¯¥è®°å½•å¤„ç†çš„å·¥ä½œæµ"

    @pytest.mark.unit
    async def test_workflow_context_preservation(self, adapter, mock_context):
        """æµ‹è¯•å·¥ä½œæµä¸Šä¸‹æ–‡ä¿ç•™"""
        await adapter.initialize()

        original_workflow_id = mock_context.workflow_id
        original_stage = mock_context.stage
        original_data = mock_context.data.copy()

        result = await adapter.on_workflow_start(mock_context)

        # éªŒè¯ä¸Šä¸‹æ–‡æœªè¢«ä¿®æ”¹
        assert mock_context.workflow_id == original_workflow_id, "å·¥ä½œæµIDåº”è¯¥ä¿æŒä¸å˜"
        assert mock_context.stage == original_stage, "é˜¶æ®µåº”è¯¥ä¿æŒä¸å˜"
        assert mock_context.data == original_data, "æ•°æ®åº”è¯¥ä¿æŒä¸å˜"

    @pytest.mark.unit
    async def test_workflow_lifecycle(self, adapter, mock_context):
        """æµ‹è¯•å®Œæ•´å·¥ä½œæµç”Ÿå‘½å‘¨æœŸ"""
        await adapter.initialize()

        # æ¨¡æ‹Ÿå®Œæ•´å·¥ä½œæµç”Ÿå‘½å‘¨æœŸ
        await adapter.on_workflow_start(mock_context)
        await adapter.on_stage_complete(mock_context, "intermediate_result")
        await adapter.on_workflow_success(mock_context, "success_result")

        # éªŒè¯ç”Ÿå‘½å‘¨æœŸè®°å½•
        workflow_types = [w.get('type') for w in adapter.processed_workflows]
        assert len(workflow_types) >= 2, "åº”è¯¥è®°å½•å¤šä¸ªå·¥ä½œæµäº‹ä»¶"

    @pytest.mark.unit
    async def test_pipeline_specific_features(self, adapter, mock_context):
        """æµ‹è¯•Pipelineç‰¹æœ‰åŠŸèƒ½"""
        await adapter.initialize()

        # æµ‹è¯•Pipelineé…ç½®åŠŸèƒ½
        pipeline_config = {"max_concurrent_workflows": 5}
        result = await adapter.on_pipeline_ready(pipeline_config)

        assert result is None, "Pipelineå°±ç»ªHookåº”è¯¥è¿”å›None"

    @pytest.mark.unit
    async def test_task_queue_initialization(self, adapter):
        """æµ‹è¯•ä»»åŠ¡é˜Ÿåˆ—åˆå§‹åŒ–"""
        await adapter.initialize()

        assert hasattr(adapter, 'task_queue'), "åº”è¯¥åˆå§‹åŒ–ä»»åŠ¡é˜Ÿåˆ—"
        assert isinstance(adapter.task_queue, asyncio.Queue), "ä»»åŠ¡é˜Ÿåˆ—åº”è¯¥æ˜¯asyncio.Queueç±»å‹"


class TestIFlowWorkflowAdapterEdgeCases:
    """iFlowé€‚é…å™¨è¾¹ç•Œæƒ…å†µæµ‹è¯•"""

    @pytest.fixture
    def adapter(self):
        """åˆ›å»ºé€‚é…å™¨å®ä¾‹"""
        # ç›´æ¥ä½¿ç”¨å®é™…çš„iFlowé€‚é…å™¨
        from src.adapters.iflow.workflow_adapter import IFlowWorkflowAdapter
        return IFlowWorkflowAdapter("iflow")

    @pytest.mark.unit
    async def test_empty_workflow_handling(self, adapter):
        """æµ‹è¯•ç©ºå·¥ä½œæµå¤„ç†"""
        await adapter.initialize()

        empty_context = MockIFlowPipelineContext(
            workflow_id="",
            stage="",
            data={}
        )

        result = await adapter.on_workflow_start(empty_context)
        assert result is None, "ç©ºå·¥ä½œæµåº”è¯¥è¿”å›None"

    @pytest.mark.unit
    async def test_malformed_workflow_data(self, adapter):
        """æµ‹è¯•æ ¼å¼é”™è¯¯çš„å·¥ä½œæµæ•°æ®"""
        await adapter.initialize()

        malformed_context = MockIFlowPipelineContext(
            workflow_id="test-workflow",
            stage="input_validation",
            data={"invalid": "data"}  # ç¼ºå°‘promptå­—æ®µ
        )

        # åº”è¯¥ä¸æŠ›å‡ºå¼‚å¸¸
        result = await adapter.on_workflow_start(malformed_context)
        assert result is None, "æ ¼å¼é”™è¯¯æ•°æ®åº”è¯¥è¿”å›None"

    @pytest.mark.unit
    async def test_very_long_workflow_task(self, adapter):
        """æµ‹è¯•è¶…é•¿å·¥ä½œæµä»»åŠ¡"""
        await adapter.initialize()

        long_prompt = "è¯·ç”¨claudeå¸®æˆ‘åˆ†æ" + "x" * 10000
        long_context = MockIFlowPipelineContext(
            workflow_id="long-workflow",
            stage="input_validation",
            data={"prompt": long_prompt}
        )

        result = await adapter.on_workflow_start(long_context)
        assert result is not None, "è¶…é•¿ä»»åŠ¡åº”è¯¥è¢«å¤„ç†"
        assert "claude" in result.lower(), "ç»“æœåº”è¯¥åŒ…å«ç›®æ ‡CLI"

    @pytest.mark.unit
    async def test_special_characters_in_workflow(self, adapter):
        """æµ‹è¯•å·¥ä½œæµä¸­çš„ç‰¹æ®Šå­—ç¬¦"""
        await adapter.initialize()

        special_prompt = "è¯·ç”¨geminiå¤„ç†åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„ä»»åŠ¡: ğŸš€ @#$%^&*(){}[]|\\:;\"'<>?,./"
        special_context = MockIFlowPipelineContext(
            workflow_id="special-chars-workflow",
            stage="input_validation",
            data={"prompt": special_prompt}
        )

        # åº”è¯¥ä¸æŠ›å‡ºå¼‚å¸¸
        result = await adapter.on_workflow_start(special_context)
        assert result is not None, "ç‰¹æ®Šå­—ç¬¦ä»»åŠ¡åº”è¯¥è¢«å¤„ç†"

    @pytest.mark.unit
    async def test_unicode_workflow_content(self, adapter):
        """æµ‹è¯•Unicodeå·¥ä½œæµå†…å®¹"""
        await adapter.initialize()

        unicode_prompt = "è¯·ç”¨qwencodeå¤„ç†åŒ…å«Unicodeçš„å†…å®¹: ğŸ¯ æµ‹è¯•ä¸­æ–‡ Ã±oÃ«l espaÃ±ol Ñ€ÑƒÑÑĞºĞ¸Ğ¹ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"
        unicode_context = MockIFlowPipelineContext(
            workflow_id="unicode-workflow",
            stage="input_validation",
            data={"prompt": unicode_prompt}
        )

        result = await adapter.on_workflow_start(unicode_context)
        assert result is not None, "Unicodeå†…å®¹åº”è¯¥è¢«æ­£ç¡®å¤„ç†"