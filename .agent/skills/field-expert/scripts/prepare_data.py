#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®å‡†å¤‡è„šæœ¬ - åœºåŸŸåˆ†ææŠ€èƒ½

åŠŸèƒ½ï¼š
- è‡ªåŠ¨æ‰«æè¾“å…¥ç›®å½•ä¸­çš„æ–‡æœ¬æ–‡ä»¶
- æŒ‰ç±»å‹åˆ†ç±»ï¼ˆæ‰æ ¹ç†è®ºã€ç¤¾ä¼šç½‘ç»œã€ESOCæ¡†æ¶ï¼‰
- åˆå¹¶ä¸ºç»Ÿä¸€çš„è¾“å…¥æ ¼å¼

è¾“å…¥ï¼š
- --input: æºæ•°æ®ç›®å½•è·¯å¾„

è¾“å‡ºï¼š
- input/processed/combined_input.json
"""

import os
import sys
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))


def scan_source_files(source_dir: Path) -> Dict[str, List[Path]]:
    """
    æ‰«ææºç›®å½•ï¼Œåˆ†ç±»æ–‡ä»¶
    
    åˆ†ç±»è§„åˆ™ï¼š
    - æ‰æ ¹ç†è®º: åŒ…å«"å¼€æ”¾ç¼–ç "ã€"é€‰æ‹©æ€§ç¼–ç "ã€"é¥±å’Œåº¦æ£€éªŒ"
    - ç¤¾ä¼šç½‘ç»œ: åŒ…å«"ç¤¾ä¼šç½‘ç»œ"ã€"network"
    - ESOCæ¡†æ¶: åŒ…å«"ESOC"ã€"ç†è®ºæ¡†æ¶"
    """
    files = {
        "grounded_theory": [],
        "social_network": [],
        "esoc_framework": []
    }
    
    if not source_dir.exists():
        print(f"é”™è¯¯: æºç›®å½•ä¸å­˜åœ¨: {source_dir}")
        return files
    
    # æ‰«ææ‰€æœ‰æ–‡æœ¬æ–‡ä»¶
    txt_files = list(source_dir.glob("*.txt"))
    
    for txt_file in txt_files:
        name = txt_file.name.lower()
        
        if "ç¤¾ä¼šç½‘ç»œ" in name or "network" in name:
            files["social_network"].append(txt_file)
        elif "esoc" in name or "ç†è®ºæ¡†æ¶" in name:
            files["esoc_framework"].append(txt_file)
        else:
            # å…¶ä»–æ–‡ä»¶å½’ç±»ä¸ºæ‰æ ¹ç†è®ºæ•°æ®
            files["grounded_theory"].append(txt_file)
    
    return files


def read_text_file(file_path: Path) -> Dict[str, Any]:
    """è¯»å–æ–‡æœ¬æ–‡ä»¶ï¼Œè¿”å›ç»“æ„åŒ–æ•°æ®"""
    if not file_path.exists():
        return {"error": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"}
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æå–æ–‡ä»¶åä½œä¸ºæ ‡é¢˜
        title = file_path.stem
        
        # ç®€å•ç»Ÿè®¡
        lines = content.split('\n')
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
        
        return {
            "source_file": str(file_path),
            "title": title,
            "content": content,
            "stats": {
                "chars": len(content),
                "lines": len(lines),
                "paragraphs": len(paragraphs)
            }
        }
    except Exception as e:
        return {"error": f"è¯»å–å¤±è´¥: {str(e)}"}


def merge_data(files: Dict[str, List[Path]]) -> Dict[str, Any]:
    """åˆå¹¶æ‰€æœ‰æ•°æ®"""
    merged = {
        "metadata": {
            "sources": [],
            "grounded_theory_count": 0,
            "social_network_count": 0,
            "esoc_framework_count": 0,
            "total_files": 0,
            "merged_at": datetime.now().isoformat()
        },
        "grounded_theory": {
            "description": "æ‰æ ¹ç†è®ºåˆ†ææ•°æ®",
            "files": []
        },
        "social_network": {
            "description": "ç¤¾ä¼šç½‘ç»œåˆ†ææ•°æ®",
            "files": []
        },
        "esoc_framework": {
            "description": "ESOC-Rç†è®ºæ¡†æ¶æ•°æ®",
            "files": []
        },
        "analysis_ready": True
    }
    
    # å¤„ç†æ‰æ ¹ç†è®ºæ•°æ®
    for file_path in sorted(files["grounded_theory"]):
        data = read_text_file(file_path)
        merged["grounded_theory"]["files"].append(data)
        merged["metadata"]["sources"].append(file_path.name)
    
    merged["metadata"]["grounded_theory_count"] = len(files["grounded_theory"])
    merged["metadata"]["total_files"] += len(files["grounded_theory"])
    
    # å¤„ç†ç¤¾ä¼šç½‘ç»œæ•°æ®
    for file_path in sorted(files["social_network"]):
        data = read_text_file(file_path)
        merged["social_network"]["files"].append(data)
        merged["metadata"]["sources"].append(file_path.name)
    
    merged["metadata"]["social_network_count"] = len(files["social_network"])
    merged["metadata"]["total_files"] += len(files["social_network"])
    
    # å¤„ç†ESOCæ¡†æ¶æ•°æ®
    for file_path in sorted(files["esoc_framework"]):
        data = read_text_file(file_path)
        merged["esoc_framework"]["files"].append(data)
        merged["metadata"]["sources"].append(file_path.name)
    
    merged["metadata"]["esoc_framework_count"] = len(files["esoc_framework"])
    merged["metadata"]["total_files"] += len(files["esoc_framework"])
    
    return merged


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    input_path = None
    output_path = None
    
    for i, arg in enumerate(sys.argv):
        if arg == "--input" and i + 1 < len(sys.argv):
            input_path = Path(sys.argv[i + 1])
        elif arg == "--output" and i + 1 < len(sys.argv):
            output_path = Path(sys.argv[i + 1])
    
    # é»˜è®¤è·¯å¾„
    if input_path is None:
        # é»˜è®¤è¾“å…¥è·¯å¾„ï¼šå½“å‰å·¥ä½œç›®å½•ä¸‹çš„æºæ•°æ®
        input_path = Path.cwd() / "test_data" / "xiyouji_analysis"
    
    if output_path is None:
        # é»˜è®¤è¾“å‡ºè·¯å¾„ï¼šæŠ€èƒ½ç›®å½•ä¸‹çš„ workflow
        skill_dir = Path(__file__).parent.parent
        output_path = skill_dir / "field_analysis_workflow" / "input" / "processed"
    
    print("=" * 60)
    print("  åœºåŸŸåˆ†ææŠ€èƒ½ - æ•°æ®å‡†å¤‡è„šæœ¬")
    print("=" * 60)
    print(f"\nğŸ“ æºç›®å½•: {input_path}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_path}")
    
    # æ‰«ææ–‡ä»¶
    print("\nğŸ“‚ æ‰«ææºç›®å½•...")
    files = scan_source_files(input_path)
    
    print(f"   æ‰æ ¹ç†è®ºæ–‡ä»¶: {len(files['grounded_theory'])}")
    for f in files['grounded_theory'][:3]:
        print(f"     - {f.name}")
    if len(files['grounded_theory']) > 3:
        print(f"     ... å…± {len(files['grounded_theory'])} ä¸ªæ–‡ä»¶")
    
    print(f"   ç¤¾ä¼šç½‘ç»œæ–‡ä»¶: {len(files['social_network'])}")
    for f in files['social_network']:
        print(f"     - {f.name}")
    
    print(f"   ESOCæ¡†æ¶æ–‡ä»¶: {len(files['esoc_framework'])}")
    for f in files['esoc_framework']:
        print(f"     - {f.name}")
    
    # åˆå¹¶æ•°æ®
    print("\nğŸ“¦ åˆå¹¶æ•°æ®...")
    merged_data = merge_data(files)
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_path.mkdir(parents=True, exist_ok=True)
    
    # å†™å…¥è¾“å‡ºæ–‡ä»¶
    output_file = output_path / "combined_input.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(merged_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… æ•°æ®å‡†å¤‡å®Œæˆ!")
    print(f"   è¾“å‡ºæ–‡ä»¶: {output_file}")
    print(f"   æ€»æ–‡ä»¶æ•°: {merged_data['metadata']['total_files']}")
    print(f"   æ‰æ ¹ç†è®º: {merged_data['metadata']['grounded_theory_count']}")
    print(f"   ç¤¾ä¼šç½‘ç»œ: {merged_data['metadata']['social_network_count']}")
    print(f"   ESOCæ¡†æ¶: {merged_data['metadata']['esoc_framework_count']}")
    
    # æ‰“å°æ‘˜è¦
    print(f"\nğŸ“‹ æ•°æ®æ‘˜è¦:")
    print(f"   - æ‰æ ¹ç†è®ºæ®µè½æ•°: {sum(f['stats']['paragraphs'] for f in merged_data['grounded_theory']['files'])}")
    print(f"   - æ€»å­—ç¬¦æ•°: {sum(f['stats']['chars'] for f in merged_data['grounded_theory']['files'])}")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
