#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¸ƒè¿ªå„åœºåŸŸåˆ†æå·¥ä½œæµ - ä½¿ç”¨ stigmergy è°ƒç”¨å®¿ä¸» LLM

å·¥ä½œæµæ­¥éª¤:
  1. æ•°æ®å‡†å¤‡ - æ‰«ææºæ–‡ä»¶ï¼Œç”Ÿæˆ combined_input.json
  2. è¾¹ç•Œåˆ†æ - è°ƒç”¨ LLM è¯†åˆ«åœºåŸŸè¾¹ç•Œ
  3. èµ„æœ¬åˆ†æ - è°ƒç”¨ LLM åˆ†æèµ„æœ¬åˆ†å¸ƒ
  4. ä¹ æ€§åˆ†æ - è°ƒç”¨ LLM åˆ†æä¹ æ€§æ¨¡å¼
  5. åŠ¨æ€åˆ†æ - è°ƒç”¨ LLM åˆ†æåœºåŸŸåŠ¨åŠ›å­¦
  6. ç”ŸæˆæŠ¥å‘Š - æ•´åˆæ‰€æœ‰ç»“æœ

è¾“å…¥è¾“å‡ºè§„èŒƒ:
  è¾“å…¥:  test_data/{analysis}/ (21ä¸ªåˆ†ææ–‡ä»¶)
  ä¸­é—´:  field_analysis_workflow/intermediate/
  è¾“å‡º:  field_analysis_workflow/output/
"""

import argparse
import json
import subprocess
import sys
import re
import os
import tempfile
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional


def run_stigmergy(prompt: str) -> str:
    """è°ƒç”¨ stigmergy qwen æ‰§è¡Œ LLM åˆ†æ"""
    print(f"\nğŸ“¤ æ­£åœ¨è°ƒç”¨ stigmergy qwen...")
    print(f"   æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
    
    try:
        # å†™å…¥æç¤ºè¯åˆ°ä¸´æ—¶æ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
        prompt_file = f"temp_prompt_{timestamp}.txt"
        
        with open(prompt_file, 'w', encoding='utf-8') as f:
            f.write(prompt)
        
        # ä½¿ç”¨ PowerShell æ‰§è¡Œ stigmergy claude
        ps_command = f'powershell -Command "$p = Get-Content \'{prompt_file}\' -Raw -Encoding UTF8; stigmergy claude $p"'
        
        result = subprocess.run(
            ps_command,
            capture_output=True,
            text=True,
            shell=True,
            encoding='utf-8',
            errors='replace',
            timeout=180
        )
        
        output = result.stdout
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            os.remove(prompt_file)
        except:
            pass
        
        if result.returncode != 0:
            print(f"âš ï¸ å‘½ä»¤è¿”å›éé›¶: {result.stderr}")
        
        if output:
            print(f"\nğŸ“¥ LLM å“åº” (å‰500å­—ç¬¦):\n{output[:500]}...")
        else:
            print(f"\nâš ï¸ LLM å“åº”ä¸ºç©ºï¼")
        
        return output
        
    except subprocess.TimeoutExpired:
        print("âš ï¸ stigmergy è°ƒç”¨è¶…æ—¶")
        return ""
    except Exception as e:
        print(f"âš ï¸ stigmergy è°ƒç”¨å¤±è´¥: {e}")
        return ""


def extract_entity_summary(input_data: Dict[str, Any]) -> str:
    """ä»è¾“å…¥æ•°æ®ä¸­æå–å…³é”®å®ä½“æ‘˜è¦ï¼ˆä¸åŒ…å«å®Œæ•´æ–‡æœ¬ï¼‰"""
    entities = {"actors": set()}
    
    grounded_theory_files = input_data.get("grounded_theory", {}).get("files", [])
    
    # æå–è§’è‰²åç§°
    for file_data in grounded_theory_files[:5]:
        content = file_data.get("content", "")
        actor_patterns = [
            r'(å”åƒ§|å­™æ‚Ÿç©º|çŒªå…«æˆ’|æ²™åƒ§|ç™½é¾™é©¬)',
            r'(ç‰çš‡å¤§å¸|å¤ªä¸Šè€å›|è§‚éŸ³è©è¨|å¦‚æ¥ä½›ç¥–)',
            r'(ç‰›é­”ç‹|é“æ‰‡å…¬ä¸»|çº¢å­©å„¿|ç™½éª¨ç²¾)',
            r'([^\sï¼Œã€‚ã€ï¼ï¼Ÿï¼šï¼›""''\'ã€ã€‘()\\[\\]]{2,4}ä½›ç¥–)',
            r'([^\sï¼Œã€‚ã€ï¼ï¼Ÿï¼šï¼›""''\'ã€ã€‘()\\[\\]]{2,4}è©è¨)',
        ]
        for pattern in actor_patterns:
            matches = re.findall(pattern, content)
            entities["actors"].update(matches)
    
    lines = []
    lines.append("ã€è§’è‰²åˆ—è¡¨ã€‘")
    actors = list(entities["actors"])[:25]
    if actors:
        lines.append(", ".join(actors))
    
    total_chars = sum(f.get("stats", {}).get("chars", 0) for f in grounded_theory_files)
    lines.append(f"ã€æ–‡æœ¬æ€»å­—ç¬¦æ•°ã€‘{total_chars}")
    
    return "\n".join(lines)


def parse_field_response(response: str) -> Dict[str, Any]:
    """è§£æåœºåŸŸåˆ†æå“åº”"""
    result = {"fields": [], "gatekeepers": [], "boundary_dynamics": {}, "field_relations": {}}
    
    # åŒ¹é… "åœºåŸŸåï¼šè§’è‰²1ã€è§’è‰²2" æ ¼å¼
    pattern = r'([^ï¼š:\n]+)[ï¼š:]\s*([^ã€‚\n]+)'
    matches = re.findall(pattern, response)
    
    field_name_map = {
        "ä½›ç•Œ": "ä½›ç•Œ", "Buddha Realm": "ä½›ç•Œ",
        "å¤©åº­": "å¤©åº­", "Heaven": "å¤©åº­",
        "äººé—´": "äººé—´", "Human World": "äººé—´",
        "å¦–ç•Œ": "å¦–ç•Œ", "Demon World": "å¦–ç•Œ",
        "å–ç»å›¢é˜Ÿ": "å–ç»å›¢é˜Ÿ", "Pilgrimage Team": "å–ç»å›¢é˜Ÿ"
    }
    
    for match in matches:
        field_raw = match[0].strip()
        actors_raw = match[1].strip()
        field_name = field_name_map.get(field_raw, field_raw)
        
        if field_name in field_name_map.values():
            actors = [a.strip() for a in re.split(r'[ã€,ï¼Œ]', actors_raw) if a.strip()]
            result["fields"].append({"name": field_name, "core_actors": actors[:5]})
    
    return result


def parse_capital_response(response: str) -> Dict[str, Any]:
    """è§£æèµ„æœ¬åˆ†æå“åº”"""
    result = {"capital_types": {}, "distribution": {}, "ranking": []}
    
    pattern = r'([^ï¼š:\n]+)[ï¼š:]([^\n]+)'
    matches = re.findall(pattern, response)
    
    capital_level_map = {"é«˜": 3, "ä¸­": 2, "ä½": 1}
    field_name_map = {"ä½›ç•Œ": "ä½›ç•Œ", "å¤©åº­": "å¤©åº­", "äººé—´": "äººé—´", "å¦–ç•Œ": "å¦–ç•Œ", "å–ç»å›¢é˜Ÿ": "å–ç»å›¢é˜Ÿ"}
    capital_names = ["ç»æµèµ„æœ¬", "æ–‡åŒ–èµ„æœ¬", "ç¤¾ä¼šèµ„æœ¬", "è±¡å¾èµ„æœ¬"]
    
    for match in matches:
        field_raw = match[0].strip()
        capital_str = match[1].strip()
        field_name = field_name_map.get(field_raw, field_raw)
        
        if field_name in field_name_map.values():
            capital_dict = {}
            for cn in capital_names:
                level_match = re.search(rf'{cn}\(([é«˜ä¸­ä½123]+)\)', capital_str)
                if level_match:
                    level_str = level_match.group(1)
                    if level_str.isdigit():
                        capital_dict[cn] = int(level_str)
                    elif level_str in capital_level_map:
                        capital_dict[cn] = capital_level_map[level_str]
            result["distribution"][field_name] = capital_dict
    
    return result


def parse_habitus_response(response: str) -> Dict[str, Any]:
    """è§£æä¹ æ€§åˆ†æå“åº”"""
    result = {"actors": [], "symbolic_violence": {}, "cross_field": {}}
    pattern = r'([^ï¼š:\n]+)[ï¼š:]([^\n]+)'
    matches = re.findall(pattern, response)
    
    for match in matches:
        actor_raw = match[0].strip()
        habitus_str = match[1].strip()
        habitus_list = [h.strip() for h in re.split(r'[ã€,ï¼Œ]', habitus_str) if h.strip()]
        result["actors"].append({"name": actor_raw, "habitus": habitus_list[:5]})
    
    return result


def parse_dynamics_response(response: str) -> Dict[str, Any]:
    """è§£æåŠ¨æ€åˆ†æå“åº”"""
    result = {"competition": {}, "power": {}, "evolution": {}, "theory": {}}
    
    # åŒ¹é… "åœºåŸŸ1-åœºåŸŸ2ï¼šå…³ç³»" æ ¼å¼
    pattern = r'([^\s-]+)-([^\sï¼š]+)[ï¼š:]([^\n]+)'
    matches = re.findall(pattern, response)
    
    for match in matches:
        field1 = match[0].strip()
        field2 = match[1].strip()
        relation = match[2].strip()
        result["competition"][f"{field1}-{field2}"] = {"type": relation, "description": relation}
    
    return result


# =============================================================================
# æ­¥éª¤å‡½æ•°
# =============================================================================

def step_1_prepare_data(input_path: Path, workflow_dir: Path) -> Path:
    """æ­¥éª¤1: æ•°æ®å‡†å¤‡"""
    print("\n" + "=" * 60)
    print("æ­¥éª¤1: æ•°æ®å‡†å¤‡")
    print("=" * 60)
    
    SKILL_DIR = Path(__file__).parent.parent
    prepare_script = SKILL_DIR / "scripts" / "prepare_data.py"
    output_dir = workflow_dir / "input" / "processed"
    
    # ç›´æ¥è°ƒç”¨ prepare_data.py
    sys.path.insert(0, str(SKILL_DIR / "scripts"))
    from prepare_data import scan_source_files, merge_data
    
    source_dir = Path(input_path)
    files = scan_source_files(source_dir)
    
    print(f"   æ‰æ ¹ç†è®ºæ–‡ä»¶: {len(files['grounded_theory'])}")
    print(f"   ç¤¾ä¼šç½‘ç»œæ–‡ä»¶: {len(files['social_network'])}")
    print(f"   ESOCæ¡†æ¶æ–‡ä»¶: {len(files['esoc_framework'])}")
    
    merged_data = merge_data(files)
    output_dir.mkdir(parents=True, exist_ok=True)
    output_file = output_dir / "combined_input.json"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(merged_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ")
    print(f"   æ€»æ–‡ä»¶æ•°: {merged_data['metadata']['total_files']}")
    
    return output_file


def step_2_boundary_analysis(combined_input: Path, workflow_dir: Path) -> Path:
    """æ­¥éª¤2: è¾¹ç•Œåˆ†æ"""
    print("\n" + "=" * 60)
    print("æ­¥éª¤2: è¾¹ç•Œåˆ†æ (LLM)")
    print("=" * 60)
    
    with open(combined_input, 'r', encoding='utf-8') as f:
        input_data = json.load(f)
    
    # æå–æ‘˜è¦
    summary = extract_entity_summary(input_data)
    
    # æ„å»ºæç¤ºè¯
    prompt = f"""{summary}

æ ¹æ®ä»¥ä¸Šæ–‡æœ¬ï¼Œè¯†åˆ«å…¶ä¸­çš„ç¤¾ä¼šåœºåŸŸåŠæ ¸å¿ƒè¡ŒåŠ¨è€…ã€‚

**å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–é¢å¤–å†…å®¹**ï¼š

```
å¤©åº­ï¼šç‰çš‡å¤§å¸ã€å¤ªä¸Šè€å›ã€è§‚éŸ³è©è¨
ä½›ç•Œï¼šå¦‚æ¥ä½›ç¥–
äººé—´ï¼šå”åƒ§ã€å­™æ‚Ÿç©ºã€çŒªå…«æˆ’ã€æ²™åƒ§
å¦–ç•Œï¼šç‰›é­”ç‹ã€ç™½éª¨ç²¾
```

åªè¾“å‡ºæ ¼å¼åŒ–çš„åœºåŸŸåˆ—è¡¨ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""
    
    response = run_stigmergy(prompt)
    result_data = parse_field_response(response)
    
    output_dir = workflow_dir / "intermediate" / "01_boundary"
    output_dir.mkdir(parents=True, exist_ok=True)
    output_file = output_dir / "boundary_results.json"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result_data, f, ensure_ascii=False, indent=2)
    
    fields = result_data.get("fields", [])
    print(f"âœ… è¾¹ç•Œåˆ†æå®Œæˆ")
    print(f"   è¯†åˆ«åœºåŸŸ: {len(fields)} ä¸ª")
    for field in fields:
        print(f"   - {field.get('name', 'æœªå‘½å')}: {', '.join(field.get('core_actors', [])[:3])}")
    
    return output_file


def step_3_capital_analysis(combined_input: Path, workflow_dir: Path) -> Path:
    """æ­¥éª¤3: èµ„æœ¬åˆ†æ"""
    print("\n" + "=" * 60)
    print("æ­¥éª¤3: èµ„æœ¬åˆ†æ (LLM)")
    print("=" * 60)
    
    # è¯»å–æ­¥éª¤2çš„è¾¹ç•Œåˆ†æç»“æœ
    boundary_file = workflow_dir / "intermediate" / "01_boundary" / "boundary_results.json"
    boundary_data = json.loads(boundary_file.read_text(encoding='utf-8')) if boundary_file.exists() else {}
    fields = boundary_data.get("fields", [])
    
    # æ„å»ºåœºåŸŸä¿¡æ¯
    fields_info = []
    for field in fields:
        name = field.get("name", "")
        actors = field.get("core_actors", [])
        if name and actors:
            fields_info.append(f"{name}ï¼š{', '.join(actors)}")
    
    with open(combined_input, 'r', encoding='utf-8') as f:
        input_data = json.load(f)
    
    summary = extract_entity_summary(input_data)
    
    fields_info_text = "\n".join(fields_info) if fields_info else "è¯·ä»æ–‡æœ¬ä¸­è¯†åˆ«åœºåŸŸ"
    
    prompt = f"""{summary}

è¯†åˆ«åˆ°çš„åœºåŸŸï¼š
{fields_info_text}

åˆ†æå„åœºåŸŸä¸­ä¸åŒè¡ŒåŠ¨è€…çš„èµ„æœ¬åˆ†å¸ƒã€‚èµ„æœ¬ç±»å‹ï¼šç»æµã€æ–‡åŒ–ã€ç¤¾ä¼šã€è±¡å¾ã€‚

**å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–é¢å¤–å†…å®¹**ï¼š

```
å¤©åº­ï¼š
  ç‰çš‡å¤§å¸ï¼šç»æµ(é«˜)ã€æ–‡åŒ–(ä¸­)ã€ç¤¾ä¼š(é«˜)ã€è±¡å¾(é«˜)
  å¤ªä¸Šè€å›ï¼šç»æµ(é«˜)ã€æ–‡åŒ–(é«˜)ã€ç¤¾ä¼š(ä¸­)ã€è±¡å¾(é«˜)
ä½›ç•Œï¼š
  å¦‚æ¥ä½›ç¥–ï¼šç»æµ(é«˜)ã€æ–‡åŒ–(é«˜)ã€ç¤¾ä¼š(é«˜)ã€è±¡å¾(é«˜)
äººé—´ï¼š
  å”åƒ§ï¼šç»æµ(ä½)ã€æ–‡åŒ–(é«˜)ã€ç¤¾ä¼š(ä¸­)ã€è±¡å¾(é«˜)
```

åªè¾“å‡ºæ ¼å¼åŒ–çš„èµ„æœ¬åˆ†å¸ƒï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""
    
    response = run_stigmergy(prompt)
    result_data = parse_capital_response(response)
    
    output_dir = workflow_dir / "intermediate" / "02_capital"
    output_dir.mkdir(parents=True, exist_ok=True)
    output_file = output_dir / "capital_results.json"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… èµ„æœ¬åˆ†æå®Œæˆ")
    print(f"   åœºåŸŸæ•°é‡: {len(result_data.get('distribution', {}))}")
    
    return output_file


def step_4_habitus_analysis(combined_input: Path, workflow_dir: Path) -> Path:
    """æ­¥éª¤4: ä¹ æ€§åˆ†æ"""
    print("\n" + "=" * 60)
    print("æ­¥éª¤4: ä¹ æ€§åˆ†æ (LLM)")
    print("=" * 60)
    
    with open(combined_input, 'r', encoding='utf-8') as f:
        input_data = json.load(f)
    
    summary = extract_entity_summary(input_data)
    
    prompt = f"""{summary}

åˆ†æå„è¡ŒåŠ¨è€…çš„ä¹ æ€§ç‰¹å¾ã€‚ä¹ æ€§æ˜¯åœºåŸŸä¸­å†…åŒ–çš„æ„ŸçŸ¥ã€è¯„åˆ¤å’Œè¡ŒåŠ¨å€¾å‘ã€‚

**å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–é¢å¤–å†…å®¹**ï¼š

```
å­™æ‚Ÿç©ºï¼šåæŠ—æƒå¨ã€è¿½æ±‚è‡ªç”±ã€æœºæ™ºçµæ´»
çŒªå…«æˆ’ï¼šè´ªåƒæ‡’æƒ°ã€æ€§æƒ…æ¸©å’Œã€çœ·æ‹äººä¸–
å”åƒ§ï¼šåšå®šä¿¡å¿µã€æ…ˆæ‚²ä¸ºå¾ªã€å¾ªè§„è¹ˆçŸ©
```

åªè¾“å‡ºæ ¼å¼åŒ–çš„ä¹ æ€§åˆ—è¡¨ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""
    
    response = run_stigmergy(prompt)
    result_data = parse_habitus_response(response)
    
    output_dir = workflow_dir / "intermediate" / "03_habitus"
    output_dir.mkdir(parents=True, exist_ok=True)
    output_file = output_dir / "habitus_results.json"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ä¹ æ€§åˆ†æå®Œæˆ")
    print(f"   è¡ŒåŠ¨è€…æ•°é‡: {len(result_data.get('actors', []))}")
    
    return output_file


def step_5_dynamics_analysis(combined_input: Path, workflow_dir: Path) -> Path:
    """æ­¥éª¤5: åœºåŸŸåŠ¨æ€åˆ†æ"""
    print("\n" + "=" * 60)
    print("æ­¥éª¤5: åœºåŸŸåŠ¨æ€åˆ†æ (LLM)")
    print("=" * 60)
    
    with open(combined_input, 'r', encoding='utf-8') as f:
        input_data = json.load(f)
    
    summary = extract_entity_summary(input_data)
    
    prompt = f"""åœºåŸŸåŠ¨æ€åˆ†æç»´åº¦ï¼š
1. åœºåŸŸé—´å…³ç³»ï¼šå¯¹ç«‹ã€åˆä½œã€ç»Ÿæ²»ã€ä¾é™„ç­‰
2. åœºåŸŸç«äº‰ï¼šèµ„æºäº‰å¤ºã€ä½ç½®äº‰å¤º
3. åœºåŸŸæ¼”å˜ï¼šå†å²å˜è¿ã€ç»“æ„é‡ç»„

{summary}

åˆ†æåœºåŸŸé—´çš„å…³ç³»å’Œæ¼”å˜ã€‚

**å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–é¢å¤–å†…å®¹**ï¼š

```
ä½›ç•Œ-å¤©åº­ï¼šç»Ÿæ²»å…³ç³»ï¼ˆä½›ç•Œç»Ÿå¾¡å¤©åº­ï¼‰
å¤©åº­-å¦–ç•Œï¼šå¯¹ç«‹å…³ç³»ï¼ˆå¤©åº­é•‡å‹å¦–ç•Œï¼‰
å–ç»è¿‡ç¨‹ï¼šä»äººé—´åˆ°ä½›ç•Œçš„ä¹ æ€§è½¬åŒ–
```

åªè¾“å‡ºæ ¼å¼åŒ–çš„åœºåŸŸåŠ¨æ€ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""
    
    response = run_stigmergy(prompt)
    result_data = parse_dynamics_response(response)
    
    output_dir = workflow_dir / "intermediate" / "04_dynamics"
    output_dir.mkdir(parents=True, exist_ok=True)
    output_file = output_dir / "dynamics_results.json"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… åœºåŸŸåŠ¨æ€åˆ†æå®Œæˆ")
    
    return output_file


def step_6_generate_report(workflow_dir: Path) -> Path:
    """æ­¥éª¤6: ç”ŸæˆæŠ¥å‘Š"""
    print("\n" + "=" * 60)
    print("æ­¥éª¤6: ç”ŸæˆæŠ¥å‘Š")
    print("=" * 60)
    
    # è¯»å–æ‰€æœ‰ä¸­é—´ç»“æœ
    boundary_file = workflow_dir / "intermediate" / "01_boundary" / "boundary_results.json"
    capital_file = workflow_dir / "intermediate" / "02_capital" / "capital_results.json"
    habitus_file = workflow_dir / "intermediate" / "03_habitus" / "habitus_results.json"
    dynamics_file = workflow_dir / "intermediate" / "04_dynamics" / "dynamics_results.json"
    
    boundary = json.loads(boundary_file.read_text(encoding='utf-8')) if boundary_file.exists() else {}
    capital = json.loads(capital_file.read_text(encoding='utf-8')) if capital_file.exists() else {}
    habitus = json.loads(habitus_file.read_text(encoding='utf-8')) if habitus_file.exists() else {}
    dynamics = json.loads(dynamics_file.read_text(encoding='utf-8')) if dynamics_file.exists() else {}
    
    # ç”Ÿæˆç»¼åˆåˆ†æ
    comprehensive = {
        "metadata": {
            "generated_at": datetime.now().isoformat(),
            "analysis_type": "bourdieu_field",
            "methodology": "LLM-based analysis"
        },
        "boundary_analysis": boundary,
        "capital_analysis": capital,
        "habitus_analysis": habitus,
        "dynamics_analysis": dynamics
    }
    
    output_json = workflow_dir / "output" / "json" / "comprehensive_analysis.json"
    output_json.parent.mkdir(parents=True, exist_ok=True)
    with open(output_json, 'w', encoding='utf-8') as f:
        json.dump(comprehensive, f, ensure_ascii=False, indent=2)
    
    # ç”ŸæˆHTMLæŠ¥å‘Š
    fields = boundary.get("fields", [])
    html = f"""<!DOCTYPE html>
<html lang=\"zh-CN\">
<head>
    <meta charset=\"UTF-8\">
    <title>å¸ƒè¿ªå„åœºåŸŸåˆ†ææŠ¥å‘Š</title>
    <style>
        body {{ font-family: serif; max-width: 900px; margin: 0 auto; padding: 20px; }}
        h1 {{ color: #2d3748; border-bottom: 3px solid #c53030; padding-bottom: 10px; }}
        .field {{ background: #f7fafc; padding: 15px; margin: 10px 0; border-left: 4px solid #3182ce; }}
    </style>
</head>
<body>
    <h1>å¸ƒè¿ªå„åœºåŸŸåˆ†ææŠ¥å‘Š</h1>
    <p><strong>ç”Ÿæˆæ—¶é—´:</strong> {comprehensive['metadata']['generated_at']}</p>
    <p><strong>åˆ†ææ–¹æ³•:</strong> LLMå®æ—¶åˆ†æ</p>
    
    <h2>è¯†åˆ«åœºåŸŸ ({len(fields)}ä¸ª)</h2>
    {''.join([f'<div class=\"field\"><strong>{f["name"]}</strong>ï¼š{", ".join(f["core_actors"])}</div>' for f in fields])}
    
    <footer><p>ç”± stigmergy qwen å®æ—¶åˆ†æç”Ÿæˆ</p></footer>
</body>
</html>"""
    
    output_html = workflow_dir / "output" / "reports" / "field_analysis_report.html"
    output_html.parent.mkdir(parents=True, exist_ok=True)
    with open(output_html, 'w', encoding='utf-8') as f:
        f.write(html)
    
    print(f"âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
    print(f"   HTML: {output_html}")
    print(f"   JSON: {output_json}")
    
    return output_html


# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================

def main():
    parser = argparse.ArgumentParser(description="å¸ƒè¿ªå„åœºåŸŸåˆ†æå·¥ä½œæµ")
    # è®¡ç®—é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ test_data/xiyouji_analysis è·¯å¾„
    project_root = Path(__file__).resolve().parent.parent.parent.parent
    input_path = project_root / "test_data" / "xiyouji_analysis"
    parser.add_argument("--input", default=str(input_path),
                        help="è¾“å…¥æ•°æ®ç›®å½•")
    parser.add_argument("--step", type=str, default="all", 
                        help="æ‰§è¡Œæ­¥éª¤: 1, 2, 3, 4, 5, 6, all")
    
    args = parser.parse_args()
    
    SKILL_DIR = Path(__file__).parent.parent
    WORKFLOW_DIR = SKILL_DIR / "field_analysis_workflow"
    
    input_path = Path(args.input)
    
    print("=" * 60)
    print("  å¸ƒè¿ªå„åœºåŸŸåˆ†æå·¥ä½œæµ")
    print(f"  è¾“å…¥: {input_path}")
    print(f"  æ­¥éª¤: {args.step}")
    print("=" * 60)
    
    # æ‰§è¡Œæ­¥éª¤
    if args.step in ["1", "all"]:
        combined_input = step_1_prepare_data(input_path, WORKFLOW_DIR)
    else:
        combined_input = WORKFLOW_DIR / "input" / "processed" / "combined_input.json"
    
    if args.step in ["2", "all"]:
        step_2_boundary_analysis(combined_input, WORKFLOW_DIR)
    
    if args.step in ["3", "all"]:
        step_3_capital_analysis(combined_input, WORKFLOW_DIR)
    
    if args.step in ["4", "all"]:
        step_4_habitus_analysis(combined_input, WORKFLOW_DIR)
    
    if args.step in ["5", "all"]:
        step_5_dynamics_analysis(combined_input, WORKFLOW_DIR)
    
    if args.step in ["6", "all"]:
        step_6_generate_report(WORKFLOW_DIR)
    
    print("\n" + "=" * 60)
    print("  âœ… å·¥ä½œæµå®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    main()