#!/usr/bin/env python3
"""
æ™ºèƒ½Wikiåˆ›å»ºç³»ç»Ÿä¸»æ§åˆ¶å™¨
æ•´åˆæ‰€æœ‰å­æ¨¡å—ï¼Œæä¾›å®Œæ•´çš„Wikiåˆ›å»ºæµç¨‹
"""

import json
import sys
import os
from datetime import datetime
from typing import Dict, List, Any

class IntelligentWikiCreator:
    """æ™ºèƒ½Wikiåˆ›å»ºç³»ç»Ÿ"""
    
    def __init__(self):
        self.current_topic = ""
        self.research_data = {}
        self.agents = []
        self.debate_results = {}
        self.wiki_content = {}
    
    def create_wiki(self, topic: str) -> Dict[str, Any]:
        """åˆ›å»ºWikiçš„ä¸»å…¥å£"""
        print(f"ğŸš€ å¼€å§‹åˆ›å»ºæ™ºèƒ½Wiki: {topic}")
        print("=" * 60)
        
        self.current_topic = topic
        
        # ç¬¬ä¸€æ­¥ï¼šä¸»é¢˜åˆ†æ
        print("\nğŸ“‹ ç¬¬ä¸€æ­¥ï¼šä¸»é¢˜åˆ†æ...")
        topic_analysis = self._analyze_topic(topic)
        print(f"   è¯†åˆ«é¢†åŸŸ: {topic_analysis['domain']}")
        print(f"   å¤æ‚åº¦: {topic_analysis['complexity']}")
        print(f"   å…³é”®è¯: {', '.join(topic_analysis['keywords'])}")
        
        # ç¬¬äºŒæ­¥ï¼šåˆ›å»ºä¸“ä¸šå­æ™ºèƒ½ä½“
        print("\nğŸ¤– ç¬¬äºŒæ­¥ï¼šåˆ›å»ºä¸“ä¸šå­æ™ºèƒ½ä½“...")
        self.agents = self._create_agents(topic_analysis)
        print(f"   åˆ›å»ºäº† {len(self.agents)} ä¸ªä¸“ä¸šå­æ™ºèƒ½ä½“:")
        for agent in self.agents:
            print(f"   - {agent['role']}: {agent['expertise']}")
        
        # ç¬¬ä¸‰æ­¥ï¼šæ–‡çŒ®æ£€ç´¢å’Œåˆ†æ
        print("\nğŸ“š ç¬¬ä¸‰æ­¥ï¼šæ–‡çŒ®æ£€ç´¢å’Œåˆ†æ...")
        literature_data = self._search_literature(topic, topic_analysis)
        print(f"   æ£€ç´¢åˆ° {len(literature_data['papers'])} ç¯‡ç›¸å…³è®ºæ–‡")
        print(f"   é«˜è´¨é‡è®ºæ–‡: {len(literature_data['high_quality_papers'])} ç¯‡")
        
        # ç¬¬å››æ­¥ï¼šå¤šæ™ºèƒ½ä½“åˆ†æ
        print("\nğŸ§  ç¬¬å››æ­¥ï¼šå¤šæ™ºèƒ½ä½“åä½œåˆ†æ...")
        analysis_results = self._multi_agent_analysis(literature_data)
        print(f"   ç”Ÿæˆäº† {len(analysis_results)} ä¸ªä¸“ä¸šè§‚ç‚¹")
        
        # ç¬¬äº”æ­¥ï¼šæ™ºèƒ½è¾©è®º
        print("\nğŸ’¬ ç¬¬äº”æ­¥ï¼šæ™ºèƒ½è¾©è®ºå’Œè§‚ç‚¹æ•´åˆ...")
        debate_results = self._intelligent_debate(analysis_results)
        print(f"   è¾©è®ºå…±è¯†åº¦: {debate_results['consensus_level']:.2f}")
        print(f"   æœ€ç»ˆè§‚ç‚¹: {debate_results['winning_perspective']}")
        
        # ç¬¬å…­æ­¥ï¼šå†…å®¹ç”Ÿæˆ
        print("\nâœï¸ ç¬¬å…­æ­¥ï¼šç”ŸæˆWikiå†…å®¹...")
        wiki_content = self._generate_wiki_content(debate_results, literature_data)
        print(f"   ç”Ÿæˆ {len(wiki_content['sections'])} ä¸ªä¸»è¦ç« èŠ‚")
        print(f"   æ€»å­—æ•°: {wiki_content['word_count']} å­—")
        
        # ç¬¬ä¸ƒæ­¥ï¼šè´¨é‡æ§åˆ¶
        print("\nğŸ” ç¬¬ä¸ƒæ­¥ï¼šè´¨é‡æ§åˆ¶å’Œä¼˜åŒ–...")
        quality_report = self._quality_control(wiki_content)
        print(f"   è´¨é‡è¯„åˆ†: {quality_report['overall_score']:.2f}")
        print(f"   å¯ä¿¡åº¦: {quality_report['credibility_level']}")
        
        # ä¿å­˜ç»“æœ
        result = {
            "topic": topic,
            "timestamp": datetime.now().isoformat(),
            "topic_analysis": topic_analysis,
            "agents": self.agents,
            "literature": literature_data,
            "analysis": analysis_results,
            "debate": debate_results,
            "wiki_content": wiki_content,
            "quality_report": quality_report
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        output_file = f"wiki_{topic}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… Wikiåˆ›å»ºå®Œæˆï¼")
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        return result
    
    def _analyze_topic(self, topic: str) -> Dict[str, Any]:
        """åˆ†æä¸»é¢˜"""
        # ç®€åŒ–çš„ä¸»é¢˜åˆ†æé€»è¾‘
        topic_lower = topic.lower()
        
        # è¯†åˆ«é¢†åŸŸ
        domain_map = {
            "æœºå™¨å­¦ä¹ ": ["æœºå™¨å­¦ä¹ ", "ml", "machine learning"],
            "æ·±åº¦å­¦ä¹ ": ["æ·±åº¦å­¦ä¹ ", "deep learning", "ç¥ç»ç½‘ç»œ", "neural network"],
            "è‡ªç„¶è¯­è¨€å¤„ç†": ["è‡ªç„¶è¯­è¨€å¤„ç†", "nlp", "natural language processing"],
            "è®¡ç®—æœºè§†è§‰": ["è®¡ç®—æœºè§†è§‰", "computer vision", "å›¾åƒè¯†åˆ«"],
            "å¼ºåŒ–å­¦ä¹ ": ["å¼ºåŒ–å­¦ä¹ ", "reinforcement learning", "rl"]
        }
        
        domain = "é€šç”¨"
        keywords = []
        for d, terms in domain_map.items():
            if any(term in topic_lower for term in terms):
                domain = d
                keywords = [term for term in terms if term in topic_lower]
                break
        
        # æå–é¢å¤–çš„å…³é”®è¯
        if not keywords:
            keywords = topic.split()
        
        # è¯„ä¼°å¤æ‚åº¦
        complexity = "ä¸­ç­‰"
        if any(word in topic_lower for word in ["å…¥é—¨", "åŸºç¡€", "ç®€ä»‹", "æ¦‚è¿°"]):
            complexity = "ç®€å•"
        elif any(word in topic_lower for word in ["é«˜çº§", "æ·±å…¥", "å¤æ‚", "å‰æ²¿"]):
            complexity = "å¤æ‚"
        
        return {
            "domain": domain,
            "complexity": complexity,
            "keywords": keywords,
            "research_questions": [
                f"{topic}çš„å®šä¹‰å’Œæ ¸å¿ƒæ¦‚å¿µæ˜¯ä»€ä¹ˆï¼Ÿ",
                f"{topic}çš„ä¸»è¦åŸç†å’Œå·¥ä½œæœºåˆ¶ï¼Ÿ",
                f"{topic}çš„åº”ç”¨åœºæ™¯å’Œå®é™…ä»·å€¼ï¼Ÿ",
                f"{topic}é¢ä¸´çš„æŒ‘æˆ˜å’Œå‘å±•è¶‹åŠ¿ï¼Ÿ"
            ]
        }
    
    def _create_agents(self, topic_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åˆ›å»ºä¸“ä¸šå­æ™ºèƒ½ä½“"""
        domain = topic_analysis["domain"]
        complexity = topic_analysis["complexity"]
        
        # åŸºç¡€æ™ºèƒ½ä½“é…ç½®
        base_agents = [
            {
                "role": "å­¦æœ¯ç ”ç©¶å‘˜",
                "expertise": "ç†è®ºåˆ†æã€æ–‡çŒ®ç»¼è¿°ã€å­¦æœ¯å†™ä½œ",
                "perspective": "å­¦æœ¯ä¸¥è°¨æ€§ã€ç†è®ºåˆ›æ–°",
                "tools": ["æ–‡çŒ®æ£€ç´¢", "ç»Ÿè®¡åˆ†æ", "ç†è®ºæ¨å¯¼"],
                "focus": ["ç†è®ºåŸºç¡€", "å‘å±•å†ç¨‹", "å­¦æœ¯è´¡çŒ®"]
            },
            {
                "role": "æŠ€æœ¯ä¸“å®¶",
                "expertise": "æŠ€æœ¯å®ç°ã€ç³»ç»Ÿæ¶æ„ã€å·¥ç¨‹å®è·µ",
                "perspective": "æŠ€æœ¯å¯è¡Œæ€§ã€å®ç°ç»†èŠ‚",
                "tools": ["æŠ€æœ¯è¯„ä¼°", "æ¶æ„è®¾è®¡", "æ€§èƒ½æµ‹è¯•"],
                "focus": ["æŠ€æœ¯æ¶æ„", "å®ç°æ–¹æ³•", "æ€§èƒ½æŒ‡æ ‡"]
            }
        ]
        
        # æ ¹æ®é¢†åŸŸæ·»åŠ ä¸“ä¸šæ™ºèƒ½ä½“
        if domain in ["æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ "]:
            base_agents.append({
                "role": "è¡Œä¸šå®è·µè€…",
                "expertise": "å®é™…åº”ç”¨ã€å•†ä¸šä»·å€¼ã€æ¡ˆä¾‹åˆ†æ",
                "perspective": "å®ç”¨ä»·å€¼ã€å•†ä¸šå½±å“",
                "tools": ["å¸‚åœºåˆ†æ", "æ¡ˆä¾‹ç ”ç©¶", "ROIè¯„ä¼°"],
                "focus": ["åº”ç”¨åœºæ™¯", "å•†ä¸šä»·å€¼", "å®æ–½æ¡ˆä¾‹"]
            })
        
        # å¤æ‚ä¸»é¢˜æ·»åŠ åˆ†æå¸ˆ
        if complexity == "å¤æ‚":
            base_agents.append({
                "role": "åˆ†æå¸ˆ",
                "expertise": "æ•°æ®åˆ†æã€è¶‹åŠ¿é¢„æµ‹ã€é£é™©è¯„ä¼°",
                "perspective": "æ•°æ®é©±åŠ¨ã€å®¢è§‚åˆ†æ",
                "tools": ["æ•°æ®æŒ–æ˜", "ç»Ÿè®¡åˆ†æ", "å¯è§†åŒ–"],
                "focus": ["æ•°æ®æ”¯æ’‘", "è¶‹åŠ¿åˆ†æ", "é£é™©è¯„ä¼°"]
            })
        
        return base_agents
    
    def _search_literature(self, topic: str, topic_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """æ–‡çŒ®æ£€ç´¢ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
        # æ¨¡æ‹Ÿæ–‡çŒ®æ•°æ®åº“
        mock_papers = self._get_mock_papers(topic, topic_analysis)
        
        # åˆ†æè®ºæ–‡è´¨é‡
        high_quality_papers = []
        for paper in mock_papers:
            if paper["citations"] > 50 or paper["year"] >= 2020:
                high_quality_papers.append(paper)
        
        return {
            "papers": mock_papers,
            "high_quality_papers": high_quality_papers,
            "total_papers": len(mock_papers),
            "search_strategy": f"åŸºäº{topic_analysis['domain']}é¢†åŸŸçš„ä¸“ä¸šæ£€ç´¢",
            "quality_filter": "å¼•ç”¨æ•° > 50 æˆ– å¹´ä»½ >= 2020"
        }
    
    def _get_mock_papers(self, topic: str, topic_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è·å–æ¨¡æ‹Ÿè®ºæ–‡æ•°æ®"""
        domain = topic_analysis["domain"]
        
        # æ ¹æ®é¢†åŸŸè¿”å›ä¸åŒçš„è®ºæ–‡
        if domain == "æœºå™¨å­¦ä¹ ":
            return [
                {
                    "title": f"æ·±åº¦å­¦ä¹ åœ¨{topic}ä¸­çš„åº”ç”¨ç ”ç©¶",
                    "authors": ["å¼ ä¸‰", "æå››", "ç‹äº”"],
                    "year": 2023,
                    "venue": "Nature Machine Intelligence",
                    "citations": 156,
                    "abstract": f"æœ¬æ–‡æ·±å…¥ç ”ç©¶äº†{topic}çš„ç†è®ºåŸºç¡€å’Œå®é™…åº”ç”¨ï¼Œé€šè¿‡å¤§é‡å®éªŒéªŒè¯äº†æ–¹æ³•çš„å¯è¡Œæ€§å’Œæœ‰æ•ˆæ€§ã€‚",
                    "key_findings": [
                        "æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚",
                        "è®¡ç®—æ•ˆç‡ç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•æå‡40%",
                        "å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›"
                    ]
                },
                {
                    "title": f"{topic}ç®—æ³•ä¼˜åŒ–ä¸å®ç°",
                    "authors": ["Alan Turing", "John von Neumann"],
                    "year": 2022,
                    "venue": "Journal of Machine Learning Research",
                    "citations": 89,
                    "abstract": f"æå‡ºäº†{topic}çš„æ–°é¢–ä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚",
                    "key_findings": [
                        "ç®—æ³•å¤æ‚åº¦ä»O(nÂ²)é™ä½åˆ°O(n log n)",
                        "åœ¨å¤§å‹æ•°æ®é›†ä¸Šè¡¨ç°ç¨³å®š",
                        "å¼€æºå®ç°å·²è¢«å¹¿æ³›é‡‡ç”¨"
                    ]
                }
            ]
        elif domain == "æ·±åº¦å­¦ä¹ ":
            return [
                {
                    "title": f"Transformeræ¶æ„åœ¨{topic}ä¸­çš„é©å‘½æ€§åº”ç”¨",
                    "authors": ["Vaswani, A.", "Shazeer, N."],
                    "year": 2021,
                    "venue": "NeurIPS",
                    "citations": 4500,
                    "abstract": f"å±•ç¤ºäº†Transformeræ¶æ„åœ¨{topic}é¢†åŸŸçš„çªç ´æ€§åº”ç”¨ï¼Œå®Œå…¨æ”¹å˜äº†ä¼ ç»Ÿçš„å¤„ç†æ–¹å¼ã€‚",
                    "key_findings": [
                        "æ³¨æ„åŠ›æœºåˆ¶æ˜¾è‘—æå‡æ€§èƒ½",
                        "å¹¶è¡Œè®¡ç®—æ•ˆç‡å¤§å¹…æé«˜",
                        "åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°SOTA"
                    ]
                }
            ]
        else:
            # é€šç”¨è®ºæ–‡
            return [
                {
                    "title": f"{topic}ï¼šç†è®ºã€æ–¹æ³•ä¸åº”ç”¨",
                    "authors": ["ä¸“å®¶å›¢é˜Ÿ"],
                    "year": 2023,
                    "venue": "ç»¼åˆæ€§æœŸåˆŠ",
                    "citations": 45,
                    "abstract": f"å…¨é¢ä»‹ç»äº†{topic}çš„ç†è®ºåŸºç¡€ã€ä¸»è¦æ–¹æ³•å’Œå®é™…åº”ç”¨ï¼Œä¸ºç›¸å…³ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒã€‚",
                    "key_findings": [
                        "ç³»ç»Ÿæ¢³ç†äº†ç›¸å…³ç†è®ºå‘å±•",
                        "æ¯”è¾ƒäº†ä¸»è¦æ–¹æ³•çš„ä¼˜åŠ£",
                        "å±•æœ›äº†æœªæ¥å‘å±•æ–¹å‘"
                    ]
                }
            ]
    
    def _multi_agent_analysis(self, literature_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """å¤šæ™ºèƒ½ä½“åˆ†æ"""
        analysis_results = []
        
        for agent in self.agents:
            # æ¨¡æ‹Ÿæ¯ä¸ªæ™ºèƒ½ä½“çš„åˆ†æè¿‡ç¨‹
            analysis = {
                "agent": agent["role"],
                "perspective": agent["perspective"],
                "key_insights": [],
                "concerns": [],
                "recommendations": [],
                "evidence": [],
                "confidence": 0.8
            }
            
            # æ ¹æ®æ™ºèƒ½ä½“è§’è‰²ç”Ÿæˆä¸åŒçš„åˆ†æ
            if agent["role"] == "å­¦æœ¯ç ”ç©¶å‘˜":
                analysis["key_insights"] = [
                    f"{self.current_topic}çš„ç†è®ºåŸºç¡€éœ€è¦è¿›ä¸€æ­¥å¤¯å®",
                    "ç›¸å…³ç ”ç©¶å­˜åœ¨ä¸€å®šçš„ç†è®ºç©ºç™½",
                    "éœ€è¦æ›´å¤šå®è¯ç ”ç©¶æ”¯æ’‘ç†è®ºå‘å±•"
                ]
                analysis["concerns"] = [
                    "éƒ¨åˆ†ç ”ç©¶ç¼ºä¹ä¸¥è°¨çš„ç†è®ºæ¨å¯¼",
                    "å®éªŒè®¾è®¡å­˜åœ¨ä¸€å®šçš„å±€é™æ€§"
                ]
                analysis["recommendations"] = [
                    "åŠ å¼ºç†è®ºç ”ç©¶å’Œæ•°å­¦å»ºæ¨¡",
                    "è®¾è®¡æ›´ä¸¥è°¨çš„å®éªŒéªŒè¯"
                ]
            
            elif agent["role"] == "æŠ€æœ¯ä¸“å®¶":
                analysis["key_insights"] = [
                    f"{self.current_topic}çš„æŠ€æœ¯å®ç°ç›¸å¯¹æˆç†Ÿ",
                    "å­˜åœ¨å¤šç§å¯è¡Œçš„æŠ€æœ¯æ–¹æ¡ˆ",
                    "æ€§èƒ½ä¼˜åŒ–ä»æœ‰è¾ƒå¤§ç©ºé—´"
                ]
                analysis["concerns"] = [
                    "éƒ¨åˆ†æŠ€æœ¯æ–¹æ¡ˆå¤æ‚åº¦è¿‡é«˜",
                    "å®æ—¶æ€§èƒ½æœ‰å¾…æå‡"
                ]
                analysis["recommendations"] = [
                    "ä¼˜åŒ–ç®—æ³•å¤æ‚åº¦",
                    "æé«˜ç³»ç»Ÿå¯æ‰©å±•æ€§"
                ]
            
            elif agent["role"] == "è¡Œä¸šå®è·µè€…":
                analysis["key_insights"] = [
                    f"{self.current_topic}åœ¨å®é™…åº”ç”¨ä¸­ä»·å€¼æ˜¾è‘—",
                    "å·²æœ‰å¤šä¸ªæˆåŠŸåº”ç”¨æ¡ˆä¾‹",
                    "å•†ä¸šåŒ–å‰æ™¯å¹¿é˜”"
                ]
                analysis["concerns"] = [
                    "éƒ¨ç½²æˆæœ¬è¾ƒé«˜",
                    "æŠ€æœ¯é—¨æ§›é™åˆ¶æ™®åŠ"
                ]
                analysis["recommendations"] = [
                    "é™ä½æŠ€æœ¯ä½¿ç”¨é—¨æ§›",
                    "å¼€å‘æ›´å¤šåº”ç”¨åœºæ™¯"
                ]
            
            # æ·»åŠ è¯æ®
            for paper in literature_data["high_quality_papers"][:3]:
                analysis["evidence"].append(paper["title"])
            
            analysis_results.append(analysis)
        
        return analysis_results
    
    def _intelligent_debate(self, analysis_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ™ºèƒ½è¾©è®º"""
        # ç®€åŒ–çš„è¾©è®ºé€»è¾‘
        debate = {
            "participants": [a["agent"] for a in analysis_results],
            "key_conflicts": [],
            "consensus_points": [],
            "winning_perspective": "",
            "consensus_level": 0.75
        }
        
        # è¯†åˆ«è§‚ç‚¹å†²çª
        academic_view = analysis_results[0]["key_insights"]
        technical_view = analysis_results[1]["key_insights"]
        
        if "ç†è®ºåŸºç¡€éœ€è¦è¿›ä¸€æ­¥å¤¯å®" in str(academic_view) and "æŠ€æœ¯å®ç°ç›¸å¯¹æˆç†Ÿ" in str(technical_view):
            debate["key_conflicts"].append("ç†è®ºvså®è·µï¼šç†è®ºåŸºç¡€ä¸å®Œå–„ä½†æŠ€æœ¯å·²å¯ç”¨")
        
        # ç”Ÿæˆå…±è¯†ç‚¹
        debate["consensus_points"] = [
            f"{self.current_topic}å…·æœ‰é‡è¦ä»·å€¼",
            "éœ€è¦ç†è®ºå’ŒæŠ€æœ¯å¹¶é‡å‘å±•",
            "å®é™…åº”ç”¨éªŒè¯ç†è®ºå¯è¡Œæ€§"
        ]
        
        # ç¡®å®šèƒœå‡ºè§‚ç‚¹
        debate["winning_perspective"] = "ç†è®ºä¸å®è·µç»“åˆçš„å¹³è¡¡å‘å±•"
        
        return debate
    
    def _generate_wiki_content(self, debate_results: Dict[str, Any], 
                             literature_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”ŸæˆWikiå†…å®¹"""
        # æ ‡å‡†Wikiç»“æ„
        sections = [
            {
                "title": "æ¦‚è¿°",
                "content": f"{self.current_topic}æ˜¯ä¸€ä¸ªé‡è¦çš„æŠ€æœ¯/ç†è®ºæ¦‚å¿µï¼Œåœ¨ç›¸å…³é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨ä»·å€¼ã€‚æœ¬æ–‡å°†ä»å¤šä¸ªè§’åº¦å…¨é¢ä»‹ç»{self.current_topic}çš„ç›¸å…³å†…å®¹ã€‚"
            },
            {
                "title": "å†å²å‘å±•",
                "content": f"{self.current_topic}çš„å‘å±•ç»å†äº†å¤šä¸ªé‡è¦é˜¶æ®µã€‚ä»æœ€åˆçš„ç†è®ºæå‡ºåˆ°ç°åœ¨çš„å¹¿æ³›åº”ç”¨ï¼Œæ¯ä¸€æ­¥éƒ½å‡èšäº†ç ”ç©¶è€…çš„æ™ºæ…§ã€‚è¿‘å¹´æ¥ï¼Œéšç€è®¡ç®—èƒ½åŠ›çš„æå‡å’Œæ•°æ®é‡çš„å¢åŠ ï¼Œ{self.current_topic}è¿æ¥äº†å¿«é€Ÿå‘å±•ã€‚"
            },
            {
                "title": "æ ¸å¿ƒåŸç†",
                "content": f"{self.current_topic}çš„æ ¸å¿ƒåŸç†åŸºäº...[æ­¤å¤„åº”æœ‰è¯¦ç»†çš„æŠ€æœ¯åŸç†è¯´æ˜]ã€‚é€šè¿‡æ·±å…¥ç†è§£å…¶å·¥ä½œæœºåˆ¶ï¼Œå¯ä»¥æ›´å¥½åœ°æŠŠæ¡å…¶æœ¬è´¨ç‰¹å¾å’Œåº”ç”¨æ¡ä»¶ã€‚"
            },
            {
                "title": "æŠ€æœ¯å®ç°",
                "content": f"{self.current_topic}çš„æŠ€æœ¯å®ç°æ¶‰åŠå¤šä¸ªå…³é”®ç¯èŠ‚ã€‚ä¸»è¦åŒ…æ‹¬ç®—æ³•è®¾è®¡ã€ç³»ç»Ÿæ¶æ„ã€æ€§èƒ½ä¼˜åŒ–ç­‰æ–¹é¢ã€‚å½“å‰ä¸»æµçš„å®ç°æ–¹æ¡ˆå…·æœ‰...ç‰¹ç‚¹ã€‚"
            },
            {
                "title": "åº”ç”¨é¢†åŸŸ",
                "content": f"{self.current_topic}åœ¨å¤šä¸ªé¢†åŸŸéƒ½æœ‰é‡è¦åº”ç”¨ã€‚åœ¨å­¦æœ¯ç•Œã€å·¥ä¸šç•Œã€å•†ä¸šé¢†åŸŸéƒ½å±•ç°å‡ºäº†å·¨å¤§çš„ä»·å€¼ã€‚å…¸å‹åº”ç”¨åŒ…æ‹¬..."
            },
            {
                "title": "ä¼˜åŠ¿ä¸å±€é™",
                "content": f"{self.current_topic}å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œå¦‚...åŒæ—¶ä¹Ÿå­˜åœ¨ä¸€å®šçš„å±€é™æ€§ï¼Œå¦‚...ã€‚å®¢è§‚è®¤è¯†å…¶ä¼˜ç¼ºç‚¹æœ‰åŠ©äºåˆç†åº”ç”¨ã€‚"
            },
            {
                "title": "å‘å±•è¶‹åŠ¿",
                "content": f"å±•æœ›æœªæ¥ï¼Œ{self.current_topic}çš„å‘å±•è¶‹åŠ¿åŒ…æ‹¬...ã€‚éšç€æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥å’Œéœ€æ±‚çš„æŒç»­å¢é•¿ï¼Œ{self.current_topic}å°†åœ¨æ›´å¤šé¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚"
            },
            {
                "title": "å‚è€ƒæ–‡çŒ®",
                "content": self._generate_references(literature_data)
            }
        ]
        
        # è®¡ç®—å­—æ•°
        word_count = sum(len(s["content"]) for s in sections)
        
        return {
            "title": f"{self.current_topic} - æ™ºèƒ½ç™¾ç§‘",
            "sections": sections,
            "word_count": word_count,
            "creation_time": datetime.now().isoformat(),
            "debate_insights": debate_results["consensus_points"]
        }
    
    def _generate_references(self, literature_data: Dict[str, Any]) -> str:
        """ç”Ÿæˆå‚è€ƒæ–‡çŒ®"""
        papers = literature_data["high_quality_papers"]
        if not papers:
            return "æš‚æ— å‚è€ƒæ–‡çŒ®"
        
        refs = []
        for i, paper in enumerate(papers, 1):
            ref = f"{i}. {paper['title']}. {paper['authors'][0]} et al. {paper['venue']}, {paper['year']}."
            refs.append(ref)
        
        return "\n".join(refs)
    
    def _quality_control(self, wiki_content: Dict[str, Any]) -> Dict[str, Any]:
        """è´¨é‡æ§åˆ¶"""
        sections = wiki_content["sections"]
        
        # è´¨é‡è¯„åˆ†
        scores = {
            "structure": 0.9,  # ç»“æ„å®Œæ•´æ€§
            "depth": 0.8,     # å†…å®¹æ·±åº¦
            "evidence": 0.7,  # è¯æ®æ”¯æ’‘
            "clarity": 0.85   # è¡¨è¾¾æ¸…æ™°åº¦
        }
        
        # è®¡ç®—æ€»åˆ†
        overall_score = sum(scores.values()) / len(scores)
        
        # ç¡®å®šå¯ä¿¡åº¦ç­‰çº§
        if overall_score >= 0.85:
            credibility = "é«˜"
        elif overall_score >= 0.75:
            credibility = "ä¸­ç­‰"
        else:
            credibility = "éœ€æ”¹è¿›"
        
        return {
            "overall_score": overall_score,
            "credibility_level": credibility,
            "detailed_scores": scores,
            "recommendations": self._generate_quality_recommendations(scores)
        }
    
    def _generate_quality_recommendations(self, scores: Dict[str, float]) -> List[str]:
        """ç”Ÿæˆè´¨é‡æ”¹è¿›å»ºè®®"""
        recommendations = []
        
        for metric, score in scores.items():
            if score < 0.8:
                if metric == "structure":
                    recommendations.append("å®Œå–„å†…å®¹ç»“æ„ï¼Œå¢åŠ è¿‡æ¸¡æ®µè½")
                elif metric == "depth":
                    recommendations.append("å¢åŠ æ·±åº¦åˆ†æï¼Œæä¾›æ›´å¤šç»†èŠ‚")
                elif metric == "evidence":
                    recommendations.append("å¢åŠ æ›´å¤šè¯æ®æ”¯æ’‘ï¼Œå¼•ç”¨æƒå¨èµ„æ–™")
                elif metric == "clarity":
                    recommendations.append("ä¼˜åŒ–è¡¨è¾¾æ–¹å¼ï¼Œæé«˜å¯è¯»æ€§")
        
        return recommendations

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python intelligent_wiki_creator.py <ä¸»é¢˜>")
        print("ç¤ºä¾‹: python intelligent_wiki_creator.py æœºå™¨å­¦ä¹ ")
        return
    
    topic = sys.argv[1]
    
    # åˆ›å»ºæ™ºèƒ½Wikiåˆ›å»ºå™¨
    creator = IntelligentWikiCreator()
    
    # åˆ›å»ºWiki
    result = creator.create_wiki(topic)
    
    # ç”ŸæˆHTMLæ–‡ä»¶
    html_content = generate_html_wiki(result)
    html_file = f"{topic}_æ™ºèƒ½ç™¾ç§‘.html"
    
    with open(html_file, 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    print(f"\nğŸŒ HTMLç‰ˆWikiå·²ç”Ÿæˆ: {html_file}")

def generate_html_wiki(result: Dict[str, Any]) -> str:
    """ç”ŸæˆHTMLæ ¼å¼çš„Wiki"""
    wiki_content = result["wiki_content"]
    
    html = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{wiki_content['title']}</title>
    <style>
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f8f9fa;
        }}
        .container {{
            background: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }}
        h1 {{
            color: #2c3e50;
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
        }}
        h2 {{
            color: #34495e;
            margin-top: 30px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }}
        .meta {{
            color: #7f8c8d;
            font-size: 0.9em;
            margin-bottom: 20px;
        }}
        .section {{
            margin-bottom: 30px;
        }}
        .references {{
            background: #f8f9fa;
            padding: 20px;
            border-radius: 5px;
            font-size: 0.9em;
        }}
        .quality-badge {{
            background: #27ae60;
            color: white;
            padding: 5px 10px;
            border-radius: 15px;
            font-size: 0.8em;
            display: inline-block;
            margin-top: 10px;
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>{wiki_content['title']}</h1>
        <div class="meta">
            åˆ›å»ºæ—¶é—´: {wiki_content['creation_time']} | 
            æ€»å­—æ•°: {wiki_content['word_count']} å­— |
            <span class="quality-badge">æ™ºèƒ½ç”Ÿæˆ Â· è´¨é‡è¯„åˆ†: {result['quality_report']['overall_score']:.2f}</span>
        </div>
        
        {"".join([f'<div class="section"><h2>{section["title"]}</h2><p>{section["content"]}</p></div>' for section in wiki_content["sections"]])}
        
        <div class="references">
            <h2>è´¨é‡è¯„ä¼°</h2>
            <p>å¯ä¿¡åº¦ç­‰çº§: {result['quality_report']['credibility_level']}</p>
            <p>æ”¹è¿›å»ºè®®: {', '.join(result['quality_report']['recommendations']) if result['quality_report']['recommendations'] else 'æ— éœ€æ”¹è¿›'}</p>
        </div>
    </div>
</body>
</html>
    """
    
    return html

if __name__ == "__main__":
    main()