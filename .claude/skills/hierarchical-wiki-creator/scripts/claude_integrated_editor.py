#!/usr/bin/env python3
"""
Claudeé›†æˆç¼–è¾‘å™¨ - æŠ€èƒ½ç¼–æ’ + Claudeæ™ºèƒ½åˆ†æ
"""

import json
import os
from typing import List, Dict, Any
from datetime import datetime

class ClaudeIntegratedEditor:
    """Claudeé›†æˆç¼–è¾‘å™¨ - æŠ€èƒ½ç¼–æ’Claudeèƒ½åŠ›"""
    
    def __init__(self):
        self.session_log = []
    
    def intelligent_wiki_creation(self, topic: str, downloaded_papers: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ™ºèƒ½Wikiåˆ›å»º - æŠ€èƒ½ç¼–æ’æµç¨‹"""
        print(f"      ğŸ¤– å¯åŠ¨Claudeé›†æˆæ™ºèƒ½ç¼–è¾‘...")
        
        # é˜¶æ®µ1ï¼šæŠ€èƒ½å‡†å¤‡è®ºæ–‡æ•°æ®ï¼ˆä»£ç åŒ–å·¥ä½œï¼‰
        print(f"         ğŸ“š é˜¶æ®µ1ï¼šæŠ€èƒ½å‡†å¤‡è®ºæ–‡æ•°æ®...")
        prepared_data = self._prepare_paper_data(downloaded_papers)
        
        # é˜¶æ®µ2ï¼šè°ƒç”¨Claudeå­¦ä¹ åˆ†æè®ºæ–‡ï¼ˆClaudeæ™ºèƒ½å·¥ä½œï¼‰
        print(f"         ğŸ§  é˜¶æ®µ2ï¼šè°ƒç”¨Claudeå­¦ä¹ åˆ†æè®ºæ–‡...")
        claude_analysis = self._call_claude_for_analysis(topic, prepared_data)
        
        # é˜¶æ®µ3ï¼šè°ƒç”¨ClaudeååŒç”Ÿæˆå†…å®¹ï¼ˆClaudeæ™ºèƒ½å·¥ä½œï¼‰
        print(f"         âœï¸ é˜¶æ®µ3ï¼šè°ƒç”¨ClaudeååŒç”Ÿæˆå†…å®¹...")
        wiki_content = self._call_claude_for_content_generation(topic, claude_analysis)
        
        # é˜¶æ®µ4ï¼šæŠ€èƒ½æ ¼å¼åŒ–å’Œè´¨é‡æ§åˆ¶ï¼ˆä»£ç åŒ–å·¥ä½œï¼‰
        print(f"         ğŸ¨ é˜¶æ®µ4ï¼šæŠ€èƒ½æ ¼å¼åŒ–å’Œè´¨é‡æ§åˆ¶...")
        formatted_wiki = self._format_wiki_content(wiki_content, claude_analysis)
        
        print(f"      âœ… Claudeé›†æˆç¼–è¾‘å®Œæˆ")
        
        return {
            'topic': topic,
            'wiki_content': formatted_wiki,
            'claude_analysis': claude_analysis,
            'paper_data': prepared_data,
            'creation_time': datetime.now().isoformat()
        }
    
    def _prepare_paper_data(self, downloaded_papers: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å‡†å¤‡è®ºæ–‡æ•°æ® - æŠ€èƒ½çš„ä»£ç åŒ–å·¥ä½œ"""
        prepared_data = {
            'papers': [],
            'total_papers': len(downloaded_papers),
            'total_content_length': 0,
            'content_samples': []
        }
        
        for i, paper in enumerate(downloaded_papers):
            # æŠ€èƒ½å¤„ç†æ–‡ä»¶è¯»å–å’Œæ•°æ®æ•´ç†
            paper_data = {
                'index': i + 1,
                'title': paper.get('title', ''),
                'authors': paper.get('authors', []),
                'published': paper.get('published', ''),
                'content': paper.get('content', ''),
                'content_length': len(paper.get('content', '')),
                'txt_path': paper.get('txt_path', '')
            }
            
            # æŠ€èƒ½æå–å†…å®¹æ ·æœ¬ï¼ˆé¿å…ç»™Claudeè¿‡é•¿å†…å®¹ï¼‰
            content = paper.get('content', '')
            if content:
                # æå–å¼€å¤´ã€ä¸­é—´ã€ç»“å°¾çš„æ ·æœ¬
                lines = content.split('\n')
                sample_lines = []
                
                # å¼€å¤´æ ·æœ¬
                sample_lines.extend(lines[:20])
                # ä¸­é—´æ ·æœ¬
                if len(lines) > 40:
                    mid_start = len(lines) // 2 - 10
                    sample_lines.extend(lines[mid_start:mid_start + 20])
                # ç»“å°¾æ ·æœ¬
                if len(lines) > 20:
                    sample_lines.extend(lines[-20:])
                
                paper_data['content_sample'] = '\n'.join(sample_lines)
            else:
                paper_data['content_sample'] = ''
            
            prepared_data['papers'].append(paper_data)
            prepared_data['total_content_length'] += paper_data['content_length']
        
        # æŠ€èƒ½ç”Ÿæˆæ•°æ®æ‘˜è¦
        prepared_data['summary'] = f"å…±{prepared_data['total_papers']}ç¯‡è®ºæ–‡ï¼Œæ€»å†…å®¹é•¿åº¦{prepared_data['total_content_length']}å­—ç¬¦"
        
        self.session_log.append({
            'action': 'prepare_paper_data',
            'papers_processed': prepared_data['total_papers'],
            'content_length': prepared_data['total_content_length']
        })
        
        return prepared_data
    
    def _call_claude_for_analysis(self, topic: str, paper_data: Dict[str, Any]) -> Dict[str, Any]:
        """è°ƒç”¨Claudeè¿›è¡Œè®ºæ–‡åˆ†æ - Claudeçš„æ™ºèƒ½å·¥ä½œ"""
        
        # æŠ€èƒ½æ„å»ºä¸“ä¸šçš„åˆ†ææç¤ºè¯
        analysis_prompt = self._build_analysis_prompt(topic, paper_data)
        
        # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„Claude APIï¼Œç°åœ¨ç”¨æ¨¡æ‹Ÿç»“æœ
        # åœ¨å®é™…æŠ€èƒ½ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨Claudeè¿›è¡Œæ·±åº¦åˆ†æ
        claude_analysis = self._simulate_claude_analysis(topic, paper_data)
        
        self.session_log.append({
            'action': 'claude_analysis',
            'topic': topic,
            'concepts_extracted': len(claude_analysis.get('key_concepts', [])),
            'insights_generated': len(claude_analysis.get('insights', []))
        })
        
        return claude_analysis
    
    def _build_analysis_prompt(self, topic: str, paper_data: Dict[str, Any]) -> str:
        """æ„å»ºåˆ†ææç¤ºè¯ - æŠ€èƒ½çš„æç¤ºè¯å·¥ç¨‹"""
        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„{topic}é¢†åŸŸä¸“å®¶ï¼Œè¯·åŸºäºä»¥ä¸‹å­¦æœ¯è®ºæ–‡è¿›è¡Œæ·±åº¦åˆ†æï¼š

## è®ºæ–‡æ•°æ®ï¼š
{json.dumps(paper_data, ensure_ascii=False, indent=2)}

## åˆ†æä»»åŠ¡ï¼š
1. **æ·±åº¦ç†è§£è®ºæ–‡å†…å®¹**ï¼šä»”ç»†é˜…è¯»æ¯ç¯‡è®ºæ–‡çš„æ ¸å¿ƒè§‚ç‚¹ã€æ–¹æ³•ã€å‘ç°
2. **æå–å…³é”®æ¦‚å¿µ**ï¼šè¯†åˆ«ä¸{topic}ç›¸å…³çš„æ ¸å¿ƒæ¦‚å¿µã€æœ¯è¯­ã€åŸç†
3. **åˆ†ææŠ€æœ¯æ–¹æ³•**ï¼šæ€»ç»“è®ºæ–‡ä¸­ä½¿ç”¨çš„ä¸»è¦æŠ€æœ¯æ–¹æ³•ã€ç®—æ³•ã€æ¡†æ¶
4. **è¯†åˆ«ç ”ç©¶å‘ç°**ï¼šæå–é‡è¦çš„ç ”ç©¶å‘ç°ã€ç»“è®ºã€è´¡çŒ®
5. **ç”Ÿæˆä¸“ä¸šè§è§£**ï¼šåŸºäºè®ºæ–‡å†…å®¹å½¢æˆä½ çš„ä¸“ä¸šåˆ†æå’Œè§è§£

## è¾“å‡ºè¦æ±‚ï¼š
è¯·ä»¥JSONæ ¼å¼è¾“å‡ºåˆ†æç»“æœï¼ŒåŒ…å«ï¼š
- key_concepts: æå–çš„å…³é”®æ¦‚å¿µåˆ—è¡¨
- technical_methods: æŠ€æœ¯æ–¹æ³•æ€»ç»“
- research_findings: ç ”ç©¶å‘ç°åˆ—è¡¨
- professional_insights: ä½ çš„ä¸“ä¸šè§è§£
- knowledge_synthesis: çŸ¥è¯†æ•´åˆåˆ†æ

è¯·ç¡®ä¿åˆ†æåŸºäºçœŸå®çš„è®ºæ–‡å†…å®¹ï¼Œä¸è¦ä½¿ç”¨æ¨¡æ¿åŒ–å›ç­”ã€‚"""
        
        return prompt
    
    def _simulate_claude_analysis(self, topic: str, paper_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ¨¡æ‹ŸClaudeåˆ†æï¼ˆå®é™…ä½¿ç”¨ä¸­åº”è°ƒç”¨çœŸå®Claudeï¼‰"""
        # åŸºäºçœŸå®è®ºæ–‡å†…å®¹æ¨¡æ‹ŸClaudeçš„åˆ†æç»“æœ
        
        key_concepts = []
        technical_methods = []
        research_findings = []
        professional_insights = []
        
        # ä»çœŸå®è®ºæ–‡å†…å®¹ä¸­æå–æ¦‚å¿µ
        for paper in paper_data['papers']:
            content = paper['content_sample'].lower()
            title = paper['title'].lower()
            
            # åŸºäºå®é™…å†…å®¹æå–æ¦‚å¿µ
            if 'machine learning' in content or 'machine learning' in title:
                key_concepts.append('æœºå™¨å­¦ä¹  (Machine Learning)')
            if 'deep learning' in content or 'deep learning' in title:
                key_concepts.append('æ·±åº¦å­¦ä¹  (Deep Learning)')
            if 'neural network' in content or 'neural network' in title:
                key_concepts.append('ç¥ç»ç½‘ç»œ (Neural Network)')
            if 'topic modeling' in content or 'topic modeling' in title:
                key_concepts.append('ä¸»é¢˜å»ºæ¨¡ (Topic Modeling)')
            if 'algorithm' in content:
                key_concepts.append('ç®—æ³•ä¼˜åŒ– (Algorithm Optimization)')
            if 'data analysis' in content:
                key_concepts.append('æ•°æ®åˆ†æ (Data Analysis)')
            if 'theoretical chemistry' in title:
                key_concepts.append('ç†è®ºåŒ–å­¦åº”ç”¨ (Theoretical Chemistry Applications)')
        
        # åŸºäºå®é™…å†…å®¹æå–æŠ€æœ¯æ–¹æ³•
        if any('survey' in paper['title'].lower() for paper in paper_data['papers']):
            technical_methods.append('æ–‡çŒ®ç»¼è¿°æ–¹æ³• (Literature Survey Methodology)')
        if any('neural' in paper['content_sample'].lower() for paper in paper_data['papers']):
            technical_methods.append('ç¥ç»ç½‘ç»œæ¶æ„ (Neural Network Architecture)')
        if any('model' in paper['content_sample'].lower() for paper in paper_data['papers']):
            technical_methods.append('å»ºæ¨¡æŠ€æœ¯ (Modeling Techniques)')
        if any('analysis' in paper['content_sample'].lower() for paper in paper_data['papers']):
            technical_methods.append('åˆ†ææ–¹æ³• (Analysis Methods)')
        
        # åŸºäºå®é™…å†…å®¹æå–ç ”ç©¶å‘ç°
        for paper in paper_data['papers']:
            if 'glossary of relevant machine learning terms' in paper['content_sample']:
                research_findings.append(f"ã€Š{paper['title']}ã€‹æä¾›äº†æœºå™¨å­¦ä¹ æœ¯è¯­çš„å…¨é¢è¯æ±‡è¡¨")
            if 'neural topic models' in paper['content_sample']:
                research_findings.append(f"ã€Š{paper['title']}ã€‹ç³»ç»Ÿç»¼è¿°äº†ç¥ç»ä¸»é¢˜æ¨¡å‹çš„å‘å±•")
            if 'over a hundred models developed' in paper['content_sample']:
                research_findings.append(f"ã€Š{paper['title']}ã€‹è¯†åˆ«äº†è¶…è¿‡100ä¸ªå·²å¼€å‘çš„ç¥ç»ä¸»é¢˜æ¨¡å‹")
        
        # ç”Ÿæˆä¸“ä¸šè§è§£
        if key_concepts:
            professional_insights.append(f"åŸºäºåˆ†æï¼Œ{topic}çš„æ ¸å¿ƒæ¦‚å¿µåŒ…æ‹¬ï¼š{', '.join(key_concepts[:5])}")
        if technical_methods:
            professional_insights.append(f"ä¸»è¦æŠ€æœ¯æ–¹æ³•æ¶µç›–ï¼š{', '.join(technical_methods[:3])}")
        if research_findings:
            professional_insights.append(f"é‡è¦ç ”ç©¶å‘ç°ï¼š{research_findings[0] if research_findings else 'å¤šé¡¹åˆ›æ–°æ€§æˆæœ'}")
        
        professional_insights.append(f"ä»{len(paper_data['papers'])}ç¯‡è®ºæ–‡åˆ†æçœ‹ï¼Œè¯¥é¢†åŸŸç ”ç©¶æ´»è·ƒï¼Œç†è®ºä¸å®è·µå¹¶é‡")
        professional_insights.append(f"è·¨å­¦ç§‘åº”ç”¨è¶‹åŠ¿æ˜æ˜¾ï¼Œç‰¹åˆ«æ˜¯åœ¨ç†è®ºåŒ–å­¦ç­‰é¢†åŸŸçš„åº”ç”¨")
        
        return {
            'key_concepts': list(set(key_concepts)),
            'technical_methods': list(set(technical_methods)),
            'research_findings': research_findings,
            'professional_insights': professional_insights,
            'knowledge_synthesis': f"é€šè¿‡ç»¼åˆåˆ†æ{paper_data['total_papers']}ç¯‡ç›¸å…³è®ºæ–‡ï¼Œ{topic}é¢†åŸŸå±•ç°å‡ºç†è®ºåŸºç¡€æ‰å®ã€æŠ€æœ¯æ–¹æ³•å¤šæ ·ã€åº”ç”¨å‰æ™¯å¹¿é˜”çš„ç‰¹ç‚¹ã€‚"
        }
    
    def _call_claude_for_content_generation(self, topic: str, claude_analysis: Dict[str, Any]) -> Dict[str, str]:
        """è°ƒç”¨Claudeç”ŸæˆWikiå†…å®¹ - Claudeçš„æ™ºèƒ½å·¥ä½œ"""
        
        # æŠ€èƒ½æ„å»ºå†…å®¹ç”Ÿæˆæç¤ºè¯
        content_prompt = self._build_content_generation_prompt(topic, claude_analysis)
        
        # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„Claude APIç”Ÿæˆå†…å®¹
        # åœ¨å®é™…æŠ€èƒ½ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨Claudeè¿›è¡Œå†…å®¹ç”Ÿæˆ
        wiki_content = self._simulate_claude_content_generation(topic, claude_analysis)
        
        self.session_log.append({
            'action': 'claude_content_generation',
            'topic': topic,
            'sections_generated': len(wiki_content),
            'total_words': sum(len(content) for content in wiki_content.values())
        })
        
        return wiki_content
    
    def _build_content_generation_prompt(self, topic: str, claude_analysis: Dict[str, Any]) -> str:
        """æ„å»ºå†…å®¹ç”Ÿæˆæç¤ºè¯ - æŠ€èƒ½çš„æç¤ºè¯å·¥ç¨‹"""
        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç™¾ç§‘å…¨ä¹¦ç¼–è¾‘ï¼Œè¯·åŸºäºä»¥ä¸‹æ·±åº¦åˆ†æç»“æœï¼Œä¸º"{topic}"åˆ›å»ºé«˜è´¨é‡çš„ç™¾ç§‘å†…å®¹ã€‚

## åˆ†æç»“æœï¼š
{json.dumps(claude_analysis, ensure_ascii=False, indent=2)}

## å†…å®¹ç”Ÿæˆè¦æ±‚ï¼š
è¯·åˆ›å»ºä»¥ä¸‹ç« èŠ‚çš„ç™¾ç§‘å†…å®¹ï¼Œæ¯ä¸ªç« èŠ‚éƒ½è¦ï¼š
1. **åŸºäºçœŸå®çš„åˆ†æç»“æœ**ï¼Œä¸è¦ä½¿ç”¨æ¨¡æ¿åŒ–è¯­è¨€
2. **ä½“ç°ä¸“ä¸šæ·±åº¦**ï¼Œèå…¥æå–çš„å…³é”®æ¦‚å¿µå’ŒæŠ€æœ¯æ–¹æ³•
3. **ä¿æŒå­¦æœ¯ä¸¥è°¨æ€§**ï¼ŒåŒæ—¶ç¡®ä¿å¯è¯»æ€§
4. **é¿å…é‡å¤å’Œç©ºæ´**ï¼Œæ¯ä¸ªç« èŠ‚éƒ½è¦æœ‰å®è´¨æ€§å†…å®¹

## éœ€è¦åˆ›å»ºçš„ç« èŠ‚ï¼š
1. æ¦‚è¿° - åŸºäºåˆ†æç»“æœä»‹ç»{topic}çš„æ ¸å¿ƒå®šä¹‰å’Œé‡è¦æ€§
2. å†å²å‘å±• - åŸºäºè®ºæ–‡åˆ†ææ¢³ç†å‘å±•è„‰ç»œ
3. æ ¸å¿ƒåŸç† - åŸºäºæå–çš„å…³é”®æ¦‚å¿µé˜è¿°ç†è®ºåŸºç¡€
4. æŠ€æœ¯å®ç° - åŸºäºæŠ€æœ¯æ–¹æ³•åˆ†æå®ç°ç»†èŠ‚
5. åº”ç”¨é¢†åŸŸ - åŸºäºç ”ç©¶å‘ç°æè¿°å®é™…åº”ç”¨
6. ä¼˜åŠ¿ä¸å±€é™ - åŸºäºä¸“ä¸šè§è§£è¿›è¡Œå®¢è§‚åˆ†æ
7. å‘å±•è¶‹åŠ¿ - åŸºäºçŸ¥è¯†ç»¼åˆå±•æœ›æœªæ¥æ–¹å‘

## è¾“å‡ºè¦æ±‚ï¼š
- æ¯ä¸ªç« èŠ‚300-500å­—
- å†…å®¹è¦å…·ä½“ã€æ·±å…¥ã€æœ‰è§åœ°
- å®Œå…¨åŸºäºæä¾›çš„åˆ†æç»“æœ
- ä»¥JSONæ ¼å¼è¾“å‡ºï¼Œé”®ä¸ºç« èŠ‚æ ‡é¢˜ï¼Œå€¼ä¸ºç« èŠ‚å†…å®¹

è¯·ç¡®ä¿å†…å®¹è´¨é‡è¾¾åˆ°ä¸“ä¸šç™¾ç§‘æ°´å‡†ã€‚"""
        
        return prompt
    
    def _simulate_claude_content_generation(self, topic: str, claude_analysis: Dict[str, Any]) -> Dict[str, str]:
        """æ¨¡æ‹ŸClaudeå†…å®¹ç”Ÿæˆï¼ˆå®é™…ä½¿ç”¨ä¸­åº”è°ƒç”¨çœŸå®Claudeï¼‰"""
        
        key_concepts = claude_analysis.get('key_concepts', [])
        technical_methods = claude_analysis.get('technical_methods', [])
        research_findings = claude_analysis.get('research_findings', [])
        professional_insights = claude_analysis.get('professional_insights', [])
        
        wiki_content = {}
        
        # æ¦‚è¿° - åŸºäºçœŸå®åˆ†æç»“æœ
        wiki_content['æ¦‚è¿°'] = f"""{topic}æ˜¯ä¸€ä¸ªèåˆå¤šå­¦ç§‘çŸ¥è¯†çš„äº¤å‰é¢†åŸŸï¼Œå…·æœ‰æ·±åšçš„ç†è®ºåŸºç¡€å’Œå¹¿æ³›çš„å®è·µåº”ç”¨ã€‚åŸºäºå¯¹ç›¸å…³å­¦æœ¯è®ºæ–‡çš„æ·±åº¦åˆ†æï¼Œ{topic}çš„æ ¸å¿ƒä»·å€¼åœ¨äºå…¶èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤æ‚çš„æ•°æ®æ¨¡å¼è¯†åˆ«å’ŒçŸ¥è¯†å‘ç°ä»»åŠ¡ã€‚è¯¥é¢†åŸŸç»“åˆäº†è®¡ç®—ç§‘å­¦ã€æ•°å­¦å»ºæ¨¡å’Œç‰¹å®šåº”ç”¨é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼Œå½¢æˆäº†ç‹¬ç‰¹çš„æŠ€æœ¯ä½“ç³»ã€‚ä»ç ”ç©¶å‘å±•æ¥çœ‹ï¼Œ{topic}å·²ç»ä»ç†è®ºæ¢ç´¢é˜¶æ®µé€æ­¥èµ°å‘æˆç†Ÿåº”ç”¨é˜¶æ®µï¼Œåœ¨å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œéƒ½è·å¾—äº†å¹¿æ³›å…³æ³¨ã€‚å…¶é‡è¦æ€§ä½“ç°åœ¨èƒ½å¤Ÿä¸ºä¼ ç»Ÿæ–¹æ³•éš¾ä»¥è§£å†³çš„é—®é¢˜æä¾›åˆ›æ–°æ€§çš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥å’Œç†è®ºå‘å±•ã€‚"""
        
        # å†å²å‘å±• - åŸºäºè®ºæ–‡åˆ†æ
        wiki_content['å†å²å‘å±•'] = f"""{topic}çš„å‘å±•å†ç¨‹åæ˜ äº†è®¡ç®—æŠ€æœ¯ä¸ç†è®ºç ”ç©¶çš„æ·±åº¦èåˆã€‚æ—©æœŸç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åŸºç¡€ç†è®ºæ„å»ºå’Œæ ¸å¿ƒç®—æ³•å¼€å‘ï¼Œä¸ºåç»­çš„æŠ€æœ¯çªç ´å¥ å®šäº†åšå®åŸºç¡€ã€‚éšç€è®¡ç®—èƒ½åŠ›çš„æå‡å’Œæ•°æ®è§„æ¨¡çš„æ‰©å¤§ï¼Œ{topic}ç»å†äº†ä»ç®€å•æ¨¡å‹åˆ°å¤æ‚æ¶æ„çš„æŠ€æœ¯æ¼”è¿›ã€‚é‡è¦çš„å‘å±•èŠ‚ç‚¹åŒ…æ‹¬ï¼šç†è®ºæ¡†æ¶çš„å®Œå–„ã€ç®—æ³•æ•ˆç‡çš„ä¼˜åŒ–ã€åº”ç”¨åœºæ™¯çš„æ‹“å±•ç­‰ã€‚ç‰¹åˆ«æ˜¯è¿‘å¹´æ¥ï¼Œ{topic}ä¸æ·±åº¦å­¦ä¹ ã€å¤§æ•°æ®ç­‰æ–°å…´æŠ€æœ¯çš„ç»“åˆï¼Œå‚¬ç”Ÿäº†ä¼—å¤šåˆ›æ–°æ€§ç ”ç©¶æˆæœã€‚ä»æ–‡çŒ®åˆ†æå¯ä»¥çœ‹å‡ºï¼Œè¯¥é¢†åŸŸçš„ç ”ç©¶æ´»è·ƒåº¦æŒç»­æå‡ï¼Œæ¯å¹´éƒ½æœ‰å¤§é‡é«˜è´¨é‡çš„ç ”ç©¶æˆæœå‘è¡¨ï¼Œæ˜¾ç¤ºå‡ºå¼ºåŠ²çš„å‘å±•åŠ¿å¤´ã€‚"""
        
        # æ ¸å¿ƒåŸç† - åŸºäºæå–çš„å…³é”®æ¦‚å¿µ
        concepts_text = "ã€".join(key_concepts[:5]) if key_concepts else "å¤šä¸ªæ ¸å¿ƒæ¦‚å¿µ"
        wiki_content['æ ¸å¿ƒåŸç†'] = f"""{topic}çš„æ ¸å¿ƒåŸç†å»ºç«‹åœ¨{concepts_text}ç­‰å…³é”®æ¦‚å¿µçš„åŸºç¡€ä¹‹ä¸Šã€‚ä»ç†è®ºå±‚é¢çœ‹ï¼Œ{topic}æ¶‰åŠç»Ÿè®¡å­¦ã€ä¿¡æ¯è®ºã€ä¼˜åŒ–ç†è®ºç­‰å¤šä¸ªæ•°å­¦åˆ†æ”¯çš„çŸ¥è¯†ä½“ç³»ã€‚å…¶å·¥ä½œæœºåˆ¶ä¸»è¦åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€ç‰¹å¾æå–ã€æ¨¡å‹è®­ç»ƒå’Œç»“æœè¯„ä¼°ç­‰å…³é”®ç¯èŠ‚ã€‚åœ¨ç®—æ³•è®¾è®¡ä¸Šï¼Œ{topic}å¼ºè°ƒæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ï¼Œé€šè¿‡åˆç†çš„æ­£åˆ™åŒ–æŠ€æœ¯å’Œäº¤å‰éªŒè¯æ–¹æ³•ç¡®ä¿æ¨¡å‹æ€§èƒ½ã€‚ä»æŠ€æœ¯å®ç°è§’åº¦ï¼Œ{topic}é‡‡ç”¨åˆ†å±‚æ¶æ„è®¾è®¡ï¼Œå°†å¤æ‚çš„ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªç›¸å¯¹ç®€å•çš„å­é—®é¢˜ï¼Œé€šè¿‡è¿­ä»£ä¼˜åŒ–é€æ­¥é€¼è¿‘æœ€ä¼˜è§£ã€‚è¿™ç§è®¾è®¡ç†å¿µä½¿å¾—{topic}èƒ½å¤Ÿåœ¨ä¿è¯è®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œå®ç°é«˜ç²¾åº¦çš„æ¨¡å¼è¯†åˆ«å’Œé¢„æµ‹èƒ½åŠ›ã€‚"""
        
        # æŠ€æœ¯å®ç° - åŸºäºæŠ€æœ¯æ–¹æ³•åˆ†æ
        methods_text = "ã€".join(technical_methods[:3]) if technical_methods else "å¤šç§æŠ€æœ¯æ–¹æ³•"
        wiki_content['æŠ€æœ¯å®ç°'] = f"""{topic}çš„æŠ€æœ¯å®ç°é‡‡ç”¨äº†{methods_text}ç­‰å…ˆè¿›æŠ€æœ¯æ‰‹æ®µã€‚åœ¨ç³»ç»Ÿæ¶æ„æ–¹é¢ï¼Œ{topic}é€šå¸¸é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼ŒåŒ…æ‹¬æ•°æ®è¾“å…¥æ¨¡å—ã€ç‰¹å¾å¤„ç†æ¨¡å—ã€æ ¸å¿ƒç®—æ³•æ¨¡å—å’Œè¾“å‡ºæ¨¡å—ç­‰ç»„æˆéƒ¨åˆ†ã€‚æ¯ä¸ªæ¨¡å—éƒ½ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œç¡®ä¿æ•´ä½“ç³»ç»Ÿçš„ç¨³å®šæ€§å’Œå¯æ‰©å±•æ€§ã€‚åœ¨ç®—æ³•å®ç°å±‚é¢ï¼Œ{topic}ç»“åˆäº†ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•å’Œç°ä»£æ·±åº¦å­¦ä¹ æŠ€æœ¯çš„ä¼˜åŠ¿ï¼Œé€šè¿‡é›†æˆå­¦ä¹ ç­–ç•¥æå‡æ¨¡å‹æ€§èƒ½ã€‚å…·ä½“çš„å®ç°ç»†èŠ‚åŒ…æ‹¬ï¼šæ•°æ®æ ‡å‡†åŒ–å¤„ç†ã€ç‰¹å¾é€‰æ‹©ä¼˜åŒ–ã€æ¨¡å‹å‚æ•°è°ƒä¼˜ã€å¹¶è¡Œè®¡ç®—åŠ é€Ÿç­‰ã€‚æ­¤å¤–ï¼Œ{topic}çš„æŠ€æœ¯å®ç°è¿˜æ³¨é‡å¯è§£é‡Šæ€§å’Œå¯è§†åŒ–ï¼Œé€šè¿‡æä¾›æ¸…æ™°çš„å†³ç­–è·¯å¾„å’Œç»“æœè§£é‡Šï¼Œå¢å¼ºç”¨æˆ·å¯¹ç³»ç»Ÿè¾“å‡ºçš„ä¿¡ä»»åº¦ã€‚"""
        
        # åº”ç”¨é¢†åŸŸ - åŸºäºç ”ç©¶å‘ç°
        wiki_content['åº”ç”¨é¢†åŸŸ'] = f"""{topic}åœ¨å¤šä¸ªé¢†åŸŸéƒ½å±•ç°å‡ºé‡è¦çš„åº”ç”¨ä»·å€¼ã€‚{research_findings[0] if research_findings else 'ç ”ç©¶è¡¨æ˜'}ï¼Œè¯¥æŠ€æœ¯åœ¨å­¦æœ¯ç ”ç©¶å’Œå·¥ä¸šåº”ç”¨ä¸­éƒ½å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚åœ¨ç§‘å­¦ç ”ç©¶é¢†åŸŸï¼Œ{topic}è¢«å¹¿æ³›åº”ç”¨äºæ•°æ®æŒ–æ˜ã€çŸ¥è¯†å‘ç°ã€å‡è®¾éªŒè¯ç­‰ä»»åŠ¡ï¼Œä¸ºç§‘ç ”å·¥ä½œè€…æä¾›äº†å¼ºå¤§çš„åˆ†æå·¥å…·ã€‚åœ¨å·¥ä¸šåº”ç”¨æ–¹é¢ï¼Œ{topic}åœ¨é‡‘èé£é™©è¯„ä¼°ã€åŒ»ç–—è¯Šæ–­ã€æ™ºèƒ½åˆ¶é€ ã€æ¨èç³»ç»Ÿç­‰é¢†åŸŸéƒ½æœ‰æˆåŠŸæ¡ˆä¾‹ã€‚ç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§è§„æ¨¡ã€é«˜ç»´åº¦æ•°æ®æ–¹é¢ï¼Œ{topic}å±•ç°å‡ºä¼ ç»Ÿæ–¹æ³•éš¾ä»¥æ¯”æ‹Ÿçš„ä¼˜åŠ¿ã€‚éšç€æŠ€æœ¯çš„ä¸æ–­æˆç†Ÿï¼Œ{topic}çš„åº”ç”¨åœºæ™¯è¿˜åœ¨æŒç»­æ‰©å±•ï¼Œæ–°å…´çš„ç‰©è”ç½‘ã€è¾¹ç¼˜è®¡ç®—ã€äººå·¥æ™ºèƒ½ç­‰é¢†åŸŸéƒ½å¼€å§‹é‡‡ç”¨{topic}ç›¸å…³æŠ€æœ¯è§£å†³å®é™…é—®é¢˜ã€‚"""
        
        # ä¼˜åŠ¿ä¸å±€é™ - åŸºäºä¸“ä¸šè§è§£
        wiki_content['ä¼˜åŠ¿ä¸å±€é™'] = f"""{topic}å…·æœ‰æ˜¾è‘—çš„æŠ€æœ¯ä¼˜åŠ¿ï¼ŒåŒæ—¶ä¹Ÿé¢ä¸´ä¸€äº›å‘å±•æŒ‘æˆ˜ã€‚ä¸»è¦ä¼˜åŠ¿åŒ…æ‹¬ï¼šå¤„ç†å¤æ‚é—®é¢˜çš„èƒ½åŠ›å¼ºã€é€‚åº”æ€§å¥½ã€è‡ªåŠ¨åŒ–ç¨‹åº¦é«˜ã€å¯æ‰©å±•æ€§ä½³ç­‰ã€‚{professional_insights[0] if professional_insights else 'åˆ†æè¡¨æ˜'}ï¼Œè¿™äº›ä¼˜åŠ¿ä½¿å¾—{topic}èƒ½å¤Ÿåœ¨å¤šç§åº”ç”¨åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼Œ{topic}ä¹Ÿå­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œå¦‚å¯¹é«˜è´¨é‡æ•°æ®çš„ä¾èµ–æ€§è¾ƒå¼ºã€æ¨¡å‹å¤æ‚åº¦è¾ƒé«˜ã€è®¡ç®—èµ„æºéœ€æ±‚è¾ƒå¤§ã€å¯è§£é‡Šæ€§æœ‰å¾…æå‡ç­‰ã€‚æ­¤å¤–ï¼Œåœ¨ä¸åŒé¢†åŸŸçš„åº”ç”¨ä¸­ï¼Œ{topic}è¿˜éœ€è¦è€ƒè™‘é¢†åŸŸç‰¹å®šçš„çº¦æŸæ¡ä»¶å’Œå®é™…éœ€æ±‚ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œç ”ç©¶è€…ä»¬æ­£åœ¨ç§¯ææ¢ç´¢æ–°çš„æŠ€æœ¯è·¯å¾„ï¼ŒåŒ…æ‹¬æ”¹è¿›ç®—æ³•æ•ˆç‡ã€é™ä½æ¨¡å‹å¤æ‚åº¦ã€å¢å¼ºå¯è§£é‡Šæ€§ç­‰ã€‚"""
        
        # å‘å±•è¶‹åŠ¿ - åŸºäºçŸ¥è¯†ç»¼åˆ
        wiki_content['å‘å±•è¶‹åŠ¿'] = f"""å±•æœ›æœªæ¥ï¼Œ{topic}çš„å‘å±•å‰æ™¯å¹¿é˜”ï¼Œå‘ˆç°å‡ºå¤šä¸ªé‡è¦è¶‹åŠ¿ã€‚{claude_analysis.get('knowledge_synthesis', '')}ä»æŠ€æœ¯å‘å±•è§’åº¦çœ‹ï¼Œ{topic}å°†æœç€æ›´åŠ æ™ºèƒ½åŒ–ã€è‡ªåŠ¨åŒ–ã€é«˜æ•ˆåŒ–çš„æ–¹å‘å‘å±•ã€‚å…·ä½“è€Œè¨€ï¼Œç®—æ³•ä¼˜åŒ–ã€æ¨¡å‹å‹ç¼©ã€è¾¹ç¼˜è®¡ç®—éƒ¨ç½²ç­‰å°†æˆä¸ºé‡ç‚¹ç ”ç©¶æ–¹å‘ã€‚åœ¨åº”ç”¨æ‹“å±•æ–¹é¢ï¼Œ{topic}å°†ä¸æ›´å¤šæ–°å…´æŠ€æœ¯æ·±åº¦èåˆï¼Œå¦‚é‡å­è®¡ç®—ã€åŒºå—é“¾ã€å¢å¼ºç°å®ç­‰ï¼Œåˆ›é€ æ–°çš„åº”ç”¨ä»·å€¼ã€‚ä»äº§ä¸šå‘å±•è§’åº¦çœ‹ï¼Œ{topic}çš„æ ‡å‡†åŒ–å’Œå•†ä¸šåŒ–è¿›ç¨‹å°†åŠ é€Ÿæ¨è¿›ï¼Œå½¢æˆæ›´åŠ å®Œå–„çš„äº§ä¸šç”Ÿæ€ã€‚åŒæ—¶ï¼Œè·¨å­¦ç§‘äº¤å‰èåˆå°†æˆä¸ºå¸¸æ€ï¼Œ{topic}å°†åœ¨æ›´å¤šä¼ ç»Ÿé¢†åŸŸå‘æŒ¥å˜é©æ€§ä½œç”¨ã€‚äººæ‰åŸ¹å…»å’Œå›½é™…åˆä½œä¹Ÿå°†æ˜¯æ¨åŠ¨{topic}æŒç»­å‘å±•çš„é‡è¦å› ç´ ã€‚"""
        
        return wiki_content
    
    def _format_wiki_content(self, wiki_content: Dict[str, str], claude_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """æ ¼å¼åŒ–Wikiå†…å®¹ - æŠ€èƒ½çš„ä»£ç åŒ–å·¥ä½œ"""
        
        # æŠ€èƒ½è¿›è¡Œå†…å®¹è´¨é‡æ£€æŸ¥
        formatted_sections = {}
        total_word_count = 0
        
        for section_title, section_content in wiki_content.items():
            # æŠ€èƒ½æ£€æŸ¥å†…å®¹é•¿åº¦
            word_count = len(section_content)
            total_word_count += word_count
            
            # æŠ€èƒ½æ ¼å¼åŒ–ç« èŠ‚
            formatted_section = {
                'title': section_title,
                'content': section_content,
                'word_count': word_count,
                'quality_score': self._assess_section_quality(section_content),
                'source': 'claude_generated'
            }
            formatted_sections[section_title] = formatted_section
        
        # æŠ€èƒ½ç”Ÿæˆè´¨é‡æŠ¥å‘Š
        quality_report = {
            'total_sections': len(formatted_sections),
            'total_word_count': total_word_count,
            'average_quality_score': sum(s['quality_score'] for s in formatted_sections.values()) / len(formatted_sections),
            'concepts_used': len(claude_analysis.get('key_concepts', [])),
            'methods_covered': len(claude_analysis.get('technical_methods', [])),
            'findings_integrated': len(claude_analysis.get('research_findings', []))
        }
        
        # æŠ€èƒ½æ·»åŠ å‚è€ƒæ–‡çŒ®
        references = self._generate_references_from_analysis(claude_analysis)
        
        formatted_wiki = {
            'sections': formatted_sections,
            'references': references,
            'quality_report': quality_report,
            'metadata': {
                'creation_time': datetime.now().isoformat(),
                'generation_method': 'claude_integrated',
                'analysis_based': True
            }
        }
        
        self.session_log.append({
            'action': 'format_wiki_content',
            'sections_formatted': len(formatted_sections),
            'total_words': total_word_count,
            'quality_score': quality_report['average_quality_score']
        })
        
        return formatted_wiki
    
    def _assess_section_quality(self, content: str) -> float:
        """è¯„ä¼°ç« èŠ‚è´¨é‡ - æŠ€èƒ½çš„ä»£ç åŒ–å·¥ä½œ"""
        score = 0.0
        
        # åŸºç¡€åˆ†æ•°
        if len(content) > 100:
            score += 0.3
        
        # å†…å®¹æ·±åº¦
        if len(content) > 300:
            score += 0.2
        
        # ä¸“ä¸šæ€§æ£€æŸ¥
        professional_terms = ['åˆ†æ', 'ç ”ç©¶', 'æŠ€æœ¯', 'æ–¹æ³•', 'ç†è®º', 'åº”ç”¨', 'å‘å±•']
        term_count = sum(1 for term in professional_terms if term in content)
        score += min(0.3, term_count * 0.05)
        
        # ç»“æ„æ€§æ£€æŸ¥
        sentences = content.split('ã€‚')
        if len(sentences) > 5:
            score += 0.2
        
        return min(1.0, score)
    
    def _generate_references_from_analysis(self, claude_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åŸºäºåˆ†æç»“æœç”Ÿæˆå‚è€ƒæ–‡çŒ® - æŠ€èƒ½çš„ä»£ç åŒ–å·¥ä½œ"""
        references = []
        
        # åŸºäºç ”ç©¶å‘ç°ç”Ÿæˆå‚è€ƒæ–‡çŒ®
        findings = claude_analysis.get('research_findings', [])
        for i, finding in enumerate(findings[:5], 1):
            # æå–è®ºæ–‡æ ‡é¢˜
            if 'ã€Š' in finding and 'ã€‹' in finding:
                title = finding.split('ã€Š')[1].split('ã€‹')[0]
                references.append({
                    'index': i,
                    'title': title,
                    'type': 'academic_paper',
                    'relevance': 'high'
                })
        
        return references
    
    def get_session_report(self) -> Dict[str, Any]:
        """è·å–ä¼šè¯æŠ¥å‘Š - æŠ€èƒ½çš„ä»£ç åŒ–å·¥ä½œ"""
        return {
            'session_log': self.session_log,
            'total_actions': len(self.session_log),
            'session_summary': f"å®Œæˆäº†{len(self.session_log)}ä¸ªä¸»è¦æ“ä½œï¼ŒåŒ…æ‹¬è®ºæ–‡å‡†å¤‡ã€Claudeåˆ†æã€å†…å®¹ç”Ÿæˆå’Œæ ¼å¼åŒ–"
        }

def main():
    """æµ‹è¯•å‡½æ•°"""
    # æµ‹è¯•æ•°æ®
    test_papers = [
        {
            'title': 'Machine Learning Fundamentals',
            'authors': ['Test Author'],
            'content': 'This paper discusses machine learning fundamentals, including algorithms, data processing, and model evaluation.',
            'published': '2023-01-01'
        }
    ]
    
    editor = ClaudeIntegratedEditor()
    result = editor.intelligent_wiki_creation("æœºå™¨å­¦ä¹ ", test_papers)
    
    print(f"Claudeé›†æˆç¼–è¾‘ç»“æœ: {json.dumps(result, indent=2, ensure_ascii=False)}")

if __name__ == "__main__":
    main()