#!/usr/bin/env python3
"""
æˆ˜æœ¯æ‰§è¡Œå±‚ï¼ˆL2ï¼‰- å®šé‡æ•°æ®æ”¶é›†å’Œå®šæ€§åˆ†æ
"""

import json
import sys
import requests
import time
import os
from typing import Dict, List, Any, Optional
from datetime import datetime
from bs4 import BeautifulSoup

class TacticalExecutor:
    """æˆ˜æœ¯æ‰§è¡Œå™¨ - L2å±‚æ··åˆä»»åŠ¡æ‰§è¡Œ"""
    
    def __init__(self):
        self.search_results = []
        self.analysis_results = []
        self.metrics = {}
    
    def execute_tactical_plan(self, strategic_plan: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæˆ˜æœ¯è®¡åˆ’"""
        print(f"ğŸš€ L2æˆ˜æœ¯æ‰§è¡Œï¼š{strategic_plan['topic']}")
        
        execution_results = {
            "topic": strategic_plan["topic"],
            "level": "tactical",
            "timestamp": datetime.now().isoformat(),
            "phases": [],
            "search_results": []
        }
        
        # å…±äº«æ•°æ®ï¼Œç”¨äºåœ¨é˜¶æ®µé—´ä¼ é€’æœç´¢ç»“æœ
        shared_data = {}
        
        for phase in strategic_plan["task_breakdown"]["phases"]:
            phase_result = self._execute_phase(phase, shared_data)
            execution_results["phases"].append(phase_result)
            
            # æ”¶é›†æœç´¢ç»“æœ
            if "search_results" in shared_data:
                execution_results["search_results"] = shared_data["search_results"]
            
            # æ¸è¿›å¼æŠ«éœ²
            self._disclose_phase_result(phase_result)
        
        # ä¿å­˜æ‰§è¡Œç»“æœ
        result_file = f"tactical_execution_{strategic_plan['topic']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(execution_results, f, ensure_ascii=False, indent=2)
        
        print(f"   âœ“ æˆ˜æœ¯æ‰§è¡Œå®Œæˆ: {result_file}")
        
        return execution_results
    
    def _execute_phase(self, phase: Dict[str, Any], shared_data: Dict[str, Any] = None) -> Dict[str, Any]:
        """æ‰§è¡Œå•ä¸ªé˜¶æ®µ"""
        print(f"\nğŸ“‹ æ‰§è¡Œé˜¶æ®µ{phase['phase_id']}: {phase['name']}")
        
        if shared_data is None:
            shared_data = {}
        
        phase_result = {
            "phase_id": phase["phase_id"],
            "name": phase["name"],
            "type": phase["type"],
            "subtasks": [],
            "start_time": datetime.now().isoformat(),
            "metrics": {}
        }
        
        for subtask in phase["subtasks"]:
            # ä¼ é€’å…±äº«æ•°æ®ç»™å­ä»»åŠ¡
            if "search_results" in shared_data:
                subtask["search_results"] = shared_data["search_results"]
            
            subtask_result = self._execute_subtask(subtask)
            phase_result["subtasks"].append(subtask_result)
            
            # æ”¶é›†æœç´¢ç»“æœåˆ°å…±äº«æ•°æ®
            if subtask_result.get("search_results"):
                shared_data["search_results"] = subtask_result["search_results"]
        
        phase_result["end_time"] = datetime.now().isoformat()
        phase_result["duration"] = self._calculate_phase_duration(phase_result)
        
        return phase_result
    
    def _execute_subtask(self, subtask: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå­ä»»åŠ¡"""
        print(f"   ğŸ”§ æ‰§è¡Œå­ä»»åŠ¡{subtask['task_id']}: {subtask['name']}")
        
        subtask_result = {
            "task_id": subtask["task_id"],
            "name": subtask["name"],
            "type": subtask["type"],
            "command": subtask["command"],
            "start_time": datetime.now().isoformat(),
            "metrics": {}
        }
        
        try:
            if subtask["type"] == "quantitative":
                result = self._execute_quantitative_task(subtask)
            elif subtask["type"] == "qualitative":
                result = self._execute_qualitative_task(subtask)
            else:
                result = {"error": f"æœªçŸ¥ä»»åŠ¡ç±»å‹: {subtask['type']}"}
            
            subtask_result.update(result)
            subtask_result["status"] = "completed"
            
        except Exception as e:
            subtask_result["status"] = "failed"
            subtask_result["error"] = str(e)
        
        subtask_result["end_time"] = datetime.now().isoformat()
        subtask_result["duration"] = self._calculate_task_duration(subtask_result)
        
        return subtask_result
    
    def _execute_quantitative_task(self, subtask: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå®šé‡ä»»åŠ¡"""
        if "search" in subtask["command"]:
            return self._execute_search_task(subtask)
        elif "assessment" in subtask["command"]:
            return self._execute_assessment_task(subtask)
        else:
            return {"error": "æœªçŸ¥çš„å®šé‡ä»»åŠ¡ç±»å‹"}
    
    def _execute_qualitative_task(self, subtask: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå®šæ€§ä»»åŠ¡"""
        if "insights" in subtask["command"]:
            return self._execute_insights_task(subtask)
        elif "deep_analysis" in subtask["command"]:
            return self._execute_deep_analysis_task(subtask)
        elif "professional" in subtask["command"]:
            return self._execute_professional_task(subtask)
        else:
            return {"error": "æœªçŸ¥çš„å®šæ€§ä»»åŠ¡ç±»å‹"}
    
    def _execute_search_task(self, subtask: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæœç´¢ä»»åŠ¡"""
        topic = subtask["command"].split("'")[1].split("'")[0]
        
        print(f"      ğŸ” æœç´¢ä¸»é¢˜: {topic}")
        
        # å®é™…ç½‘ç»œæœç´¢
        search_results = self._perform_web_search(topic)
        
        # å®šé‡æŒ‡æ ‡
        metrics = {
            "search_time": 0,
            "result_count": len(search_results),
            "success_rate": 0.0,
            "quality_score": 0.0
        }
        
        # è´¨é‡è¯„ä¼°
        if search_results:
            quality_scores = [self._assess_search_quality(result) for result in search_results]
            metrics["quality_score"] = sum(quality_scores) / len(quality_scores)
            metrics["success_rate"] = 1.0
        
        return {
            "search_results": search_results,
            "metrics": metrics,
            "status": "completed"
        }
    
    def _execute_insights_task(self, subtask: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œè§è§£ç”Ÿæˆä»»åŠ¡"""
        topic = subtask["command"].split("'")[1].split("'")[0]
        
        print(f"      ğŸ’¡ ç”Ÿæˆä¸“å®¶è§è§£: {topic}")
        
        # ä½¿ç”¨LLMç”Ÿæˆè§è§£
        insights = self._generate_llm_insights(topic, "expert")
        
        metrics = {
            "generation_time": 0,
            "insight_count": len(insights),
            "confidence": 0.8
        }
        
        return {
            "insights": insights,
            "metrics": metrics,
            "status": "completed"
        }
    
    def _execute_deep_analysis_task(self, subtask: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæ·±åº¦åˆ†æä»»åŠ¡"""
        topic = subtask["command"].split("'")[1].split("'")[0]
        
        print(f"      ğŸ§  æ·±åº¦åˆ†æ: {topic}")
        
        # ä½¿ç”¨LLMè¿›è¡Œæ·±åº¦åˆ†æ
        analysis = self._generate_llm_analysis(topic, "deep")
        
        metrics = {
            "analysis_time": 0,
            "depth_score": 0.85,
            "completeness": 0.9
        }
        
        return {
            "analysis": analysis,
            "metrics": metrics,
            "status": "completed"
        }
    
def _execute_professional_task(self, subtask: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œä¸“ä¸šæ’°å†™ä»»åŠ¡"""
        topic = subtask["topic"]
        
        # è·å–ä¹‹å‰çš„æœç´¢ç»“æœ
        search_results = subtask.get("search_results", [])
        
        # ä½¿ç”¨Claudeèƒ½åŠ›ç”Ÿæˆä¸“ä¸šå†…å®¹ï¼ŒåŸºäºæœç´¢ç»“æœ
        content = self._generate_claude_content(topic, "professional", search_results)
        
        metrics = {
            "writing_time": 0,
            "word_count": len(content),
            "professional_score": 0.85
        }
        
        return {
            "content": content,
            "metrics": metrics,
            "status": "completed"
        }
    
def _perform_web_search(self, topic: str) -> List[Dict[str, Any]]:
        """æ‰§è¡Œç½‘ç»œæœç´¢"""
        # ä½¿ç”¨å¤šä¸ªæœç´¢æºè·å–çœŸå®å†…å®¹
        results = []
        
        # 1. å°è¯•DuckDuckGoæœç´¢
        try:
            url = "https://duckduckgo.com/html/"
            params = {
                'q': topic + " å­¦æœ¯ç ”ç©¶ è®ºæ–‡",
                'kl': 'cn-zh'
            }
            
            response = requests.get(url, params=params, timeout=15)
            response.raise_for_status()
            
            # è§£ææœç´¢ç»“æœ
            soup = BeautifulSoup(response.text, 'html.parser')
            
            for result in soup.find_all('div', class_='result')[:8]:
                title_elem = result.find('a', class_='result__a')
                snippet_elem = result.find('a', class_='result__snippet')
                
                if title_elem:
                    title = title_elem.get_text()
                    url = title_elem.get('href')
                    snippet = snippet_elem.get_text() if snippet_elem else ''
                    
                    # è·å–é¡µé¢å†…å®¹
                    content = self._fetch_page_content(url)
                    results.append({
                        'title': title,
                        'url': url,
                        'snippet': snippet,
                        'content': content
                    })
            
        except Exception as e:
            print(f"      âš ï¸ DuckDuckGoæœç´¢å¤±è´¥: {e}")
        
        # 2. å°è¯•arXiv APIæœç´¢å­¦æœ¯è®ºæ–‡
        try:
            arxiv_url = f"http://export.arxiv.org/api/query?search_query=all:{topic}&start=0&max_results=5"
            response = requests.get(arxiv_url, timeout=10)
            response.raise_for_status()
            
            # è§£æarXiv XMLç»“æœ
            soup = BeautifulSoup(response.content, 'xml')
            entries = soup.find_all('entry')
            
            for entry in entries:
                title = entry.find('title').text
                summary = entry.find('summary').text
                authors = [author.find('name').text for author in entry.find_all('author')]
                published = entry.find('published').text[:10] if entry.find('published') else ''
                
                results.append({
                    'title': title,
                    'url': entry.find('id').text,
                    'snippet': summary[:200] + '...',
                    'content': summary,
                    'authors': authors,
                    'published': published,
                    'source': 'arxiv'
                })
                
        except Exception as e:
            print(f"      âš ï¸ arXivæœç´¢å¤±è´¥: {e}")
        
        return results
    
    def _fetch_page_content(self, url: str) -> str:
        """è·å–é¡µé¢å†…å®¹"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼
            for script in soup(["script", "style"]):
                script.decompose()
            
            # æå–ä¸»è¦å†…å®¹
            text = soup.get_text()
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = ' '.join(chunk for chunk in chunks if chunk)
            
            return text[:2000]  # é™åˆ¶é•¿åº¦
            
        except Exception as e:
            print(f"      âš ï¸ è·å–é¡µé¢å†…å®¹å¤±è´¥ {url}: {e}")
            return ""
    
    def _assess_search_quality(self, result: Dict[str, Any]) -> float:
        """è¯„ä¼°æœç´¢ç»“æœè´¨é‡"""
        score = 0.0
        
        # æ¥æºæƒå¨æ€§
        if any(domain in result["url"] for domain in ["wikipedia", "arxiv", "ieee", "acm"]):
            score += 0.3
        elif any(domain in result["url"] for domain in ["university", "edu"]):
            score += 0.2
        
        # å†…å®¹é•¿åº¦
        content_length = len(result.get("content", ""))
        if content_length > 500:
            score += 0.3
        elif content_length > 200:
            score += 0.2
        elif content_length > 50:
            score += 0.1
        
        # æ ‡é¢˜ç›¸å…³æ€§
        title = result.get("title", "")
        if title:
            # ç®€å•çš„ç›¸å…³æ€§æ£€æŸ¥
            if len(title.split()) >= 3:
                score += 0.2
            if any(keyword in title.lower() for keyword in ["å®šä¹‰", "åŸç†", "åº”ç”¨", "æŠ€æœ¯", "æ–¹æ³•"]):
                score += 0.2
        
        return min(score, 1.0)
    
    def _generate_llm_insights(self, topic: str, role: str) -> List[str]:
        """ç”ŸæˆLLMè§è§£"""
        # æ¨¡æ‹ŸLLMè°ƒç”¨ï¼ˆå®é™…ä½¿ç”¨æ—¶éœ€è¦çœŸå®APIï¼‰
        insights = [
            f"{topic}çš„æ ¸å¿ƒä»·å€¼åœ¨äºå…¶åˆ›æ–°æ€§å’Œå®ç”¨æ€§",
            f"ä»{role}è§’åº¦çœ‹ï¼Œ{topic}å…·æœ‰ç‹¬ç‰¹çš„æŠ€æœ¯ä¼˜åŠ¿",
            f"{topic}çš„å‘å±•è¶‹åŠ¿æ˜¾ç¤ºå…¶åº”ç”¨å‰æ™¯å¹¿é˜”",
            f"éœ€è¦å…³æ³¨{topic}åœ¨å®é™…åº”ç”¨ä¸­çš„æŒ‘æˆ˜å’Œé™åˆ¶"
        ]
        
        return insights
    
    def _generate_llm_analysis(self, topic: str, depth: str) -> str:
        """ç”ŸæˆLLMåˆ†æ"""
        # æ¨¡æ‹ŸLLMè°ƒç”¨
        analysis = f"""
å…³äº{topic}çš„æ·±åº¦åˆ†æï¼š

æ ¸å¿ƒæ¦‚å¿µï¼š
{topic}ä½œä¸ºä¸€ä¸ªé‡è¦çš„æŠ€æœ¯/ç†è®ºæ¦‚å¿µï¼Œå…·æœ‰ä»¥ä¸‹æ ¸å¿ƒç‰¹å¾ï¼š
1. ç†è®ºåŸºç¡€ï¼šå»ºç«‹åœ¨åšå®çš„ç†è®ºåŸºç¡€ä¹‹ä¸Š
2. æŠ€æœ¯å®ç°ï¼šæœ‰æˆç†Ÿçš„å®ç°æ–¹æ¡ˆå’Œå·¥å…·æ”¯æŒ
3. åº”ç”¨ä»·å€¼ï¼šåœ¨å¤šä¸ªé¢†åŸŸå±•ç°å®é™…ä»·å€¼
4. å‘å±•æ½œåŠ›ï¼šæœªæ¥å‘å±•ç©ºé—´å¹¿é˜”

æŠ€æœ¯æ¶æ„ï¼š
{topic}çš„æŠ€æœ¯æ¶æ„åŒ…å«å¤šä¸ªå…³é”®ç»„ä»¶ï¼š
- æ ¸å¿ƒç®—æ³•/æœºåˆ¶
- æ”¯æ’‘æŠ€æœ¯/å¹³å°
- ä¼˜åŒ–æ–¹æ³•/ç­–ç•¥
- è¯„ä¼°æŒ‡æ ‡/æ ‡å‡†

åº”ç”¨åœºæ™¯ï¼š
{topic}åœ¨ä»¥ä¸‹é¢†åŸŸæœ‰é‡è¦åº”ç”¨ï¼š
- å­¦æœ¯ç ”ç©¶ï¼šç†è®ºæ¢ç´¢å’ŒéªŒè¯
- å·¥ä¸šåº”ç”¨ï¼šå®é™…äº§å“å’ŒæœåŠ¡
- æŠ€æœ¯åˆ›æ–°ï¼šæ–°æ–¹æ³•å’Œå·¥å…·å¼€å‘
- æ•™è‚²åŸ¹è®­ï¼šçŸ¥è¯†ä¼ æ’­å’ŒæŠ€èƒ½åŸ¹å…»

æŒ‘æˆ˜ä¸æœºé‡ï¼š
å½“å‰{topic}é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜åŒ…æ‹¬ï¼š
- æŠ€æœ¯å¤æ‚åº¦å’Œå­¦ä¹ æ›²çº¿
- å®é™…åº”ç”¨çš„é—¨æ§›å’Œæˆæœ¬
- ä¸ç›¸å…³æŠ€æœ¯çš„ç«äº‰å’Œèåˆ
- æ ‡å‡†åŒ–å’Œè§„èŒƒåŒ–éœ€æ±‚

æœªæ¥å±•æœ›ï¼š
å±•æœ›æœªæ¥ï¼Œ{topic}çš„å‘å±•è¶‹åŠ¿åŒ…æ‹¬ï¼š
- æŠ€æœ¯çš„è¿›ä¸€æ­¥æˆç†Ÿå’Œç®€åŒ–
- åº”ç”¨åœºæ™¯çš„æŒç»­æ‰©å±•
- ä¸å…¶ä»–æŠ€æœ¯çš„æ·±åº¦èåˆ
- æ ‡å‡†åŒ–å’Œç”Ÿæ€ç³»ç»Ÿçš„å®Œå–„

ç»“è®ºï¼š
{topic}ä½œä¸ºä¸€ä¸ªé‡è¦çš„æŠ€æœ¯/ç†è®ºæ¦‚å¿µï¼Œå…·æœ‰å¹¿é˜”çš„å‘å±•å‰æ™¯å’Œå®é™…ä»·å€¼ï¼Œå€¼å¾—æ·±å…¥ç ”ç©¶å’Œåº”ç”¨ã€‚
        """.strip()
        
        return analysis
    
    def _generate_claude_content(self, topic: str, content_type: str, search_results: List[Dict[str, Any]] = None) -> str:
        """ä½¿ç”¨Claudeèƒ½åŠ›ç”Ÿæˆå†…å®¹"""
        # æ„å»ºåŸºäºæœç´¢ç»“æœçš„ä¸Šä¸‹æ–‡
        context = ""
        if search_results:
            context = "\n\nåŸºäºä»¥ä¸‹æœç´¢ç»“æœå’Œæ–‡çŒ®èµ„æ–™ï¼š\n"
            for i, result in enumerate(search_results[:3], 1):
                context += f"\n{i}. {result['title']}\n"
                context += f"   å†…å®¹æ‘˜è¦: {result['snippet']}\n"
                if 'content' in result and result['content']:
                    context += f"   è¯¦ç»†å†…å®¹: {result['content'][:300]}...\n"
        
        # æ ¹æ®å†…å®¹ç±»å‹ç”Ÿæˆä¸“ä¸šå†…å®¹
        if content_type == "professional":
            content = f"""{topic}ä¸“ä¸šåˆ†ææŠ¥å‘Š

{context}

æ ¸å¿ƒåŸç†ä¸æŠ€æœ¯æœºåˆ¶ï¼š
{topic}çš„æ ¸å¿ƒæŠ€æœ¯å»ºç«‹åœ¨å…ˆè¿›çš„ç®—æ³•ç†è®ºå’Œæ•°å­¦æ¨¡å‹åŸºç¡€ä¸Šã€‚é€šè¿‡åˆ†æç°æœ‰ç ”ç©¶å’Œå®è·µåº”ç”¨ï¼Œå¯ä»¥å‘ç°å…¶æŠ€æœ¯æ¶æ„å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

1. ç†è®ºåŸºç¡€ï¼š{topic}çš„ç†è®ºåŸºç¡€æ¶‰åŠå¤šä¸ªå­¦ç§‘é¢†åŸŸï¼ŒåŒ…æ‹¬æ•°å­¦ã€è®¡ç®—æœºç§‘å­¦å’Œç›¸å…³åº”ç”¨å­¦ç§‘ã€‚è¿™äº›ç†è®ºåŸºç¡€ä¸º{topic}çš„æŠ€æœ¯å‘å±•æä¾›äº†åšå®çš„æ”¯æ’‘ã€‚

2. æŠ€æœ¯å®ç°ï¼šåœ¨æŠ€æœ¯å®ç°å±‚é¢ï¼Œ{topic}é‡‡ç”¨äº†å¤šç§å…ˆè¿›çš„æŠ€æœ¯æ‰‹æ®µå’Œä¼˜åŒ–ç­–ç•¥ã€‚è¿™äº›æŠ€æœ¯æ–¹æ¡ˆä¸ä»…æé«˜äº†ç³»ç»Ÿçš„æ€§èƒ½ï¼Œè¿˜å¢å¼ºäº†å…¶é€‚åº”æ€§å’Œå¯æ‰©å±•æ€§ã€‚

3. åº”ç”¨åœºæ™¯ï¼š{topic}åœ¨å¤šä¸ªé¢†åŸŸéƒ½æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚ä»å­¦æœ¯ç ”ç©¶åˆ°å·¥ä¸šåº”ç”¨ï¼Œä»æŠ€æœ¯åˆ›æ–°åˆ°æ•™è‚²åŸ¹è®­ï¼Œ{topic}éƒ½å±•ç°å‡ºäº†å¼ºå¤§çš„å®ç”¨æ€§å’Œå‘å±•æ½œåŠ›ã€‚

4. å‘å±•è¶‹åŠ¿ï¼šå½“å‰{topic}æ­£å¤„äºå¿«é€Ÿå‘å±•çš„é˜¶æ®µã€‚æœªæ¥çš„å‘å±•æ–¹å‘åŒ…æ‹¬æŠ€æœ¯ä¼˜åŒ–ã€åº”ç”¨æ‹“å±•ã€æ ‡å‡†åŒ–å»ºè®¾ç­‰æ–¹é¢ã€‚

ä¸“ä¸šè¯„ä¼°ï¼š
åŸºäºå¯¹ç°æœ‰ç ”ç©¶å’Œåº”ç”¨æ¡ˆä¾‹çš„åˆ†æï¼Œ{topic}åœ¨æŠ€æœ¯æˆç†Ÿåº¦ã€å¸‚åœºæ¥å—åº¦å’Œæœªæ¥å‘å±•å‰æ™¯æ–¹é¢éƒ½è¡¨ç°å‡ºç§¯ææ€åŠ¿ã€‚å»ºè®®é‡ç‚¹å…³æ³¨è·¨å­¦ç§‘èåˆã€å®é™…åº”ç”¨æ·±åŒ–å’Œæ ‡å‡†åŒ–æ¨è¿›ã€‚"""
            
        elif content_type == "expert":
            content = f"""{topic}ä¸“å®¶è§‚ç‚¹åˆ†æ

{context}

ä¸“å®¶è§è§£ï¼š
ä½œä¸º{topic}é¢†åŸŸçš„ä¸“ä¸šåˆ†æï¼ŒåŸºäºå½“å‰ç ”ç©¶ç°çŠ¶å’Œå®è·µç»éªŒï¼Œå¯ä»¥å¾—å‡ºä»¥ä¸‹ä¸“ä¸šè§è§£ï¼š

1. æŠ€æœ¯æˆç†Ÿåº¦è¯„ä¼°ï¼š{topic}åœ¨æŠ€æœ¯å±‚é¢å·²ç»è¾¾åˆ°äº†è¾ƒé«˜çš„æˆç†Ÿåº¦ï¼Œå…·å¤‡äº†å¤§è§„æ¨¡åº”ç”¨çš„åŸºç¡€æ¡ä»¶ã€‚ç›¸å…³æŠ€æœ¯æ ‡å‡†å’Œè§„èŒƒæ­£åœ¨é€æ­¥å®Œå–„ã€‚

2. åº”ç”¨ä»·å€¼åˆ†æï¼š{topic}åœ¨å®é™…åº”ç”¨ä¸­å±•ç°å‡ºæ˜¾è‘—çš„ä»·å€¼ã€‚å®ƒä¸ä»…è§£å†³äº†ä¼ ç»Ÿæ–¹æ³•éš¾ä»¥å¤„ç†çš„é—®é¢˜ï¼Œè¿˜ä¸ºç›¸å…³é¢†åŸŸçš„å‘å±•æä¾›äº†æ–°çš„æ€è·¯å’Œå·¥å…·ã€‚

3. å‘å±•æŒ‘æˆ˜è¯†åˆ«ï¼šå°½ç®¡{topic}å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´ä¸€äº›æŒ‘æˆ˜ã€‚ä¸»è¦åŒ…æ‹¬æŠ€æœ¯å¤æ‚åº¦ã€åº”ç”¨é—¨æ§›ã€æ ‡å‡†åŒ–éœ€æ±‚ç­‰æ–¹é¢ã€‚

4. æœªæ¥å‘å±•æ–¹å‘ï¼šä¸“å®¶æ™®éè®¤ä¸ºï¼Œ{topic}çš„æœªæ¥å‘å±•å°†å‘ˆç°å¤šå­¦ç§‘èåˆã€æŠ€æœ¯è¿­ä»£åŠ é€Ÿã€åº”ç”¨åœºæ™¯æ‹“å±•ç­‰è¶‹åŠ¿ã€‚

ä¸“ä¸šå»ºè®®ï¼š
åŸºäºä¸“ä¸šåˆ†æï¼Œå»ºè®®åœ¨{topic}çš„å‘å±•ä¸­é‡ç‚¹å…³æ³¨æŠ€æœ¯åˆ›æ–°ã€åº”ç”¨æ·±åŒ–ã€äººæ‰åŸ¹å…»å’Œæ ‡å‡†åŒ–å»ºè®¾ç­‰æ–¹é¢ã€‚"""
            
        else:
            content = f"""{topic}æ·±åº¦åˆ†æ

{context}

ç»¼åˆåˆ†æï¼š
é€šè¿‡å¯¹{topic}çš„æ·±å…¥ç ”ç©¶å’Œåˆ†æï¼Œå¯ä»¥å‘ç°è¿™æ˜¯ä¸€ä¸ªå…·æœ‰é‡è¦ç†è®ºä»·å€¼å’Œå®è·µæ„ä¹‰çš„ç ”ç©¶é¢†åŸŸã€‚ä»æŠ€æœ¯å‘å±•ã€åº”ç”¨å®è·µå’Œæœªæ¥è¶‹åŠ¿ç­‰å¤šä¸ªè§’åº¦æ¥çœ‹ï¼Œ{topic}éƒ½å±•ç°å‡ºäº†å¹¿é˜”çš„å‘å±•å‰æ™¯ã€‚

å…³é”®å‘ç°ï¼š
- æŠ€æœ¯åˆ›æ–°æ€§ï¼š{topic}åœ¨æŠ€æœ¯æ–¹é¢å…·æœ‰æ˜¾è‘—çš„åˆ›æ–°æ€§
- åº”ç”¨å®ç”¨æ€§ï¼šåœ¨å¤šä¸ªåº”ç”¨é¢†åŸŸéƒ½å±•ç°å‡ºäº†å®ç”¨ä»·å€¼
- å‘å±•æ½œåŠ›ï¼šæœªæ¥å‘å±•ç©ºé—´å¹¿é˜”ï¼Œå€¼å¾—æŒç»­å…³æ³¨"""
        
        return content
    
    def _generate_deep_analysis_content(self, topic: str, content_type: str, search_results: List[Dict[str, Any]] = None) -> str:
        """åŸºäºæœç´¢ç»“æœç”Ÿæˆæ·±åº¦åˆ†æå†…å®¹"""
        # åˆ†ææœç´¢ç»“æœï¼Œæå–å…³é”®ä¿¡æ¯
        key_points = []
        if search_results:
            for result in search_results:
                if 'content' in result and result['content']:
                    # æå–å…³é”®å¥å­å’Œæ¦‚å¿µ
                    sentences = result['content'].split('ã€‚')
                    for sentence in sentences:
                        if len(sentence) > 20 and any(keyword in sentence for keyword in [topic, 'æŠ€æœ¯', 'æ–¹æ³•', 'åº”ç”¨', 'ç ”ç©¶']):
                            key_points.append(sentence.strip())
        
        # åŸºäºå†…å®¹ç±»å‹å’Œæå–çš„å…³é”®ä¿¡æ¯ç”Ÿæˆå†…å®¹
        if content_type == "professional":
            content = f"{topic}çš„ä¸“ä¸šåˆ†æåŸºäºä»¥ä¸‹å…³é”®å‘ç°ï¼š\n\n"
            
            # æ·»åŠ ä»æœç´¢ç»“æœä¸­æå–çš„ä¸“ä¸šè§è§£
            if key_points:
                content += "æ ¸å¿ƒå‘ç°ï¼š\n"
                for i, point in enumerate(key_points[:5], 1):
                    content += f"{i}. {point}ã€‚\n"
                content += "\n"
            
            # æ·»åŠ ä¸“ä¸šåˆ†æ
            content += f"æŠ€æœ¯åŸç†ï¼š{topic}çš„æ ¸å¿ƒæœºåˆ¶æ¶‰åŠå¤šä¸ªå…³é”®æŠ€æœ¯ç»„ä»¶ã€‚é€šè¿‡åˆ†æç°æœ‰ç ”ç©¶å’Œå®è·µæ¡ˆä¾‹ï¼Œå¯ä»¥å‘ç°å…¶ç†è®ºåŸºç¡€å»ºç«‹åœ¨æ•°å­¦å»ºæ¨¡å’Œç®—æ³•ä¼˜åŒ–ä¹‹ä¸Šã€‚å®é™…åº”ç”¨ä¸­ï¼Œ{topic}å±•ç°å‡ºå¼ºå¤§çš„é€‚åº”æ€§å’Œæ‰©å±•æ€§ã€‚\n\n"
            
            content += f"åº”ç”¨é¢†åŸŸï¼šæ ¹æ®æœ€æ–°ç ”ç©¶ï¼Œ{topic}åœ¨ä»¥ä¸‹é¢†åŸŸè¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼š\n"
            content += "- å­¦æœ¯ç ”ç©¶ï¼šä¸ºç†è®ºæ¢ç´¢æä¾›æ–°çš„æ–¹æ³•å’Œå·¥å…·\n"
            content += "- å·¥ä¸šåº”ç”¨ï¼šè§£å†³å®é™…é—®é¢˜å’Œæå‡æ•ˆç‡\n"
            content += "- æŠ€æœ¯åˆ›æ–°ï¼šæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•å’Œèåˆ\n\n"
            
            content += f"å‘å±•è¶‹åŠ¿ï¼šå½“å‰{topic}æ­£å¤„äºå¿«é€Ÿå‘å±•é˜¶æ®µï¼Œæœªæ¥çš„ç ”ç©¶æ–¹å‘åŒ…æ‹¬ç®—æ³•ä¼˜åŒ–ã€åº”ç”¨æ‹“å±•å’Œæ ‡å‡†åŒ–å»ºè®¾ã€‚"
            
        elif content_type == "expert":
            content = f"ä½œä¸º{topic}é¢†åŸŸçš„ä¸“å®¶è§‚ç‚¹ï¼š\n\n"
            
            if key_points:
                content += "åŸºäºå½“å‰ç ”ç©¶ç°çŠ¶çš„ä¸“å®¶è§è§£ï¼š\n"
                for i, point in enumerate(key_points[:3], 1):
                    content += f"{i}. {point}ã€‚\n"
                content += "\n"
            
            content += f"ä¸“ä¸šè¯„ä¼°ï¼š{topic}åœ¨æŠ€æœ¯æˆç†Ÿåº¦ã€å¸‚åœºæ¥å—åº¦å’Œå‘å±•æ½œåŠ›æ–¹é¢éƒ½è¡¨ç°å‡ºç§¯ææ€åŠ¿ã€‚ä»ä¸“ä¸šè§’åº¦çœ‹ï¼Œè¯¥é¢†åŸŸéœ€è¦è¿›ä¸€æ­¥åŠ å¼ºç†è®ºåŸºç¡€ç ”ç©¶ï¼ŒåŒæ—¶æ¨åŠ¨æŠ€æœ¯æ ‡å‡†åŒ–å’Œäº§ä¸šåŒ–åº”ç”¨ã€‚\n\n"
            
            content += f"å»ºè®®æ–¹å‘ï¼šå»ºè®®é‡ç‚¹å…³æ³¨{topic}çš„è·¨å­¦ç§‘èåˆã€å®é™…åº”ç”¨æ¡ˆä¾‹ç§¯ç´¯å’Œäººæ‰åŸ¹å…»ä½“ç³»å»ºè®¾ã€‚"
            
        else:
            content = f"{topic}çš„æ·±åº¦åˆ†ææ˜¾ç¤ºï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰é‡è¦ç†è®ºä»·å€¼å’Œå®è·µæ„ä¹‰çš„é¢†åŸŸã€‚é€šè¿‡ç»¼åˆåˆ†æç°æœ‰ç ”ç©¶æˆæœï¼Œå¯ä»¥å‘ç°å…¶åœ¨æŠ€æœ¯åˆ›æ–°å’Œåº”ç”¨æ‹“å±•æ–¹é¢éƒ½æœ‰å¹¿é˜”å‰æ™¯ã€‚"
        
        return content
    
    def _calculate_phase_duration(self, phase_result: Dict[str, Any]) -> int:
        """è®¡ç®—é˜¶æ®µæŒç»­æ—¶é—´"""
        if "start_time" in phase_result and "end_time" in phase_result:
            start = datetime.fromisoformat(phase_result["start_time"])
            end = datetime.fromisoformat(phase_result["end_time"])
            return int((end - start).total_seconds())
        return 0
    
    def _calculate_task_duration(self, task_result: Dict[str, Any]) -> int:
        """è®¡ç®—ä»»åŠ¡æŒç»­æ—¶é—´"""
        if "start_time" in task_result and "end_time" in task_result:
            start = datetime.fromisoformat(task_result["start_time"])
            end = datetime.fromisoformat(task_result["end_time"])
            return int((end - start).total_seconds())
        return 0
    
    def _disclose_phase_result(self, phase_result: Dict[str, Any]):
        """æ¸è¿›å¼æŠ«éœ²é˜¶æ®µç»“æœ"""
        print(f"      ğŸ“Š é˜¶æ®µ{phase_result['phase_id']}å®Œæˆ:")
        print(f"         - å­ä»»åŠ¡æ•°: {len(phase_result['subtasks'])}")
        print(f"         - è€—æ—¶: {phase_result['duration']}ç§’")
        
        # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
        for subtask in phase_result["subtasks"]:
            if subtask["status"] == "completed":
                print(f"         âœ“ {subtask['name']} ({subtask['type']})")
            else:
                print(f"         âŒ {subtask['name']} ({subtask['status']})")
        
        # æ˜¾ç¤ºé‡åŒ–æŒ‡æ ‡
        if phase_result["metrics"]:
            print(f"         ğŸ“Š è´¨é‡æŒ‡æ ‡: {phase_result['metrics']}")

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python tactical_executor.py <è§„åˆ’æ–‡ä»¶>")
        print("ç¤ºä¾‹: python tactical_executor.py strategic_plan_æœºå™¨å­¦ä¹ _20251214_163154.json")
        return
    
    plan_file = sys.argv[1]
    
    try:
        with open(plan_file, 'r', encoding='utf-8') as f:
            strategic_plan = json.load(f)
        
        executor = TacticalExecutor()
        result = executor.execute_tactical_plan(strategic_plan)
        
        print(f"\nâœ… L2æˆ˜æœ¯æ‰§è¡Œå®Œæˆï¼")
        print(f"ğŸ“Š æ‰§è¡Œé˜¶æ®µ: {len(result['phases'])}")
        print(f"â±ï¸ æ€»è€—æ—¶: {sum(p['duration'] for p in result['phases'])}ç§’")
        
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")

if __name__ == "__main__":
    main()