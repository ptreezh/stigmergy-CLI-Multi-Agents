--- Á¨¨1È°µ ---
A Bimodal Network Approach to Model Topic Dynamics
Luigi Di Caro1;3, Marco Guerzoni1;2,
Massimiliano Nuccio1;2, Giovanni Siragusa1;3
1Despina, Big Data Lab
2Department of Economics and Statistics "Cognetti de Martiis", University of Turin, Italy
3Department of Computer Science, University, of Turin, Italy
Abstract
This paper presents an intertemporal bimodal network to analyze the evolution of the semantic
content of a scientiÔ¨Åc Ô¨Åeld within the framework of topic modeling, namely using the Latent
Dirichlet Allocation (LDA). The main contribution is the conceptualization of the topic dynam-
ics and its formalization and codiÔ¨Åcation into an algorithm. To benchmark the eÔ¨Äectiveness
of this approach, we propose three indexes which track the transformation of topics over time,
their rate of birth and death, and the novelty of their content. Applying the LDA, we test the
algorithm both on a controlled experiment and on a corpus of several thousands of scientiÔ¨Åc
papers over a period of more than 100 years which account for the history of the economic
thought.
Keywords: topic modeling, LDA, bimodal network, topic dynamics, economic thought
1 Introduction
A crucial issue in the philosophy of science consists in the understanding of the evolution of scientiÔ¨Åc
paradigms within a discipline. Following Kuhn [1970, p.10], a scientiÔ¨Åc paradigm can be thought
as the set of assumptions, legitimate theories, methods, and experiments both adequately new to
attract a group of scholars, to build a contribution to a Ô¨Åeld and to open enough the exploration
of diÔ¨Äerent directions of research.
1We would like to thank JSTOR ( www.jstor.org ) for providing the data and DESPINA -Big Data Lab ( www.
despina.unito.it ) and the Department of Computer Science at the Univeristy of Turin for Ô¨Ånancial support.
1arXiv:1709.09373v1  [cs.CL]  27 Sep 2017

--- Á¨¨2È°µ ---
In the traditional view, as developed for hard and mature sciences, the evolution of scientiÔ¨Åc
paradigm consists in " the successive transition from one paradigm to another via revolution " [Kuhn,
1970, p.12]. However, a scientiÔ¨Åc Ô¨Åeld is usually composed by several research paradigms either
competing or addressing diÔ¨Äerent issues, and a revolution in one of those necessarily involves eÔ¨Äects
and readjustments in the entire discipline. Moreover, each new paradigm carries the legacy of the
existing knowledge of past paradigms, which is often recombined into the new one. This is especially
true for social sciences, in which the identiÔ¨Åcation of clear scientiÔ¨Åc paradigms in the sense of Kuhn
is often blurred and it is probably more correct referring to "research traditions" [Laudan, 1978].
However, whether you call paradigms or traditions, the existence of patterns of thoughts which
are legitimate contributions to a theory is undeniable. Thus, we can postulate that the evolution of
knowledge inascientiÔ¨ÅcÔ¨Åeld isgenerated amongacommunityof researcherswhichshare asemantic
area to deÔ¨Åne speciÔ¨Åc research issues, describe methodologies, and lay down results. Thus, the
heterogeneity of the research tradition of a scientiÔ¨Åc Ô¨Åeld can be described with semantic analysis.
The idea that some measure of words co-occurrence reveals an underlying epistemic pattern and,
therefore, it can capture the essence of evolution in science is not a new one. Despite the diÔ¨Éculty in
programming, the Ô¨Årst attempts date back to the work of Callon et al. [1983] and reÔ¨Åned when the
Ô¨Årst open code have been made available a decade later [Vlieger and LeydesdorÔ¨Ä, 2011, LeydesdorÔ¨Ä
and Welbers, 2011].
The challenge of classifying science on the basis of its semantic content has found a renewal
with the diÔ¨Äusion of machine learning techniques and, in particular, in the subÔ¨Åeld of unsupervised
learning[LeydesdorÔ¨ÄandNerghes,2015]. Topicmodelingincludesafamilyofalgorithms[Bleietal.,
2003], whichareparticularlyperformantinextractinginformationfromlargecorporaoftextualdata
by reducing dimensionality. This feature has been clearly recognised in mapping science [Suominen
and Toivanen, 2015] or news [DiMaggio et al., 2013]. Alghamdi and Alfalqi [2015] review four major
methods of topic modeling, including Latent Semantic Analysis (LSA), Probabilistic LSA, Latent
Dirichelet Allocation (LDA) and Correlated Topic Model (CTM). The LDA proposed in [Blei et al.,
2003] is one of the most diÔ¨Äused approaches. LDA retrieves latent patterns in texts on the basis
of a probabilistic Bayesian model, where each document is a mixture of latent topics described by
a multinomial distribution of words. One of the major limitations of LDA lies on its inability to
model and represent relationships among topics over time [Alghamdi and Alfalqi, 2015].
In this paper, we address a major recurring issue in topic modeling, that is the topic dynamics,
or, in other words, we test a method to track the transformation of topics over time. As stated
by Blei and LaÔ¨Äerty [2006], LDA is a powerful approach to reduce dimensionality, but it assumes
that documents in a corpus are exchangeable. On the contrary, articles and themes are sequentially
organized and evolve over time. Therefore, it is not only relevant to develop a statistical model
to determine the evolving topics from a corpus of a sequential collection of documents, but also to
measure and describe the transformation of topics and their appearance and disappearance.
In the literature of information retrieval, the dynamics of topics has been faced with two ap-
proaches [He et al., 2009]: a discriminative one monitors a change in the distribution of words or
in the mixture over documents, while a generative approach searches for general topics over the
whole corpus and, then, it assigns the documents which belong to each topic [Bolelli et al., 2009,
He et al., 2009].
SpeciÔ¨Åcally Blei and LaÔ¨Äerty [2006] introduced Dynamic Topic Modeling (DTM), a class of
generative models in which the per document topic distribution and per topic word distributions
are generated from the same distributions in a previous time frame. This approach has been very
inÔ¨Çuential since it imposes a connection between the sets of topics at diÔ¨Äerent periods and allows
2

--- Á¨¨3È°µ ---
to track the evolution of a single topic over time.
DTM performs very well in capturing the evolution of a single topic. However, the evolution
of knowledge is much more complicated that the change of relative importance of words within
a topic, since it may involve also the creation of new topics, their mutual re-combinations and,
eventually their possible demise. The major contribution of the paper is the conceptualization and
formalizationoftheevolutionofknowledge, conceivedasdiÔ¨Äerentstreamsofsemanticcontentwhich
continuously appears and disappears, merges and splits. Thereby we propose an original method
based on inter-temporal bimodal networks of topics compute the key elements in the evolution of
knowledge.
Moreover, the ultimate goal of the paper is not to track in detail what happens within a single
topic, but rather to develop indexes which can measure at the aggregate level some properties of
the observed knowledge dynamics, such as an overall degree of novelty or the level of turbulence at
speciÔ¨Åc time windows.
The paper is organized as follows: in the next section, we suggest a method to analytically
conceptualize and measure diÔ¨Äerent patterns of topics evolution. Section 2.2 translates it into an
algorithm which calculate some measures of merging, splitting and novelty of the topics generated
by the LDA. In section 3.1, a simple simulation tests the robustness of the method on artiÔ¨Åcial
data. Finally, in Section 4, the same algorithm is applied to a large dataset of papers in economics:
main results are presented and discussed by describing the evolution of the topics in the economic
science in the past century.
2 A Conceptualization of Knowledge Evolution
In this paper, we focus on the dynamic evolution of topics over time. With DTM, each topic Ktis
linked toKt+1creating a topics chain which spans the years covered by the documents. SpeciÔ¨Åcally,
Blei and LaÔ¨Äerty [2006] maps each topic at time t-1 into a topic in t by chaining the per document
topic distribution tand the per topic word distribution, t;kin a sate space model with a Gaussian
noise:
t;kjt 1;kN(t 1;k;2I) (1)
tjt 1N(t 1;2I) (2)
This approach is highly performing to track incremental changes of the same topic but it does
not focus on revealing neither birth nor death nor possible combinations of topics and it imposes
a constant number of topics within the model. On the contrary, we are interested to discover the
structural change of topics in a corpus and to understand the underlying topic dynamics which
explain it. Thereby, we do not focus on the evolution of the single topic. The inter-temporal
link across topics is not a constraint in the estimation of the model as in the DTM, but it is
introduced ex-post in the empirical analysis by looking at the similarities (co-occurrence of words)
amongst topics generated by independent LDAs. More in detail, while DTM models sequences of
compositional random variables by chaining Gaussian distributions (thus directly embodying topics
dynamics in the model), our approach operates on single and static LDAs in order to track and
measure such dynamics out of the model.
The evolution of a topic structure of a corpus accumulating knowledge overtime takes place be-
cause of two main reasons. On the one hand, any epistemic community (say for instance journalists
3

--- Á¨¨4È°µ ---
or scientists) can shift their intellectual interest to new issues and problems, which will result in
diÔ¨Äerent choices, frequencies and co-occurrence of words. On the other hand, language is subject
to a constant evolution, in which new words, named entities, acronyms, etc. appear while other
ones disappear due to an increasingly lesser use of them by the same community. We rule out this
second scenario, by assuming that in the short time frame the language is fairly stable.
Under this assumption, when comparing the topics generated by a topic modeling exercise in
two diÔ¨Äerent, although adjacent, time windows, we should be able to capture the evolution of the
scientiÔ¨Åc debate and highlight the birth, death and recombination of topics. On the one extreme,
we can Ô¨Ånd a situation in which knowledge does not evolve and thus topics are stable. On the
other, we Ô¨Ågure out the maximum of turbulence in which new topics emerge without any semantic
relation with the incumbent ones. In the latter case, we may assume the death of past topics and
the birth of new ones. In between the two ideal cases, we can also draw a continuum in which we
can observe both deaths and births of topics. Finally, in a most interesting scenario, rather than
observing stability or turbulence, knowledge may evolve recombining existing topics in both old and
new ones. Table 1 summarizes Ô¨Åve typical patterns of knowledge evolution and their interpretation
within a topic modeling framework.
Table 1: Topic modeling and typical patterns of knowledge evolution
Stability a topic A exists at time t and t+1
Birth the topic A at time t+1 has no antecedent at time t
Death the topic A at time t disappears at time t+1
Merging multiple topics at time t combine in a new topic A at time t+1
Splitting multiple topics at time t+1 share an antecedent at time t
Figure 1 presents the Ô¨Åve ideal types of knowledge evolution as a proximity network of topics,
that we mathematically formalize as follows. Let us consider Mtopics emerged as the result of a
topic modeling exercise from a corpus of articles at time tandNtopics at time t+ 1. We tackle
the critical problem of tracking the transformation of the set of topics M= (1;:::;A;:::;M )at
t into the set of topics N= (1;:::;a;:::;N )att+ 1. SpeciÔ¨Åcally, we are interested in measuring
the magnitude of the various phenomena such as birth, death, merging, and splitting. Consider
a similarity index based on word co-occurrence, simil1, between each couple of topics (A;a)with
A2Manda2Nand consider the similarity matrix S (MN)
S=2
64a ::: N
Asimil 1;1:::: simil 1;N
......
Msimil M;1::: simil M;N3
75
For the sake of clarity and with reference to Figure 1, let us consider the minimal example in
whichM= (A;B)andN= (a;b)
1Typically, this index is the cosine similarity index, as it is used in the empirical part of the paper
4

--- Á¨¨5È°µ ---
Figure 1: Ideal types of topic evolution
A b
(a) StabilityA
Cb
(b) Merging
b
(c) BirthA b
c
(d) Splitting
A
(e) Death
S=a b
A 
B 
The network representation allows to visualize the Ô¨Åve ideal types of knowledge evolution: Table
2 summarizes them and the necessary and suÔ¨Écient conditions on the values of the similarity index
to observe such cases. However, with a higher number of topics, a derivation of the conditions on
the values of the similarity index would be cumbersome. Moreover, Table 2 depicts ideal situations
only, while the observed reality usually deals with a continuous mixture of the paradigmatic cases
presented above. For instance, already in the case with M= 4andN= 3depicted in Figure 2,
the analysis becomes strenuous.
With this purpose in mind, we consider the similarity matrix Sas the incidence matrix of M
overN. We can thus employ Sto create a bi-adjacency matrix D, and thus consider Figure 2 as
the resulting bipartite network in which MandNare the sets of nodes, while the elements of the
matrix are the weights of the edges.
5

--- Á¨¨6È°µ ---
Table 2: Bimodal network and empirical indexes
Network Matrix param. Cases
A
Ba
b;6= 0;= 0 STABILITY: no births, no deaths
A
Ba
b;;; = 0 INSTABILITY: births and deaths
A
Ba
b;6= 0;= 0 MERGING: no deaths, but births
A
Ba
b;6= 0;= 0 SPLITTING: no births, but deaths
Figure 2: bipartite network of topics of two time windows
A B C D
a b c
6

--- Á¨¨7È°µ ---
D=0S
ST0
=
=2
66666666664A ::: M a b ::: N
A 0 0 0
B 0 0 0 S
::: 0 0 0
M 0 0 0
a 0 0 0 0
b 0 0 0 0
:::ST0 0 0 0
N 0 0 0 03
77777777775
Wenowshowhowthisrepresentationcanhelpmeasurethemagnitudeofbirths, deaths, merging
and splitting.
Birthsanddeathscanbeeasilycalculatedfromthematrix S. Arowsumequaltozerohighlights
a death, while a column sum equals to zero indicates a birth. A death means that the semantic
legacy completely disappears while a birth means that a topic carries no semantic similarity with
other topics in the past. Once again it is important to notice that these cases are extreme scenarios
while in the reality we observe a continuum between births and deaths. We might thus calculate an
indexNovelty_i(NI) for each topic iat timet+ 1where forNI_i=MAXwe have a birth, that
is a topic with no similarity to any other previous one. For higher value we have a higher novelty
of the topic. We can also measure an average change in NIon the overall structure of a scientiÔ¨Åc
Ô¨Åeld by looking at distributions of these indexes over the topics. For instance, let us consider the
Novelty Index and the average, deÔ¨Åning:
NIj= 1 PM
iSi;j
M(3)
wherejis the index of the j-th column in the matrix S, and
NI= 1 PM
iPN
jSi;j
MN(4)
We take the average of all the cell values in matrix S. If the similarity index is bounded between
0 and 1, such it is the very common case of the cosine similarity index, thus NIranges from 0 to
1. For very small value of novelty, new topics show diÔ¨Äerent word distribution from old ones.
As mentioned, transformation of topics can take the form of merging and splitting. We say that
a merging occurs if a topic at time t+ 1shows a high similarity with two topics at time t, meaning
that the semantic universe of AandBatt(as in Ô¨Ågure 2) is combined in the topic a. Similarly, we
can say that a split occurs if the semantic legacy of one topic at tis to be found in multiple topics
att+ 1as in the case for topic C.
To analyse the intensity of a merging we can project the bipartite network of Ô¨Ågure 2 into its
two 1-mode-network of Ô¨Ågure 3. This is achieved by a matrix multiplication SSTfor the merging
andSTSfor the splitting which result in two matrices PmergingandPsplittingof dimension
respectively MMandNN. Please note, that for the properties of matrix multiplication
7

--- Á¨¨8È°µ ---
Figure 3: 1 mode network, projection of Figure 2
A
BC
Da
bc
Merging Splitting
PmergingandPsplittingare always square matrices, even when the number of topics in two periods
diÔ¨Äers.
The network is represented by the matrix P
Pmerging=2
664A B ::: M
A
BSST
:::
M3
775
Psplitting=2
664a b ::: N
a
bSTS
:::
N3
775
Thematrixtransformationallowsustodrawthe1-mode-networkasinFigure3,whichrepresents
the merging and splitting between two time windows. The matrix formulation of the network is
also useful for computing the intensity of merging and splitting on the basis of the two relative
matricesP. Let us consider the matrix Pmergingin a minimal example of the table 2
Pmerging=SST= 
 
 
 
=+ +
+ +
(5)
The matrix Pis always symmetric and, for our purpose, we focus on the low triangle. The
merging is captured by the number outside the diagonal (+), where ()is the intensity
of the merging of AandBina, while ()is the intensity of the merging of AandBinb. In this
exemplary case shown in Table 1, andare equal to zero and andare diÔ¨Äerent from zero:
thus, we have a merging between AandBas depicted in Figure 3.
Mutatis mutandis, we can consider the case of splitting. Once again, the low triangle oÔ¨Ä the
diagonal highlights the intensity of split with ()the split of Ainaandb, while ()the split
ofB.
Psplit=STS= 
 
 
 
=+ +
+ +
(6)
When we have a large number of topics in both time windows, we can use this formulation to
create indexes measuring the intensity of merging and splitting or other properties of the transition.
SpeciÔ¨Åcally, we aim at comparing the values below the diagonal with those on the diagonal. We
8

--- Á¨¨9È°µ ---
Figure 4: Similarity network among topics in t
A
BC
D
thus create a normalized matrix in which all elements of the diagonal and below the diagonal add
up to one.
Pmerging
normalized=Pmerging1P
ijP(i;j)(7)
In this way, we can compute a MergingIndex (MI) which takes value 0when no merging occurs
and it ranges up to an upper limit which can not exceed 1.
MI= 1 trace (Pmerging
normalized) (8)
Symmetrically, we calculate a SplittingIndex (SI)
2.1 Conditional dependence
A last important issue to be addressed consists of the impact of the conditional dependence of topics
at timetand its relation with the 1-mode network projection. Two topics at tcan appear to merge
into a topic at t+ 1only because they are already similar to each other at time t. In this case we
might run the risk of identify a spurious process of merging. However, it is possible to account for
this dynamic conditional dependence. We can compute a similarity index among topics at time t,
simT, which can also be represented by a network.
Q=2
64simT 1;1::: simT 1;M
...
simT M;1::: simT M;M3
75
Note thatQis a symmetric matrix, with the same dimension (MM)asPmerging.
The same procedure can be applied to topics at t+ 1. In this case, we obtain a matrix (NN),
with the same dimension of Psplit.
In order to take into account the conditional dependence, we might consider Rmerging;splitting=
(Pmerging;splittingjQmerging;splitting)and recompute the indexes, substituting RwithP. There
exist diÔ¨Äerent ways to operationalize the dependence. Probably the most sophisticated one would
be to encode the overall conditional dependence structure within a graphical network [Jordan,
1998, Lauritzen, 1996]. However, we might also consider that the similarity measure has a scalar
meaning which goes beyond a simple probabilistic relation. For this reason, we surmise that the
conditional dependence can be at best considered by dividing or subtracting element by element
the two matrices: in the developed algorithm (see next paragraphs), we divide. Table 3 summarizes
the indexes we use and their range.
9

--- Á¨¨10È°µ ---
Table 3: Measuring change in topic modeling
Type of change IndexMinMax
Introduction of new semantic areas or legacy from the past NI 01
Merging of the semantic content of topics MI 01
Splitting of the semantic content of topics SI01
2.2 The Proposed Algorithm
This paragraph describes the algorithm which we developed to operationalize the former theoretical
approach. Our example relies on the Latent Dirichlet Allocation (LDA) [Blei et al., 2003], although
thismethodologydoesnotinvolveanyassumptioninthewaytopicsarecreated. LDAisagenerative
model that summarizes the documents through a mixture of topics, where each topic is a probability
distribution in the dictionary. The algorithm Ô¨Årst generates a database which allows query of
documents per time period. Thereafter, it divides the dataset into unigrams where stopwords
are eliminated according to the NLTK list ( www.nltk.org ). Finally, we have applied the Porter
Stemmer [Porter, 1980] on individual words. This algorithm transforms (or truncates) every word
in a morphological root form. We create a subset per each T time window and compute Nttopics
using standard LDAs2. On the generated output we are able to compute the three indexes. For
the similarity computation, we use the probability of the Ô¨Årst 100 topic‚Äôs words to generate the
vector weights.
Algorithm 1 shows the pseudo-code to compute the time window from ttot+1. It simply takes
in input the cleaned documents of the selected windows and the number of topics at time tand
t+ 1and returns the merging, splitting and value indexes. In details, the algorithm generate a
LDA model for each time window tandt+ 1and computes the similarity between topics at time t
andt+ 1(and themselves). Then, it computes the matrices Pmerging,Psplittingusing the similarity
matrixSand the matrix Q. The twoPmatrices are used to compute MIandSI, while the matrix
Qis used to compute NI.
3 Evaluation
As to evaluate this approach, we cannot benchmark it with other dynamic methods such as DTM,
since we do not track the single topics over time, but we compare adjacent time windows to measure
the degree of topics recombination. Therefore, we test the methodology by applying the algorithm
on an artiÔ¨Åcially-generated dataset with controlled characteristics.
3.1 ArtiÔ¨Åcial Data Creation
To generate the experimental datasets, we create artiÔ¨Åcial topics reÔ¨Çecting natural and realistic
textual content. Instead of directly producing topics as sets of artiÔ¨Åcially-built sets of words, we
started from concept seeds , used as query of real textual data. A concept seed is a word (or
compound word) that represents a concept in a text-based resource. For example, the concept seed
physicswithin the Wikipedia resource is the Wikipedia page about Physics. From a set of concept
2https://radimrehurek.com/gensim/
10

--- Á¨¨11È°µ ---
Algorithm 1 computeSingleWindow(documentSet, numTopic t,numTopic t+1)
1:topic t LDA(documentSet, numTopic t)
2:St computeTopicSimilarity( topic t,topic t)
3:topic t+1 LDA(documentSet, numTopic t+1)
4:St+1 computeTopicSimilarity( topic t+1,topic t+1)
5:Q computeTopicSimilarity( topic t,topic t+1)
6:Rmerger StST
t
7:Rsplit St+1ST
t+1
8:Qmerger QQT
9:Qsplit QTQ
10:Pmerger zeros(Rmerger.numRow(), Rmerger.numCol())
11:Psplit zeros(Psplit.numRow(), Psplit.numCol())
12:fori 1..Rmerger.numRow() do
13: forj 1..Rmerger.numCol() do
14:Pmerger [i;j] Rmerger [i;j]
Qmerger [i;j]
15: end for
16:end for
17:fori 1..Rsplit.numRow() do
18: forj 1..Rsplit.numCol() do
19:Psplit[i;j] Rsplit[i;j]
Qsplit[i;j]
20: end for
21:end for
22:merger merger(normalize( Pmerger))
23:split split(normalize( Psplit))
24:novelty novelty(Q)
25:
26:return merger, split, novelty
seeds and their associated Wikipedia pages, it is possible to extract the whole textual content and
build artiÔ¨Åcial documents for the chosen concepts3.
In the following exercise, we selected 8 concept seeds, all related to the Ô¨Åeld of Economics, in
order to understand how well our approach works on a toy model reÔ¨Çecting contents which are
consistent with the real data we used in Section 4).
As in most natural language processing systems, we applied some pre-processing phase, which
includes the removal of stopwords as well as functional linguistic items such as determiners, punc-
tuations, etc4.
Once the sets of words are built, we generated a document for each seed concept by randomly
selecting the words5with uniform probability. We maintained word repetitions to allow us to sam-
pling words with their real frequency and generate documents closed to real cases. The documents
3We used the library Wikipedia available at https://github.com/goldsmith/Wikipedia , which acts as a wrapper
of the MediaWiki API ( https://www.mediawiki.org/wiki/ )
4We used the library Spacy(https://spacy.io/ ), Ô¨Åltering out the words having the following Part-of-Speech
tags: DET (article), NUM (number) and PUNCT (punctuation).
5The number of words of each document has been chosen randomly.
11

--- Á¨¨12È°µ ---
generated are used to train diÔ¨Äerent LDA models with diÔ¨Äerent seeds concepts.
Finally, we compared the topics of diÔ¨Äerent LDA models by means of the proposed measures to
see whether they capture the dynamics of the topic changes. We refer the reader to Appendix A
for details about the algorithms.
3.2 Controlled Experiments
Toevaluatethealgorithmwecreate8diÔ¨Äerentcontrolledexperimentswhicharedesignedtocapture
the 4 ideal cases of knowledge evolution. SpeciÔ¨Åcally, we conducted twice 4 experiments to test the
functioning of the method in 4 diÔ¨Äerent situations by changing (or not) the number of topics and
by replacing (or not) the concept seed. In the Ô¨Årst 4 runs we kept the scenario as simple as possible
and we slightly increased the complexity of the exercise in the second 4 runs.
In the former, the number of topics at time tare Ô¨Åxed to 2 for the Ô¨Årst experiment and 4 for
the second one; the number of topics at time t+ 1is determined by the experiment (see Table 4 for
details). In details, we set each experiment as follows:
stability the number of topics and seed concepts are kept the same. The variation is only stochas-
tic.
birth/death the number of topics does not change, but we replace the concept seeds to force
death of the previous topics and birth of new ones.
merging the seed concepts do not change, but we reduce the number of topics to force a situation
of merging. For instance, if we cluster the same concept seeds in 2 and 1 topics, we necessarily
observe only merging and no splitting.
splitting the seed concepts do not change, but we increase the number of topics to force a situation
of splitting.
Table 4 summarizes the design of the experiments and depicts avarage values of 100 runs of
theAlgorithm 2 . Concerning with the Ô¨Årst 4 simple designs, experiments are conceived to force
the results and create only splitting and only merging. For the splitting the number of topics
increases from one to two and we should not observe merging since at t 1there is also one topic.
Analogously, in the case of merging the number of topic shrinks to one in t+ 1. The remaining two
experiments compare stability with births and deaths, which lead to a higher degree of novelty. Our
indexes vary as expected: in splitting andmerging theMIandSIrespectively go to zero. If we
compare stability withbirths and deaths theNIis much higher in the former case. Table 4 shows
four diÔ¨Äerent experiments with higher number of topics. It is relevant to notice that even with a
few topics, it is impossible to get a clear-cut outcome since the recombination of knowledge may be
unexpected and typically reproduces at the same time merging, splitting, stability for some topics,
and birth and death for others. However, these baseline examples clearly points at the aggregate
behaviour of topics within a discipline.
12

--- Á¨¨13È°µ ---
Table 4: The table shows the experimental results conducted over the four cases.
Type of experiment Concept seed Replacement Topics at
time tTopics at
time t+1index value
stabilitylabour economics
innovation economicsno 2 2MI
SI
NI0.189
0.192
0.43
splittinglabour economics
innovation economicsno 1 2MI
SI
NI0.0
0.328
0.155
merginglabour economics
innovation economicsno 2 1MI
SI
NI0.398
0.0
0.296
birth/deathlabour economics
innovation economicscultural economics
environmental economics2 2MI
SI
NI0.363
0.385
0.768
stabilitylabour economics
innovation economics
cultural economics
environmental ecsno 4 4MI
SI
NI0.541
0.545
0.555
splittinglabour economics
innovation economics
cultural economics
environmental ecsno 4 8MI
SI
NI0.574
0.747
0.561
merginglabour economics
innovation economics
cultural economics
environmental economicsno 4 2MI
SI
NI0.587
0.297
0.472
birth/deathlabour ecs
innovation economics
cultural economics
environmental economicsindustrial economics
transport economics
economic history
health economics4 4MI
SI
NI0.640
0.662
0.777
13

--- Á¨¨14È°µ ---
4 The Evolution of Knowledge in Economics
The dataset is a collection of documents which appear in the JSTOR database ( www.jstor.org )
and were published from 1845 to 2013 in more than 190 journals concerning with economic sciences
(also deÔ¨Åned as economics ). They are more than 460,000 documents, classiÔ¨Åed as research articles
(about 250,000), book reviews (135,000), miscellaneous (73,000), news (4,000) and editorials (500).
Foreachdocument, inadditiontobibliographicinformation(title, publicationdate, authors, journal
title, etc.), the dataset provides full content in form of a bag of words, i.e. the set of words used in
the documents associated with their frequencies.
The following analysis only considers the research articles in order to remove the possible noise
caused by using diÔ¨Äerent types of documents, which can be written in diÔ¨Äerent languages. The
distribution of research articles over the time considered is very skewed (see Figure 5). Although
the Ô¨Årst documents date back to 1845, until the end of the XIX century the corpus of articles
accounts only for 2930 items. The increase is almost linear till the beginning of the 1960s, when
the number of documents more than doubled in a few years and rose to over 5000 items published
every year during the 1990s and 2000s. From 2011 to 2013 we count 8220 items published.
The LDA has been applied to research papers published between 1890 and 2013: decades before
1890 were dropped because of the extremely low number of documents. Thereby, the resulting
dataset of articles consists of 755,838,336 words and 3,169,515 unique words. We experimented
varying the hyper-parameters of the method, namely the number of topics and the dimension of
time windows, in order to evaluate the robustness and sensitivity of our approach in the 123 years
considered. We selected 25, 50 and 100 topics and time windows of 5, 10 and 20 years, keeping
Ô¨Åxed one parameter and varying the other one. In details, we Ô¨Årst show the values of SIandMI
Ô¨Åxing the window dimension to 10 years and varying the number of topics. In the following Ô¨Ågures,
for example, 1900-1920 indicates the value of the indexes between 1900 and 1910 compared with
the corresponding value between 1910 and 1920. Figures 6, 7 and 8 show the indexes for 25, 50 and
100 topics within a window of 10 years. Then, we Ô¨Åxed the number of topics to 25 and we varied
the size of the time window. Figures 9 and 10 show the indexes for 25 topics and windows of 5 and
20 years.
These simple tests have demonstrated that the main trends of the indexes do not change sub-
stantially by varying the hyper-parameters, meaning that our method is robust to the number of
topics and the size of the time windows.
To further prove invariance to number of topics and windows-size, we applied Greene metric
Greene et al. [2014] on a subset of the research articles with a time windows of 10 years to capture
all the possible changing in economic knowledge. Values of the metric reveal how much the topics
generated capture the information presented in the dataset. Greene metric requires a range in
input, which is formed by the minimum and maximum number of topics, and a step parameter,
used by the metric to shift the number of topics considered at the current step starting from the
minimum ones. For example, if the minimum number of topics is 10, the maximum is 50 and the
step is 20, the Greene metrics will compute a score at 10, 30 and 50 topics. The plot of the metric
in Figures 11 and 12 concerns with two windows and shows that increasing the number of topics
we can increase stability too, but of course, it becomes very diÔ¨Écult to interpret the meaning of
each topic.
As suggested by Mimno and Blei [2011] when topic modeling is employed to explore the content
ofadataset-asinthispaper-ratherthantopredictthereisnotadeÔ¨Ånitivetesttosupportthechoice
of the optimal number of topics. We solved this trade-oÔ¨Ä between stability and meaningfulness by
14

--- Á¨¨15È°µ ---
Figure 5: Distribution of documents in the corpus per year of publication
manually controlling for the topics generated by the model with 25 topics within time window of
10 years. When we found that a few topics could be split up again because they were too general,
we set an optimal and analytically useful number of topics to 27. Therefore, the following analysis
is based on 27 topics within time windows of 10 years, which perform the maximum stability of the
indexes varying the number of topics.
Figure 13 shows the values of MIandSIrespectively for each time window, as deÔ¨Åned in Section
2.2. In the corpus we analyzed, both indexes show a general trend of decreasing values over time,
which becomes particularly severe starting from 1960s. Merging and splitting increase only between
the 1940s and the 1950s while dropping dramatically in the second half of the XX century. The
transformation of topics seems to Ô¨Ånd new urge only around the end of the century, when merging
is increasing againg and splitting is stable. As for the NI, we mentioned that the index tends to
one when new topics emerge without matching with topics at t 1. On the average the value is
higher than 0.9 all over the 123 years considered, so we tracked both micro-variation and general
trend. In Figure 14 NIdoes not show relevant variations until 1990s, with some local maximum in
the Ô¨Årst decade of the past century and a local minimum around the half of it. In the last decade
15

--- Á¨¨16È°µ ---
Figure 6:MIandSI- 25 topics 10 years window size
Figure 7:MIandSI- 50 topics and 10 years window size
of the century it grows sharply, revealing a higher rate of brand new topics or at least of topics
deÔ¨Åned by new words.
16

--- Á¨¨17È°µ ---
Figure 8:MIandSI- 100 topics and 10 years window size
Figure 9:MIandSI- 25 topics and 5 years window size
Suchamethodologicalapproachhastheadvantageoftrackingtheevolutionofeachsinglestream
of economic theory by looking simultaneously at all the others. On the whole, the analysis of such
a big corpus of documents suggests that merging and splitting cannot be considered as opposite
phenomena, but a complementary measure of recombination of topics. In particular, trends in the
Ô¨Åeld of economics suggest a steady decrease of both splitting and merging only temporally balanced
by a weak growth before and after the WWII. From a historical perspective this is absolutely
consistent with the need of theoretical elaboration in economics following the great Depression in
17

--- Á¨¨18È°µ ---
Figure 10:MIandSI- 25 topics and 20 years window size
Figure 11: Greene et al.‚Äôs stability values for the time window 1910-1920.
1929 and the dramatic economic changed imposed by the post war reconstruction. During the
1960s and in combination with the boom of academic publications, many topics are spreads over
18

--- Á¨¨19È°µ ---
Figure 12: Greene et al.‚Äôs stability values for the time window 1940-1950.
a relevant number of documents and journals, although they seem to elaborate on relative stable
basis of autonomous topics. Only by the end of the century we have witnessed the development
of new-brand topics. The birth of new topics strengthens the hypothesis of self-standing topics
shaped by their own specialised language and a lesser exchange of knowledge across the economic
discipline. In other words, the terriÔ¨Åc expansion of the academic production seems to come with
a fragmentation and dispersion in multiple niches of knowledge Cedrini and Fontana [2017] which
elaborate on a new language, but not necessarily producing new paradigms.
5 Conclusion
In this paper we proposed a method to measure the evolution of knowledge in a scientiÔ¨Åc Ô¨Åeld
extracting topics in a corpus of documents. Topic modeling techniques are becoming increasingly
reÔ¨Åned in treating large and complex corpora of documents, but they may lack of a theoretical
reÔ¨Çection of the underlying empirical phenomenon. Taking a dynamic perspective we recognise Ô¨Åve
paradigmatic cases of knowledge evolution. We then surmise that modeling the proximity between
topics of diÔ¨Äerent time windows as a proximity network might be a useful tool to measure their
knowledge dynamics. Indeed, this network approach allows us to develop 3 indexes, which grasp
i) the stability of topics over time measuring their rate of death and birth ( Novelty Index -NI),
and ii) the degree of recombination of topics ( Merging Index - MIandSplitting Index - SI). For
very simple cases, we are also able to analytically derive those conditions, which link the proximity
network and the value of each index. Testing the algorithm over a set of simulated documents, we
showed its robustness for each the indexed developed. Finally, we applied our approach to a real
and large corpus of academic publications in economics to illustrate how the combined use of MI,
19

--- Á¨¨20È°µ ---
Figure 13:MIandSI- 27 topics 10 years
Figure 14:NI- 27 topics 10 years
SIandNIis eÔ¨Äective to understand dynamics and trends in economic knowledge and thought.
We believe, this is a Ô¨Årst step towards the development of a closer connection between algorithms
20

--- Á¨¨21È°µ ---
Figure 15: Combined graph of SIandNI
Figure 16: Combined graph of MIandNI
for dynamic topic modeling and the empirical phenomenon they are supposed to describe.
A ArtiÔ¨Åcial Data Creation: Algorithms
InAlgorithm 2 , the function getNum(minNum,maxNum) returns a number, randomly selected, be-
tweenminNum emaxNum ; thegetWord() function returns a word, randomly chosen on the selected
set; the function computeTopicSimilarity() calculates the cosine similarity between the input topics;
thefunction zeros()returnsanarraycontainingallzeros. Finally, thefunction getWordList(concept)
generates a set of words. The words are taken from the Wikipedia page that points to the chosen
concept.
21

--- Á¨¨22È°µ ---
In rows [1-6], the function getWordList collects, for each concept seed, a set of words. In details,
getWordList , as shown in Algorithm 3 , extracts all words contained both in the Wikipedia page
related to the concept in input through the python library Wikipedia6. Words are extracted using
the library Spacy7and stored in wordList8. Then, the wordList of eachconcept seed is inserted
intowordConceptList . In rows [7-16], Algorithm 2 generates a document for each concept, sampling
words (with uniform probability) from the wordList related to the concept seed . The number of
words to sample is speciÔ¨Åed by numWords , which ranges from 1000 to 10000. Successively, in
rows [18-20], the algorithm divides documents in two sets, a set containing the Ô¨Årst numDocument
documents and a set containing the remain documents, and applies LDA. The LDA can be applied
over the two documents sets or only over a single documents set according to the replaceDoc Ô¨Çag.
IfreplaceDoc is set to True, the Ô¨Årst documents set is replaced with the second one (it is set to
Falseby default).
Algorithm 4 shows how words are processed. We Ô¨Åltered stopwords and words having Part-Of-
SpeechtagsDet(Determiner), X(foreign word), NUM(Numeral), Punct(Punctuation), SPACE
andEOL(end of line symbols). We also Ô¨Åltered words that does not match the python regular
expressionnw+. Furthermore, all unÔ¨Åltered words are brought back to their morphological root.
6https://github.com/goldsmith/Wikipedia
7https://spacy.io/
8There exists a wordList for each conceptSeed in input.
22

--- Á¨¨23È°µ ---
Algorithm 2 ToyEvaluation(seedConcepts, numDocument, numTopic t,numTopic t+1, replace-
Doc)
1:wordsConceptList = {}
2:// create a words list for each concept seed
3:forconcept in seedConcepts do
4:wordsList getWordList(concept)
5:wordsConceptList.append(wordsList)
6:end for
7:documents = {}
8:fori 1..len(seedConcepts) do
9:numWords getNum(1000, 10000)
10:document = {}
11: forj 1..numWords do
12:word wordsConceptList[i].getWord()
13:document.append(word)
14: end for
15:documents.append(document)
16:end for
17:// get topic
18:documentSet documents[1:numDocument]
19:topic t LDA(documentSet, numTopic t)
20:Mt computeTopicSimilarity( topic t,topic t)
21:ifreplaceDoc6= False then
22:documentSet documents[numDocument:len(seedConcepts)]
23:end if
24:topic t+1 LDA(documentSet, numTopic t+1)
25:Mt+1 computeTopicSimilarity( topic t+1,topic t+1)
26:
27:/* it then continues as computeSingleWindow algorithm */
Algorithm 3 getWordList(concept)
1:posTags {X, NUM, DET, PUNCT}
2:parser parser(lan=eng)
3:wordList {}
4:wordList getWordList(content, posTags)
5:return wordList
23

--- Á¨¨24È°µ ---
Algorithm 4 getWords(content, posTags)
1:words {}
2:wikiPage Wikipedia.getPage(concept)
3:forsentence in parser(wikiPage.content).sentences do
4:forword in sentence.words do
5: if:(word in stopwords) ^:(word.pos in posTags) ^match(word,nw+)then
6: words.append(word.lemma)
7: end if
8:end for
9:end for
10:return words
24

--- Á¨¨25È°µ ---
References
R. Alghamdi and K. Alfalqi. A survey of topic modeling in text mining. International Journal of
Advanced Computer Science and Applications , 6(1):147‚Äì153, 2015.
D. M. Blei and J. D. LaÔ¨Äerty. Dynamic topic models. In Proceedings of the 23rd International Con-
ference on Machine Learning , ICML ‚Äô06, pages 113‚Äì120, New York, NY, USA, 2006. ACM. ISBN
1-59593-383-2. doi: 10.1145/1143844.1143859. URL http://doi.acm.org/10.1145/1143844.
1143859.
D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. Journal of machine Learning
research, 3:993‚Äì1022., January 2003.
L. Bolelli, S. Ertekin, D. Zhou, and C. L. Giles. Finding topic trends in digital libraries. In
Proceedings of the 9th ACMIEEE-CS joint conference on Digital libraries , pages 69‚Äì72. ACM,
2009.
M. Callon, J.-P. Courtial, W. A. Turner, and S. Bauin. From translations to problematic networks:
An introduction to co-word analysis. Social science information , 22(2):191‚Äì235, 1983.
M. Cedrini and M. Fontana. Just another niche in the wall? how specialization is changing the face
of mainstream economics. Cambridge Journal of Economics , forthcoming, 2017.
P. DiMaggio, M. Nag, and D. Blei. Exploiting aÔ¨Énities between topic modeling and the sociological
perspectiveonculture: Applicationtonewspapercoverageofusgovernmentartsfunding. Poetics,
41(6):570‚Äì606, 2013.
D. Greene, D. O‚ÄôCallaghan, and P. Cunningham. How Many Topics? Stability Analysis for
Topic Models , pages 498‚Äì513. Springer Berlin Heidelberg, Berlin, Heidelberg, 2014. ISBN
978-3-662-44848-9. doi: 10.1007/978-3-662-44848-9_32. URL http://dx.doi.org/10.1007/
978-3-662-44848-9_32 .
Q. He, B. Chen, J. Pei, B. Qiu, P. Mitra, and L. Giles. Detecting topic evolution in scientiÔ¨Åc
literature: how can citations help? Proceedings of the 18th ACM conference on Information and
knowledge management , pages 957‚Äì966, November 2009.
M. I. Jordan. Learning in graphical models , volume 89. Springer Science & Business Media, 1998.
T. S. Kuhn. The structure of scientiÔ¨Åc revolutions, International Encyclopedia of UniÔ¨Åed Science,
vol. 2, no. 2 . Chicago: The University of Chicago Press, 1970.
L. Laudan. Progress and its problems: Towards a theory of scientiÔ¨Åc growth . Univ of California
Press, 1978.
S. L. Lauritzen. Graphical models , volume 17. Clarendon Press, 1996.
L. LeydesdorÔ¨Ä and A. Nerghes. Co-word maps and topic modeling: A comparison from a user‚Äôs
perspective. arXiv preprint arXiv:1511.03020 , 2015.
L. LeydesdorÔ¨Ä and K. Welbers. The semantic mapping of words and co-words in contexts. Journal
of Informetrics , 5(3):469‚Äì475, 2011.
25

--- Á¨¨26È°µ ---
D. Mimno and D. Blei. Bayesian checking for topic models. Proceedings of the Conference on
Empirical Methods in Natural Language Processing , pages 227‚Äì237, 2011.
M. F. Porter. An algorithm for suÔ¨Éx stripping. Program, 14(3):130‚Äì137, 1980.
A. Suominen and H. Toivanen. Map of science with topic modeling: Comparison of unsupervised
learning and human?assigned subject classiÔ¨Åcation. Journal of the Association for Information
Science and Technology , October 2015.
E. Vlieger and L. LeydesdorÔ¨Ä. Content analysis and the measurement of meaning: The visualization
of frames in collections of messages. Public Journal of Semiotics , 3(1):28‚Äì50, 2011.
26