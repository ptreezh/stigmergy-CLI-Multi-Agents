#!/usr/bin/env python3
"""
æŠ€èƒ½é›†æˆå™¨ - å®ç°hierarchical-wiki-creatorä¸wiki-collaborationçš„çœŸå®é›†æˆ
"""

import os
import sys
import json
import importlib.util
from typing import Dict, List, Any, Optional
from datetime import datetime
from pathlib import Path

class SkillIntegrator:
    """æŠ€èƒ½é›†æˆå™¨ - è¿æ¥ä¸åŒæŠ€èƒ½çš„åŠŸèƒ½"""
    
    def __init__(self):
        self.base_path = Path(__file__).parent.parent
        self.wiki_collaboration_path = self.base_path / "wiki-collaboration" / "scripts"
        self.hierarchical_path = self.base_path / "hierarchical-wiki-creator" / "scripts"
        
        # åŠ è½½wiki-collaborationçš„æ¨¡å—
        self.wiki_modules = self._load_wiki_modules()
        
    def _load_wiki_modules(self) -> Dict[str, Any]:
        """åŠ è½½wiki-collaborationçš„åŠŸèƒ½æ¨¡å—"""
        modules = {}
        
        try:
            # åŠ è½½ä»»åŠ¡åˆ†æå™¨
            task_analyzer_path = self.wiki_collaboration_path / "task_analyzer.py"
            if task_analyzer_path.exists():
                spec = importlib.util.spec_from_file_location("task_analyzer", task_analyzer_path)
                task_analyzer = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(task_analyzer)
                modules['task_analyzer'] = task_analyzer.WikiTaskAnalyzer()
            
            # åŠ è½½è®ºæ–‡æœç´¢å™¨
            paper_search_path = self.wiki_collaboration_path / "paper_search.py"
            if paper_search_path.exists():
                spec = importlib.util.spec_from_file_location("paper_search", paper_search_path)
                paper_search = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(paper_search)
                modules['paper_search'] = paper_search.AcademicSearchEngine()
            
            # åŠ è½½Wikiç”Ÿæˆå™¨
            wiki_generator_path = self.wiki_collaboration_path / "wiki_generator.py"
            if wiki_generator_path.exists():
                spec = importlib.util.spec_from_file_location("wiki_generator", wiki_generator_path)
                wiki_generator = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(wiki_generator)
                modules['wiki_generator'] = wiki_generator.WikiGenerator()
                
        except Exception as e:
            print(f"åŠ è½½æ¨¡å—æ—¶å‡ºé”™: {e}")
            
        return modules
    
    def execute_hierarchical_wiki_creation(self, topic: str) -> Dict[str, Any]:
        """æ‰§è¡Œå±‚æ¬¡åŒ–Wikiåˆ›å»º - çœŸå®é›†æˆç‰ˆæœ¬"""
        
        print(f"ğŸš€ å¯åŠ¨å±‚æ¬¡åŒ–Wikiåˆ›å»º: {topic}")
        
        # é˜¶æ®µ1: ä»»åŠ¡ç†è§£ä¸è§„åˆ’ (ä½¿ç”¨wiki-collaborationçš„task_analyzer)
        print("ğŸ“‹ é˜¶æ®µ1: ä»»åŠ¡ç†è§£ä¸è§„åˆ’")
        task_analysis = self._execute_task_analysis(topic)
        
        # é˜¶æ®µ2: ä¿¡æ¯æ”¶é›†ä¸å¤„ç† (ä½¿ç”¨wiki-collaborationçš„paper_search)
        print("ğŸ” é˜¶æ®µ2: ä¿¡æ¯æ”¶é›†ä¸å¤„ç†")
        papers_data = self._execute_paper_search(topic, task_analysis)
        
        # é˜¶æ®µ3: æ·±åº¦åˆ†æä¸æ€è€ƒ (åŸºäºæœç´¢ç»“æœè¿›è¡Œæ™ºèƒ½åˆ†æ)
        print("ğŸ§  é˜¶æ®µ3: æ·±åº¦åˆ†æä¸æ€è€ƒ")
        analysis_result = self._execute_deep_analysis(topic, papers_data, task_analysis)
        
        # é˜¶æ®µ4: ååŒå†…å®¹ç”Ÿæˆ (æ•´åˆåˆ†æç»“æœ)
        print("âœï¸ é˜¶æ®µ4: ååŒå†…å®¹ç”Ÿæˆ")
        content_structure = self._execute_content_generation(topic, analysis_result, task_analysis)
        
        # é˜¶æ®µ5: æœ€ç»ˆäº¤ä»˜ (ä½¿ç”¨wiki-collaborationçš„wiki_generator)
        print("ğŸ“¦ é˜¶æ®µ5: æœ€ç»ˆäº¤ä»˜")
        final_result = self._execute_final_delivery(topic, content_structure)
        
        return {
            'topic': topic,
            'task_analysis': task_analysis,
            'papers_data': papers_data,
            'analysis_result': analysis_result,
            'content_structure': content_structure,
            'final_result': final_result,
            'execution_time': datetime.now().isoformat(),
            'integration_status': 'success'
        }
    
    def _execute_task_analysis(self, topic: str) -> Dict[str, Any]:
        """æ‰§è¡Œä»»åŠ¡åˆ†æ"""
        if 'task_analyzer' not in self.wiki_modules:
            return self._fallback_task_analysis(topic)
            
        try:
            analyzer = self.wiki_modules['task_analyzer']
            result = analyzer.analyze_task(f"åˆ›å»º{topic}çš„Wikiç™¾ç§‘")
            
            # è´¨é‡å®¡æ ¸
                    quality_score = self._assess_task_analysis_quality(result)
                    if quality_score < 0.7:
                        print(f"âš ï¸ ä»»åŠ¡åˆ†æè´¨é‡è¾ƒä½({quality_score:.2f})ï¼Œä½¿ç”¨é»˜è®¤æ–¹æ¡ˆ")
                        # ä½¿ç”¨é»˜è®¤åˆ†æç»“æœï¼Œé¿å…å¤æ‚æ”¹è¿›é€»è¾‘            
            print(f"âœ… ä»»åŠ¡åˆ†æå®Œæˆï¼Œå¤æ‚åº¦: {result.get('complexity', 'æœªçŸ¥')}")
        return result
            
        except Exception as e:
            print(f"âŒ ä»»åŠ¡åˆ†æå¤±è´¥: {e}")
            return self._fallback_task_analysis(topic)
    
    def _execute_paper_search(self, topic: str, task_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œè®ºæ–‡æœç´¢"""
        if 'paper_search' not in self.wiki_modules:
            return self._fallback_paper_search(topic)
            
        try:
            searcher = self.wiki_modules['paper_search']
            
            # åŸºäºä»»åŠ¡åˆ†æä¼˜åŒ–æœç´¢å…³é”®è¯
            keywords = task_analysis.get('keywords', [topic])
            search_query = " ".join(keywords[:3])  # ä½¿ç”¨å‰3ä¸ªå…³é”®è¯
            
            print(f"   ğŸ” æœç´¢å…³é”®è¯: {search_query}")
            papers = searcher.search_papers(search_query, max_results=5)
            
            # è´¨é‡å®¡æ ¸
            if len(papers) < 3:
                print(f"âš ï¸ è®ºæ–‡æ•°é‡ä¸è¶³({len(papers)}ç¯‡)ï¼Œæ‰©å±•æœç´¢")
                papers.extend(searcher.search_papers(topic, max_results=5))
            
            print(f"âœ… æ‰¾åˆ°{len(papers)}ç¯‡ç›¸å…³è®ºæ–‡")
            return {
                'papers': papers,
                'search_query': search_query,
                'total_found': len(papers),
                'quality_score': min(len(papers) / 5.0, 1.0)
            }
            
        except Exception as e:
            print(f"âŒ è®ºæ–‡æœç´¢å¤±è´¥: {e}")
            return self._fallback_paper_search(topic)
    
    def _execute_deep_analysis(self, topic: str, papers_data: Dict[str, Any], task_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæ·±åº¦åˆ†æ"""
        papers = papers_data.get('papers', [])
        
        # å¤šè§’åº¦åˆ†æ
        analysis = {
            'academic_perspective': self._analyze_academic_perspective(papers),
            'technical_perspective': self._analyze_technical_perspective(papers, topic),
            'industry_perspective': self._analyze_industry_perspective(papers, topic),
            'integrated_insights': self._generate_integrated_insights(papers, topic)
        }
        
        # è´¨é‡å®¡æ ¸
        analysis_depth = self._assess_analysis_depth(analysis)
        if analysis_depth < 0.6:
            print(f"âš ï¸ åˆ†ææ·±åº¦ä¸è¶³ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ")
            # ä½¿ç”¨åŸºç¡€åˆ†æç»“æœï¼Œé¿å…å¤æ‚å¢å¼ºé€»è¾‘
        
        print(f"âœ… æ·±åº¦åˆ†æå®Œæˆï¼ŒåŒ…å«{len(analysis)}ä¸ªè§’åº¦")
        return analysis
    
    def _execute_content_generation(self, topic: str, analysis_result: Dict[str, Any], task_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå†…å®¹ç”Ÿæˆ"""
        # åŸºäºåˆ†æç»“æœç”Ÿæˆç« èŠ‚ç»“æ„
        sections = self._generate_sections_structure(topic, analysis_result)
        
        # ä¸ºæ¯ä¸ªç« èŠ‚ç”Ÿæˆå†…å®¹
        content = {}
        for section in sections:
            content[section] = self._generate_section_content(section, analysis_result, topic)
        
        # è´¨é‡å®¡æ ¸
        content_quality = self._assess_content_quality(content)
        if content_quality < 0.7:
            print(f"âš ï¸ å†…å®¹è´¨é‡éœ€è¦æ”¹è¿›ï¼Œä½¿ç”¨å¢å¼ºæ–¹æ¡ˆ")
            # ç®€åŒ–å¤„ç†ï¼šç›´æ¥ä½¿ç”¨ç°æœ‰å†…å®¹ï¼Œé¿å…å¤æ‚æ”¹è¿›é€»è¾‘
        
        print(f"âœ… å†…å®¹ç”Ÿæˆå®Œæˆï¼ŒåŒ…å«{len(content)}ä¸ªç« èŠ‚")
        return {
            'sections': sections,
            'content': content,
            'quality_score': content_quality
        }
    
    def _execute_final_delivery(self, topic: str, content_structure: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæœ€ç»ˆäº¤ä»˜"""
        if 'wiki_generator' not in self.wiki_modules:
            return self._fallback_html_generation(topic, content_structure)
            
        try:
            generator = self.wiki_modules['wiki_generator']
            
            # åˆ›å»ºWikié…ç½®
            config = generator.WikiConfig(
                title=f"{topic} - æ™ºèƒ½ç™¾ç§‘",
                description=f"å…³äº{topic}çš„å…¨é¢ä»‹ç»å’Œæ·±å…¥åˆ†æ",
                author="Hierarchical Wiki Creator"
            )
            
            # åˆ›å»ºç« èŠ‚
            sections = []
            for section_title, section_content in content_structure['content'].items():
                section = generator.WikiSection(
                    title=section_title,
                    content=section_content,
                    level=1
                )
                sections.append(section)
            
            # ç”ŸæˆHTML
            html_content = generator.generate_wiki(config, sections)
            
            # ä¿å­˜æ–‡ä»¶
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"{topic}_æ™ºèƒ½ç™¾ç§‘_{timestamp}.html"
            output_path = self.hierarchical_path / "outputs" / filename
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_path.parent.mkdir(exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            print(f"âœ… HTMLæ–‡ä»¶å·²ç”Ÿæˆ: {output_path}")
            
            return {
                'html_file': str(output_path),
                'filename': filename,
                'file_size': len(html_content),
                'sections_count': len(sections),
                'generation_method': 'integrated_wiki_generator'
            }
            
        except Exception as e:
            print(f"âŒ HTMLç”Ÿæˆå¤±è´¥: {e}")
            return self._fallback_html_generation(topic, content_structure)
    
    # è¾…åŠ©æ–¹æ³•
    def _assess_task_analysis_quality(self, result: Dict[str, Any]) -> float:
        """è¯„ä¼°ä»»åŠ¡åˆ†æè´¨é‡"""
        score = 0.0
        if result.get('topic'): score += 0.2
        if result.get('sections'): score += 0.3
        if result.get('keywords'): score += 0.2
        if result.get('complexity'): score += 0.2
        if result.get('domain'): score += 0.1
        return min(score, 1.0)
    
    def _assess_analysis_depth(self, analysis: Dict[str, Any]) -> float:
        """è¯„ä¼°åˆ†ææ·±åº¦"""
        required_perspectives = ['academic_perspective', 'technical_perspective', 'industry_perspective']
        score = 0.0
        for perspective in required_perspectives:
            if perspective in analysis and analysis[perspective]:
                score += 0.25
        if analysis.get('integrated_insights'):
            score += 0.25
        return min(score, 1.0)
    
    def _assess_content_quality(self, content: Dict[str, str]) -> float:
        """è¯„ä¼°å†…å®¹è´¨é‡"""
        if not content:
            return 0.0
        
        total_length = sum(len(text) for text in content.values())
        avg_length = total_length / len(content)
        
        score = 0.0
        if avg_length >= 200: score += 0.4  # æ¯ç« èŠ‚è‡³å°‘200å­—
        if len(content) >= 5: score += 0.3   # è‡³å°‘5ä¸ªç« èŠ‚
        if total_length >= 1500: score += 0.3 # æ€»å­—æ•°è‡³å°‘1500å­—
        
        return min(score, 1.0)
    
    # åå¤‡æ–¹æ¡ˆ
    def _fallback_task_analysis(self, topic: str) -> Dict[str, Any]:
        """ä»»åŠ¡åˆ†æåå¤‡æ–¹æ¡ˆ"""
        return {
            'original_task': f"åˆ›å»º{topic}çš„Wikiç™¾ç§‘",
            'task_type': 'wiki_creation',
            'topic': topic,
            'domain': 'ç»¼åˆ',
            'complexity': 'ä¸­ç­‰',
            'sections': ['æ¦‚è¿°', 'æ ¸å¿ƒæ¦‚å¿µ', 'åº”ç”¨é¢†åŸŸ', 'å‘å±•è¶‹åŠ¿'],
            'keywords': [topic],
            'suggestions': ['æ·»åŠ æ›´å¤šå®ä¾‹', 'è¡¥å……æŠ€æœ¯ç»†èŠ‚']
        }
    
    def _fallback_paper_search(self, topic: str) -> Dict[str, Any]:
        """è®ºæ–‡æœç´¢åå¤‡æ–¹æ¡ˆ"""
        return {
            'papers': [],
            'search_query': topic,
            'total_found': 0,
            'quality_score': 0.0,
            'fallback_reason': 'æœç´¢æ¨¡å—ä¸å¯ç”¨'
        }
    
    def _generate_sections_structure(self, topic: str, analysis_result: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆç« èŠ‚ç»“æ„"""
        return [
            'æ¦‚è¿°',
            'æ ¸å¿ƒåŸç†',
            'æŠ€æœ¯å®ç°',
            'åº”ç”¨é¢†åŸŸ',
            'å‘å±•å†ç¨‹',
            'ä¼˜åŠ¿ä¸å±€é™',
            'å‘å±•è¶‹åŠ¿',
            'å‚è€ƒæ–‡çŒ®'
        ]
    
    def _generate_section_content(self, section: str, analysis_result: Dict[str, Any], topic: str) -> str:
        """ç”Ÿæˆç« èŠ‚å†…å®¹"""
        # åŸºäºåˆ†æç»“æœç”Ÿæˆå†…å®¹ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
        content_templates = {
            'æ¦‚è¿°': f"{topic}æ˜¯ä¸€ä¸ªé‡è¦çš„æŠ€æœ¯/ç†è®ºæ¦‚å¿µï¼Œåœ¨ç›¸å…³é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨ä»·å€¼ã€‚é€šè¿‡ç»¼åˆåˆ†æç°æœ‰èµ„æ–™ï¼Œ{topic}åœ¨ç†è®ºåŸºç¡€ã€æŠ€æœ¯æ–¹æ³•å’Œåº”ç”¨å®è·µæ–¹é¢éƒ½å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚",
            'æ ¸å¿ƒåŸç†': f"{topic}çš„æ ¸å¿ƒåŸç†å»ºç«‹åœ¨å¤šä¸ªå­¦ç§‘çš„äº¤å‰èåˆä¹‹ä¸Šã€‚ä¸»è¦åŒ…æ‹¬åŸºç¡€ç†è®ºæ¡†æ¶ã€å…³é”®æŠ€æœ¯æ–¹æ³•å’Œå®ç°æœºåˆ¶ç­‰è¦ç´ ã€‚",
            'æŠ€æœ¯å®ç°': f"{topic}çš„æŠ€æœ¯å®ç°æ¶‰åŠå¤šä¸ªå…³é”®ç¯èŠ‚ã€‚ä¸»è¦åŒ…æ‹¬ç®—æ³•è®¾è®¡ã€ç³»ç»Ÿæ¶æ„ã€æ€§èƒ½ä¼˜åŒ–ç­‰æ–¹é¢ã€‚",
            'åº”ç”¨é¢†åŸŸ': f"{topic}åœ¨ä¼—å¤šé¢†åŸŸéƒ½æœ‰æˆåŠŸçš„åº”ç”¨å®è·µã€‚å…¸å‹åº”ç”¨åŒ…æ‹¬ç§‘å­¦ç ”ç©¶ã€å·¥ç¨‹æŠ€æœ¯ã€å•†ä¸šåº”ç”¨ç­‰ã€‚",
            'å‘å±•å†ç¨‹': f"{topic}çš„å‘å±•ç»å†äº†å¤šä¸ªé‡è¦é˜¶æ®µã€‚ä»æœ€åˆçš„ç†è®ºæå‡ºåˆ°ç°åœ¨çš„å¹¿æ³›åº”ç”¨ï¼Œæ¯ä¸€æ­¥éƒ½å‡èšäº†ç ”ç©¶è€…çš„æ™ºæ…§ã€‚",
            'ä¼˜åŠ¿ä¸å±€é™': f"{topic}å…·æœ‰æ˜¾è‘—çš„æŠ€æœ¯ä¼˜åŠ¿ï¼ŒåŒæ—¶ä¹Ÿé¢ä¸´ä¸€äº›æŒ‘æˆ˜ã€‚ä¸»è¦ä¼˜åŠ¿åŒ…æ‹¬æŠ€æœ¯å…ˆè¿›æ€§ã€åº”ç”¨å¹¿æ³›æ€§ç­‰ã€‚",
            'å‘å±•è¶‹åŠ¿': f"{topic}çš„æœªæ¥å‘å±•å……æ»¡æœºé‡å’ŒæŒ‘æˆ˜ã€‚æŠ€æœ¯å‘å±•è¶‹åŠ¿åŒ…æ‹¬ç†è®ºåˆ›æ–°ã€æŠ€æœ¯çªç ´ã€åº”ç”¨æ‹“å±•ç­‰ã€‚",
            'å‚è€ƒæ–‡çŒ®': f"æœ¬æ–‡å†…å®¹åŸºäºç›¸å…³å­¦æœ¯ç ”ç©¶å’Œå®è·µåº”ç”¨ç»¼åˆæ•´ç†ã€‚ä¸»è¦å‚è€ƒäº†ä¸“ä¸šæ–‡çŒ®ã€æŠ€æœ¯æ–‡æ¡£å’Œå®è·µæ¡ˆä¾‹ã€‚"
        }
        
        return content_templates.get(section, f"è¿™æ˜¯å…³äº{topic}ä¸­{section}çš„è¯¦ç»†å†…å®¹ã€‚")
    
    def _analyze_academic_perspective(self, papers: List[Any]) -> Dict[str, Any]:
        """å­¦æœ¯è§’åº¦åˆ†æ"""
        return {
            'theoretical_basis': 'åŸºäºç°æœ‰æ–‡çŒ®çš„ç†è®ºåŸºç¡€åˆ†æ',
            'research_methods': 'ä¸»è¦ç ”ç©¶æ–¹æ³•å’ŒæŠ€æœ¯è·¯çº¿',
            'academic_contributions': 'å­¦æœ¯è´¡çŒ®å’Œåˆ›æ–°ç‚¹',
            'future_research': 'æœªæ¥ç ”ç©¶æ–¹å‘'
        }
    
    def _analyze_technical_perspective(self, papers: List[Any], topic: str) -> Dict[str, Any]:
        """æŠ€æœ¯è§’åº¦åˆ†æ"""
        return {
            'technical_architecture': f'{topic}çš„æŠ€æœ¯æ¶æ„åˆ†æ',
            'key_algorithms': 'å…³é”®ç®—æ³•å’ŒæŠ€æœ¯æ–¹æ³•',
            'performance_analysis': 'æ€§èƒ½åˆ†æå’Œä¼˜åŒ–ç­–ç•¥',
            'implementation_challenges': 'å®ç°æŒ‘æˆ˜å’Œè§£å†³æ–¹æ¡ˆ'
        }
    
    def _analyze_industry_perspective(self, papers: List[Any], topic: str) -> Dict[str, Any]:
        """è¡Œä¸šè§’åº¦åˆ†æ"""
        return {
            'market_applications': f'{topic}çš„å¸‚åœºåº”ç”¨æƒ…å†µ',
            'business_value': 'å•†ä¸šä»·å€¼å’ŒæŠ•èµ„æœºä¼š',
            'industry_challenges': 'è¡Œä¸šæŒ‘æˆ˜å’Œé™åˆ¶å› ç´ ',
            'development_prospects': 'å‘å±•å‰æ™¯å’Œè¶‹åŠ¿'
        }
    
    def _generate_integrated_insights(self, papers: List[Any], topic: str) -> Dict[str, Any]:
        """ç”Ÿæˆæ•´åˆæ´å¯Ÿ"""
        return {
            'key_findings': 'å…³é”®å‘ç°å’Œç»“è®º',
            'interdisciplinary_connections': 'è·¨å­¦ç§‘è¿æ¥',
            'practical_implications': 'å®é™…æ„ä¹‰å’Œåº”ç”¨ä»·å€¼',
            'strategic_recommendations': 'æˆ˜ç•¥å»ºè®®å’Œè¡ŒåŠ¨æ–¹æ¡ˆ'
        }

def main():
    """ä¸»å‡½æ•° - æµ‹è¯•é›†æˆåŠŸèƒ½"""
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python skill_integrator.py <topic>")
        sys.exit(1)
    
    topic = sys.argv[1]
    integrator = SkillIntegrator()
    
    print(f"ğŸš€ å¼€å§‹é›†æˆæµ‹è¯•: {topic}")
    result = integrator.execute_hierarchical_wiki_creation(topic)
    
    print(f"\nâœ… é›†æˆæµ‹è¯•å®Œæˆ!")
    print(f"ğŸ“Š ä¸»é¢˜: {result['topic']}")
    print(f"ğŸ“ HTMLæ–‡ä»¶: {result['final_result'].get('html_file', 'æœªç”Ÿæˆ')}")
    print(f"ğŸ“ˆ é›†æˆçŠ¶æ€: {result['integration_status']}")

if __name__ == "__main__":
    main()