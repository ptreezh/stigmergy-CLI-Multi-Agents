{
  "topic": "机器学习",
  "strategic_plan_id": "",
  "execution_start": "2025-12-14T21:50:21.859159",
  "phases": [
    {
      "phase_id": 1,
      "name": "信息收集",
      "type": "tactical",
      "subtasks": [
        {
          "task_id": "1.1",
          "name": "网络搜索",
          "type": "quantitative",
          "command": "python scripts/data_collector.py --topic '{topic}' --mode 'search'",
          "search_results": [
            {
              "title": "Guest Editorial: Special Topic on Data-enabled Theoretical Chemistry",
              "url": "http://arxiv.org/abs/1806.02690v2",
              "snippet": "A survey of the contributions to the Special Topic on Data-enabled Theoretical Chemistry is given, including a glossary of relevant machine learning terms....",
              "content": "A survey of the contributions to the Special Topic on Data-enabled Theoretical Chemistry is given, including a glossary of relevant machine learning terms.",
              "authors": [
                "Matthias Rupp",
                "O. Anatole von Lilienfeld",
                "Kieron Burke"
              ],
              "published": "2018-06-07",
              "source": "arxiv"
            },
            {
              "title": "Topic Modelling Meets Deep Neural Networks: A Survey",
              "url": "http://arxiv.org/abs/2103.00498v1",
              "snippet": "Topic modelling has been a successful technique for text analysis for almost twenty years. When topic modelling met deep neural networks, there emerged a new and increasingly popular research area, ne...",
              "content": "Topic modelling has been a successful technique for text analysis for almost twenty years. When topic modelling met deep neural networks, there emerged a new and increasingly popular research area, neural topic models, with over a hundred models developed and a wide range of applications in neural language understanding such as text generation, summarisation and language models. There is a need to summarise research developments and discuss open problems and future directions. In this paper, we provide a focused yet comprehensive overview of neural topic models for interested researchers in the AI community, so as to facilitate them to navigate and innovate in this fast-growing research area. To the best of our knowledge, ours is the first review focusing on this specific topic.",
              "authors": [
                "He Zhao",
                "Dinh Phung",
                "Viet Huynh",
                "Yuan Jin",
                "Lan Du",
                "Wray Buntine"
              ],
              "published": "2021-02-28",
              "source": "arxiv"
            },
            {
              "title": "A Bimodal Network Approach to Model Topic Dynamics",
              "url": "http://arxiv.org/abs/1709.09373v1",
              "snippet": "This paper presents an intertemporal bimodal network to analyze the evolution of the semantic content of a scientific field within the framework of topic modeling, namely using the Latent Dirichlet Al...",
              "content": "This paper presents an intertemporal bimodal network to analyze the evolution of the semantic content of a scientific field within the framework of topic modeling, namely using the Latent Dirichlet Allocation (LDA). The main contribution is the conceptualization of the topic dynamics and its formalization and codification into an algorithm. To benchmark the effectiveness of this approach, we propose three indexes which track the transformation of topics over time, their rate of birth and death, and the novelty of their content. Applying the LDA, we test the algorithm both on a controlled experiment and on a corpus of several thousands of scientific papers over a period of more than 100 years which account for the history of the economic thought.",
              "authors": [
                "Luigi Di Caro",
                "Marco Guerzoni",
                "Massimiliano Nuccio",
                "Giovanni Siragusa"
              ],
              "published": "2017-09-27",
              "source": "arxiv"
            },
            {
              "title": "A Topic Model Approach to Multi-Modal Similarity",
              "url": "http://arxiv.org/abs/1405.6886v1",
              "snippet": "Calculating similarities between objects defined by many heterogeneous data modalities is an important challenge in many multimedia applications. We use a multi-modal topic model as a basis for defini...",
              "content": "Calculating similarities between objects defined by many heterogeneous data modalities is an important challenge in many multimedia applications. We use a multi-modal topic model as a basis for defining such a similarity between objects. We propose to compare the resulting similarities from different model realizations using the non-parametric Mantel test. The approach is evaluated on a music dataset.",
              "authors": [
                "Rasmus Troelsgård",
                "Bjørn Sand Jensen",
                "Lars Kai Hansen"
              ],
              "published": "2014-05-27",
              "source": "arxiv"
            },
            {
              "title": "Evaluating topic coherence measures",
              "url": "http://arxiv.org/abs/1403.6397v1",
              "snippet": "Topic models extract representative word sets - called topics - from word counts in documents without requiring any semantic annotations. Topics are not guaranteed to be well interpretable, therefore,...",
              "content": "Topic models extract representative word sets - called topics - from word counts in documents without requiring any semantic annotations. Topics are not guaranteed to be well interpretable, therefore, coherence measures have been proposed to distinguish between good and bad topics. Studies of topic coherence so far are limited to measures that score pairs of individual words. For the first time, we include coherence measures from scientific philosophy that score pairs of more complex word subsets and apply them to topic scoring.",
              "authors": [
                "Frank Rosner",
                "Alexander Hinneburg",
                "Michael Röder",
                "Martin Nettling",
                "Andreas Both"
              ],
              "published": "2014-03-25",
              "source": "arxiv"
            }
          ],
          "metrics": {
            "search_time": 0,
            "result_count": 5,
            "success_rate": 1.0,
            "quality_score": 1.0
          },
          "status": "completed",
          "execution_time": 6.0585386753082275
        },
        {
          "error": "未知的定性任务类型",
          "execution_time": 0.0
        }
      ],
      "start_time": "2025-12-14T21:50:21.859159",
      "metrics": {},
      "end_time": "2025-12-14T21:50:27.917697",
      "duration": 6.058538
    },
    {
      "phase_id": 2,
      "name": "深度分析",
      "type": "tactical",
      "subtasks": [
        {
          "task_id": "2.1",
          "name": "专家观点生成",
          "type": "qualitative",
          "insights": [
            "{topic}在技术发展上具有显著的创新性和前瞻性",
            "从应用角度看，{topic}解决了多个传统方法的局限性",
            "{topic}的理论基础扎实，实践价值突出",
            "未来{topic}的发展潜力巨大，值得持续关注"
          ],
          "metrics": {
            "generation_time": 0,
            "insight_count": 4,
            "confidence": 0.8
          },
          "status": "completed",
          "execution_time": 0.0
        },
        {
          "task_id": "2.2",
          "name": "深度思考分析",
          "type": "qualitative",
          "analysis": [
            "{topic}的技术架构设计合理，具有良好的扩展性",
            "在性能优化方面，{topic}展现了显著的改进",
            "{topic}的应用场景广泛，覆盖多个重要领域",
            "从发展趋势看，{topic}将持续发挥重要作用"
          ],
          "metrics": {
            "analysis_time": 0,
            "insight_count": 4,
            "depth_score": 0.85
          },
          "status": "completed",
          "execution_time": 0.0
        }
      ],
      "start_time": "2025-12-14T21:50:27.917697",
      "metrics": {},
      "end_time": "2025-12-14T21:50:27.917697",
      "duration": 0.0
    },
    {
      "phase_id": 3,
      "name": "内容生成",
      "type": "operational",
      "subtasks": [
        {
          "error": "未知的定量任务类型",
          "execution_time": 0.0
        },
        {
          "task_id": "3.2",
          "name": "专业撰写",
          "type": "qualitative",
          "content": "机器学习专业分析报告\n\n\n\n基于以下搜索结果和文献资料：\n\n1. Guest Editorial: Special Topic on Data-enabled Theoretical Chemistry\n   内容摘要: A survey of the contributions to the Special Topic on Data-enabled Theoretical Chemistry is given, including a glossary of relevant machine learning terms....\n   详细内容: A survey of the contributions to the Special Topic on Data-enabled Theoretical Chemistry is given, including a glossary of relevant machine learning terms....\n\n2. Topic Modelling Meets Deep Neural Networks: A Survey\n   内容摘要: Topic modelling has been a successful technique for text analysis for almost twenty years. When topic modelling met deep neural networks, there emerged a new and increasingly popular research area, ne...\n   详细内容: Topic modelling has been a successful technique for text analysis for almost twenty years. When topic modelling met deep neural networks, there emerged a new and increasingly popular research area, neural topic models, with over a hundred models developed and a wide range of applications in neural l...\n\n3. A Bimodal Network Approach to Model Topic Dynamics\n   内容摘要: This paper presents an intertemporal bimodal network to analyze the evolution of the semantic content of a scientific field within the framework of topic modeling, namely using the Latent Dirichlet Al...\n   详细内容: This paper presents an intertemporal bimodal network to analyze the evolution of the semantic content of a scientific field within the framework of topic modeling, namely using the Latent Dirichlet Allocation (LDA). The main contribution is the conceptualization of the topic dynamics and its formali...\n\n\n核心原理与技术机制：\n机器学习的核心技术建立在先进的算法理论和数学模型基础上。通过分析现有研究和实践应用，可以发现其技术架构具有以下特点：\n\n1. 理论基础：机器学习的理论基础涉及多个学科领域，包括数学、计算机科学和相关应用学科。这些理论基础为机器学习的技术发展提供了坚实的支撑。\n\n2. 技术实现：在技术实现层面，机器学习采用了多种先进的技术手段和优化策略。这些技术方案不仅提高了系统的性能，还增强了其适应性和可扩展性。\n\n3. 应用场景：机器学习在多个领域都有重要的应用价值。从学术研究到工业应用，从技术创新到教育培训，机器学习都展现出了强大的实用性和发展潜力。\n\n4. 发展趋势：当前机器学习正处于快速发展的阶段。未来的发展方向包括技术优化、应用拓展、标准化建设等方面。\n\n专业评估：\n基于对现有研究和应用案例的分析，机器学习在技术成熟度、市场接受度和未来发展前景方面都表现出积极态势。建议重点关注跨学科融合、实际应用深化和标准化推进。",
          "metrics": {
            "writing_time": 0,
            "word_count": 2025,
            "professional_score": 0.85
          },
          "status": "completed",
          "execution_time": 0.0
        }
      ],
      "start_time": "2025-12-14T21:50:27.917697",
      "metrics": {},
      "end_time": "2025-12-14T21:50:27.917697",
      "duration": 0.0
    },
    {
      "phase_id": 4,
      "name": "质量控制",
      "type": "operational",
      "subtasks": [
        {
          "task_id": "4.1",
          "name": "质量评估",
          "type": "quantitative",
          "status": "completed",
          "execution_time": 0.0
        },
        {
          "error": "未知的定性任务类型",
          "execution_time": 0.0
        }
      ],
      "start_time": "2025-12-14T21:50:27.917697",
      "metrics": {},
      "end_time": "2025-12-14T21:50:27.917697",
      "duration": 0.0
    }
  ],
  "search_results": [
    {
      "title": "Guest Editorial: Special Topic on Data-enabled Theoretical Chemistry",
      "url": "http://arxiv.org/abs/1806.02690v2",
      "snippet": "A survey of the contributions to the Special Topic on Data-enabled Theoretical Chemistry is given, including a glossary of relevant machine learning terms....",
      "content": "A survey of the contributions to the Special Topic on Data-enabled Theoretical Chemistry is given, including a glossary of relevant machine learning terms.",
      "authors": [
        "Matthias Rupp",
        "O. Anatole von Lilienfeld",
        "Kieron Burke"
      ],
      "published": "2018-06-07",
      "source": "arxiv"
    },
    {
      "title": "Topic Modelling Meets Deep Neural Networks: A Survey",
      "url": "http://arxiv.org/abs/2103.00498v1",
      "snippet": "Topic modelling has been a successful technique for text analysis for almost twenty years. When topic modelling met deep neural networks, there emerged a new and increasingly popular research area, ne...",
      "content": "Topic modelling has been a successful technique for text analysis for almost twenty years. When topic modelling met deep neural networks, there emerged a new and increasingly popular research area, neural topic models, with over a hundred models developed and a wide range of applications in neural language understanding such as text generation, summarisation and language models. There is a need to summarise research developments and discuss open problems and future directions. In this paper, we provide a focused yet comprehensive overview of neural topic models for interested researchers in the AI community, so as to facilitate them to navigate and innovate in this fast-growing research area. To the best of our knowledge, ours is the first review focusing on this specific topic.",
      "authors": [
        "He Zhao",
        "Dinh Phung",
        "Viet Huynh",
        "Yuan Jin",
        "Lan Du",
        "Wray Buntine"
      ],
      "published": "2021-02-28",
      "source": "arxiv"
    },
    {
      "title": "A Bimodal Network Approach to Model Topic Dynamics",
      "url": "http://arxiv.org/abs/1709.09373v1",
      "snippet": "This paper presents an intertemporal bimodal network to analyze the evolution of the semantic content of a scientific field within the framework of topic modeling, namely using the Latent Dirichlet Al...",
      "content": "This paper presents an intertemporal bimodal network to analyze the evolution of the semantic content of a scientific field within the framework of topic modeling, namely using the Latent Dirichlet Allocation (LDA). The main contribution is the conceptualization of the topic dynamics and its formalization and codification into an algorithm. To benchmark the effectiveness of this approach, we propose three indexes which track the transformation of topics over time, their rate of birth and death, and the novelty of their content. Applying the LDA, we test the algorithm both on a controlled experiment and on a corpus of several thousands of scientific papers over a period of more than 100 years which account for the history of the economic thought.",
      "authors": [
        "Luigi Di Caro",
        "Marco Guerzoni",
        "Massimiliano Nuccio",
        "Giovanni Siragusa"
      ],
      "published": "2017-09-27",
      "source": "arxiv"
    },
    {
      "title": "A Topic Model Approach to Multi-Modal Similarity",
      "url": "http://arxiv.org/abs/1405.6886v1",
      "snippet": "Calculating similarities between objects defined by many heterogeneous data modalities is an important challenge in many multimedia applications. We use a multi-modal topic model as a basis for defini...",
      "content": "Calculating similarities between objects defined by many heterogeneous data modalities is an important challenge in many multimedia applications. We use a multi-modal topic model as a basis for defining such a similarity between objects. We propose to compare the resulting similarities from different model realizations using the non-parametric Mantel test. The approach is evaluated on a music dataset.",
      "authors": [
        "Rasmus Troelsgård",
        "Bjørn Sand Jensen",
        "Lars Kai Hansen"
      ],
      "published": "2014-05-27",
      "source": "arxiv"
    },
    {
      "title": "Evaluating topic coherence measures",
      "url": "http://arxiv.org/abs/1403.6397v1",
      "snippet": "Topic models extract representative word sets - called topics - from word counts in documents without requiring any semantic annotations. Topics are not guaranteed to be well interpretable, therefore,...",
      "content": "Topic models extract representative word sets - called topics - from word counts in documents without requiring any semantic annotations. Topics are not guaranteed to be well interpretable, therefore, coherence measures have been proposed to distinguish between good and bad topics. Studies of topic coherence so far are limited to measures that score pairs of individual words. For the first time, we include coherence measures from scientific philosophy that score pairs of more complex word subsets and apply them to topic scoring.",
      "authors": [
        "Frank Rosner",
        "Alexander Hinneburg",
        "Michael Röder",
        "Martin Nettling",
        "Andreas Both"
      ],
      "published": "2014-03-25",
      "source": "arxiv"
    }
  ],
  "analysis_results": [],
  "literature_data": [],
  "downloaded_papers": [
    {
      "index": 1,
      "title": "Guest Editorial: Special Topic on Data-enabled Theoretical Chemistry",
      "authors": [
        "Matthias Rupp",
        "O. Anatole von Lilienfeld",
        "Kieron Burke"
      ],
      "published": "2018-06-07",
      "source": "arxiv",
      "url": "http://arxiv.org/abs/1806.02690v2",
      "pdf_url": "http://arxiv.org/pdf/1806.02690v2.pdf",
      "pdf_path": "papers\\01_Guest Editorial_ Special Topic on Data-enabled Theoretical Chemistry.pdf",
      "txt_path": "papers\\01_Guest Editorial_ Special Topic on Data-enabled Theoretical Chemistry.txt",
      "content": "--- 第1页 ---\nGuest Editorial: Special Topic on Data-enabled Theoretical Chemistry\nMatthias Rupp,1,a)O. Anatole von Lilienfeld,2,b)and Kieron Burke3,c)\n1)Fritz Haber Institute of the Max Planck Society, Faradayweg 4{6, 14195 Berlin, Germany\n2)Department of Chemistry, Institute of Physical Chemistry and National Center for Computational\nDesign and Discovery of Novel Materials, University of Basel, 4056 Basel, Switzerland\n3)Departments of Chemistry and of Physics, University of California, Irvine, CA 92697, USA\n(Published in The Journal of Chemical Physics 148(24): 241401, 2018. DOI: 10.1063/1.5043213)\nA survey of the contributions to the Special Topic on Data-enabled Theoretical Chemistry is given, including\na glossary of relevant machine learning terms.\nNOMENCLATURE\nAI Arti\fcial Intelligence, see Sec. II A\nB3LYP Becke, three-parameter, Lee-Yang-Parr, a\nhybrid DFT functional\nCCSD(T) Coupled Cluster with Single, Double and\nperturbative Triple excitations, an electronic\nstructure method\nDFT Density Functional Theory, an electronic\nstructure method\nDFTB Density Functional Theory Tight Binding,\nan electronic structure method\nDNN Deep Neural Network, see Sec. II C\nEAM Embedded Atom Model/Method, an inter-\natomic potential\nGAP Gaussian Approximation Potential, a ma-\nchine learning potential\nHOMO Highest Occupied Molecular Orbital\nKRR Kernel Ridge regression, see Sec. II C\nLUMO Lowest Unoccupied Molecular Orbital\nMAE Mean Absolute Error, see Sec. II D\nMD Molecular Dynamics, a simulation technique\nML Machine Learning, see Sec. II A\nMP2 M\u001cller-Plesset perturbation theory to Sec-\nond order, an electronic structure method\nQM/MM Quantum Mechanics/Molecular Mechanics,\na molecular simulation method\n(A)NN (Arti\fcial) Neural Network, see Sec. II C\nQSPR Quantitative Structure-Property Relation-\nship, see Sec. II A\nRMSE Root Mean Squared Error, see Sec. II D\nSINDy Sparse Identi\fcation of Nonlinear Dynamics,\na machine learning method\nSNAP Spectral Neighbor Analysis Potential, a ma-\nchine learning potential\nSVM Support Vector Machine, see Sec. II C\ntICA time structure Independent Component\nAnalysis, see Sec. II C\na)matthias.rupp@fhi-berlin.mpg.de; http://www.mrupp.info\nb)anatole.vonlilienfeld@unibas.ch\nc)kieron@uci.eduI. Introduction\nWelcome to the Journal of Chemical Physics Special\nTopic on data-enabled theoretical chemistry. We expect\nthat this will be a timely addition to this new and rapidly\nevolving \feld, with a variety of articles from the front\nlines.\nUnless you have disconnected from all social media,\nyou will have noticed that arti\fcial intelligence, machine\nlearning, big data, and other vague but computer-driven\nterms have invaded many realms of public life. Facial\nrecognition software has been revolutionized by machine\nlearning, cars now drive themselves, the world's best\nchess and go players are algorithms, and perhaps some-\nday soon they'll even be able to recommend a good movie.\nThe same revolution has also been occurring in many\nbranches of theoretical and computational chemistry,\ndriven by the same force: the never-ending increase in\ndata being generated by computers. Our Special Topic\nis devoted to data-enabled chemistry, which we interpret\nbroadly. We cover essentially all algorithmic develop-\nments that \ft under the broad rubric of machine learning,\nusing varying amounts of data, and driven by applica-\ntions from small molecule chemistry to materials science\nto protein behavior.\n1993 1997 2001 2005 2009 2013 20170100200300400500\nyear#publications\nFIG. 1. Number of publications per year from a web of sci-\nence search for articles with topics of machine learning and\neither chemistry or materials, taken June 5, 2018. The aver-\nage number of citations per article is 12.arXiv:1806.02690v2  [physics.chem-ph]  3 Jul 2018\n\n--- 第2页 ---\n2TABLE I. Overview of contributions to the Special Topic.\nRef. Sec. ML Method QM Method Systems Keywords\n30 III A NN DFT Hydrocarbon molecules Size-independence\n31 III A Multilinear regression DFT Small organic molecules Representation, wavelets\n32 III A KRR DFT Organic molecules, water, solids Representation, many-body terms\n33 III A NN DFT Small organic molecules NN architecture\n34 III A NN DFT Small organic molecules Representation, symmetry func-\ntions\n35 III A Regression DFT Small organic molecules Polynomial \ft, active learning\n36 III A KRR DFT Small organic molecules Graph-based representation\n76 III A NN DFT Organic molecules Covariant compositional networks\n37 III B KRR DFT, CCSD(T)Dimers, hydrogen-bonded\ncomplexes, and othersNon-covalent interactions\n38 III B GPR, NN DFTLiquid water, Al-Si-Mg alloy,\norganic moleculesFeature selection\n39 III B GPR DFT Li-C guest-host systems Combination of potentials\n40 III B NN DFT Small organic molecules Active learning\n41 III B NN DFT Small organic molecules Molecular properties\n42 III B DNN DFTOrganic molecules, bulk\ncrystals, C 20-fullereneDNN architecture\n43 III B GPR DFT, force \feld Na+, Cl\u0000ion-water clusters Ion-water interactions\n44 III BRegularized linear\nregressionDFT Tantalum Bispectrum quadratic terms\n45 III B GPR DFT Ni nanoclusters Interatomic forces, k-body kernels\n46 III B NN DFT Nicotine, water cluster Sampling, meta-dynamics\n47 III B NN DFT Cu surface grain boundaries Hybrid QM/ML models\n48 III C NN DFT Water/ZnO(10 \u001610) interface Anharmonic vibrational spectra\n49 III C Linear regression CCSD(T) Formic acid dimer Dipole moment surface, infrared\nspectrum\n50 III C NN, GPR CCSD(T) Water (ice, liquid, clusters) Representation, invariant polyno-\nmials\n51 III C NN, GPR Force \feld Formaldehyde Comparison, vibrational spectra\n52 III D NN, genetic algorithm DFT Li xSi alloysPhase diagrams of amorphous\nmaterials\n53 III D Regression trees DFT AB 2C2ternary intermetallics Stable compound search\n54 III D Clustering Harris approximation Rigid-molecule crystals Crystal structure prediction\n55 III D Monte Carlo tree search EAM Ag, Co grain boundaries Segregation\n56 III D Binary classi\fcation trees DFT Inorganic crystals Recommender system\n57 III DMonte Carlo tree search,\nGPRDFT Boron-doped graphene Stable structure search\n58 III ESubset selection, outlier\ndetectionDFT Main group chemistry Doubly hybrid functional\n60 III E NN DFT Model systemsHartree-exchange-correlation\npotential\n62 III E KRR DFT Organic molecules Representation\n63 III E KRR DFT Model systems Exact conditions\n64 III E NN DFT Atoms and molecules Kinetic energy density functional\n65 III F Sparse regression Analytic potential Model systems Stochastic dynamical equations\n66 III F Time-lagged autoencoder Force \feld Model systems, villin peptideSlow dynamics, dimensionality\nreduction\n67 III F Markov state model,tICA Force \feld Dye-labeled polyproline-20 Dynamics, transition probabilities\n68 III G None DFT Various (G3/99 test set) Error statistics\n69 III G Autoencoder, NN DFT Donor-acceptor polymers Screening, solar cells\n70 III G SVM DFT Organic polymers Refraction index\n71 III G KRR DFTPerovskite oxides, elpasolite\nhalidesLanthanide-doped scintillators\n72 III G GPR CCSD(T) Small organic molecules Geometry optimization\n73 III G Clustering DFTB Anatase TiO 2(001) Global structure optimization\n74 III G SVM, graph analysis Force \feld Tyrosine phosphatase 1E Proteins, dynamic allostery\n75 III G Data analysis Force \feld Antimicrobial peptides Visualization\n\n--- 第3页 ---\n3\nIn Fig. 1, we show papers being published involving\nmachine learning and chemistry or materials over the last\nthree decades. The absolute rate is rather arbitrary, de-\npending on the precise search terms, but the rapid growth\nis robust, as is the average citation rate of each article.\nThere is no doubt that data-enabled chemistry is rapidly\nmaking a large impact in the \feld.\nThis editorial is designed for non-experts who are out-\nside this \feld, and trying to \fgure out what is going\non and how they might want to get in on the action.\nWe provide a brief glossary of machine-learning terms for\nnon-experts in Sec. II, focusing on the concepts and algo-\nrithms used most often in physical chemistry and materi-\nals science. In Sec. III, using the introduced terminology,\nwe brie\ry survey the contributions in this Special Topic,\ngrouped by the physical and chemical processes and sys-\ntems to which they are applied.\nA nomenclature and a table are provided to aid the\nreader: The Nomenclature summarizes the used abbre-\nviations and Table I presents an overview of all articles\nin the Special Topic, acting as a quick guide to the meth-\nods (both quantum chemical and computer science) and\nthe systems included. Not only is it a quick way to \fnd\nsomething in the issue, but it also represents a snapshot\nof the state of the \feld today.\nII. SOME DATA-ENABLED TERMINOLOGY\nThis section is an introduction to common terminology\nin machine learning, with an emphasis on those concepts\ncurrently in use in the applications in this Special Topic.\nTerms used both in this editorial and throughout the\nSpecial Topic are set in small capitals, followed by their\nexplanation. This is by no means a comprehensive ex-\nplanation, and interested readers should consult further\nsources for more detailed explanations.\nA. Machine learning and related scienti\fc \felds\nMachine learning (ML)1,2is an umbrella term re-\nferring to algorithms that improve with data (\"learn from\nexperience\"),3mostly for analysis or prediction. Instead\nof being explicitly programmed to solve a speci\fc prob-\nlem, these algorithms rely on given data to make state-\nments about new data. An example for a ML algorithm\nis regression (Fig. 2): Based on a \fnite number of points\n(examples ,samples ), a function is inferred which en-\nables predictions for new examples; the \ft gets better\nthe more examples there are. While ML encompasses\nmany di\u000berent tasks besides regression, such as classi\f-\ncation, dimensionality reduction, clustering, anomaly de-\ntection, optimization, and o\u000bers a wide variety of speci\fc\nalgorithms, such as Gaussian process regression, support\nvector machines, principal component analysis, (deep)\nneural networks, the underlying principle of data-driven\nimprovement remains the same.\nML is related to, but distinct from, arti\fcial intel-\nligence and data mining. Artificial intelligence\n(AI)5is the study of machines that exhibit intelligent\nbehavior. The scope of this \feld is less clear-cut, evi-\ndenced by the lack of a formal de\fnition of intelligence.\nAItraditionally involves (symbolic) knowledge represen-\ntation and logical reasoning. Data mining is similar to\nproperty●\n●●\n●●structureFIG. 2. Sketch illustrating the idea of machine learning ,4us-\ning prediction of molecular energies as an example. The hor-\nizontal axis represents molecular space (molecules are points\non the axis), the vertical axis represents energy. Instead of\ncalculating all energies (solid line), only a few reference calcu-\nlations are done (dots), and machine learning is used to learn\nthe mapping from molecule to energy (dashed line).\nML, but more concerned with extraction of new patterns\nin large datasets. Pattern recognition is essentially\na synonym for ML. For the more recent term data sci-\nence , no consensus has emerged yet, but it is often used\nto mean applied MLand statistics.\nTwo major application areas of ML closely related\nto this Special Topic are cheminformatics and materi-\nals informatics. Cheminformatics6(also chemoinfor-\nmatics) is at the intersection of chemistry and com-\nputer science. In particular, quantitative structure-\nproperty relationships (QSPR)7relate molecular\nfeatures or descriptors to, usually experimental, molec-\nular properties, and virtual screening8is the com-\nputational screening of large databases for compounds\nwith desired properties. Materials informatics9is a\nnewer \feld at the intersection of materials science and\ncomputer science.\nB. Types of problems machine learning addresses\nOne way to categorize problem types in ML is accord-\ning to the type of examples involved. In supervised\nlearning , examples are pairs of input xand labely, for\nexample molecules and their energy, and the task is to\npredict the label of new examples, that is to learn the\nfunctionf:x!y. Inunsupervised learning , only\ninputsxare given, and the task is to \fnd structure in\nthe data. An example would be identifying a reaction\ncoordinate from molecular dynamics (MD) data. Mixed\nforms are possible as well: In semi-supervised learn-\ning, only some examples are labeled, the idea being that\nlarge amounts of unlabeled data can still help with pre-\ndictions by characterizing the manifold the data lie on.\nAn example would be a large combinatorial chemistry\ndatabase of molecules where only some have been mea-\nsured or calculated.\n\n--- 第4页 ---\n4\nFrequent types of problems within supervised learn-\ning are classi\fcation and regression. In classification ,\nlabels belong to a \fnite set of outcomes, where one dis-\ntinguishes between two possible labels in binary classi-\nfication , for example active and inactive, and, multiple\npossible labels in multi-class classification , for ex-\nample di\u000berent phases. The special case with only one\npossible label is one-class learning (also novelty\ndetection ,outlier detection , oranomaly detec-\ntion ), where examples from a single class are given and\nthe task is to detect whether new examples fall outside of\nthis class or not. In regression , labels are continuous.\nUsually, these are scalar values, but vectors, distributions\nor other structured objects like graphs can also be pre-\ndicted using structured-output learning10.\nFrequent problem types within unsupervised learning\nare dimensionality reduction and clustering. In dimen-\nsionality reduction11the goal is to \fnd a subspace or\nmanifold of low dimension on which the data live. Clus-\ntering attempts to group samples into clusters such that\nsamples within a cluster are more similar to each other\nthan to samples in other clusters.\nThere are many other concepts that have found their\nway into data-enabled theoretical chemistry and mate-\nrials science: In active learning12, the training data\nare not sampled randomly but \\actively\" chosen by the\nMLalgorithm; this often enables achieving the same pre-\ndiction error with much smaller training sets. In rein-\nforcement learning , the ML algorithm chooses an\naction from a set of possible actions based on the state of\nits environment. It is then rewarded accordingly and the\nprocess repeats. The goal of the algorithm is to maximize\nreward.\nC. Speci\fc algorithms\nMany ML algorithms exist, but the ones used most\noften in cheminformatics and materials informatics be-\nlong to two large families, kernel-based ML and (deep)\narti\fcial neural networks.\nInkernel-based ML ,13,14inputsxare non-linearly\ntransformed into a higher-dimensional space, where prob-\nlems can become linear with the right transformation.\nAs working directly in these high-dimensional feature\nspaces is impractical, kernel functions kare used. These\nare computed in the original input space, but yield in-\nner product values, and thus geometric information, in\nthe high-dimensional space. Since their invention in the\n1990s,15,16many linear ML algorithms have been \\kernel-\nized\". Popular algorithms include support vector ma-\nchines (SVM), kernel principal component anal-\nysis16,15kernel ridge regression (KRR),17and\nGaussian process regression (GPR)18(also called\nKriging due to its origins in geostatistics). While KRR\nis a frequentist algorithm and GPR is a Bayesian one,\ntheir predictions are formally identical, which is why the\nterms KRR and GPR are occasionally used interchange-\nably in practice.\nArti\fcial neural networks (NNs)19,20are repeated\ncompositions of simple functions, where the input of onefunction are the weighted outputs of other functions.\nThese functions are typically arranged in consecutive lay-\ners. In graph representations of NNs, vertices correspond\nto functions and edges correspond to weighted connec-\ntions between them. Determining the weights is a non-\nconvex optimization problem. Deep NNs (DNNs)21are\ncharacterized by having many layers of functions. This\ndepth enables them to learn internal representations of\nthe data of increasing complexity and abstraction.\nKernel learning and NN are simply two di\u000berent ways\nof \ftting a \rexible function to data. Many other learning\nalgorithms exist, including tree-based algorithms such\nasdecision trees ,regression trees , and random\nforests .\nA classic algorithm for dimensionality reduction is\nprincipal component analysis (PCA),22,23which\n\fnds orthogonal directions of maximal variance in the\ndata. Many variants of this idea exist, such as Inde-\npendent Component Analysis (ICA), which \fnds inde-\npendent latent variables and explains data as mixtures\nof these variables. For time-structure Independent\nComponent Analysis (tICA), these variables are cho-\nsen to maximize autocorrelation. A NN approach to\ndimensionality reduction are autoencoder networks,\nwhere the size of function layers \frst decreases, then in-\ncreases again and the task is to reproduce the inputs.\nHaving the data go through a \\bottleneck\" forces the au-\ntoencoder NN to \fnd a low-dimensional representation of\nthe data.\nD. Model building\nUnlike classical potentials, which are parametrized\nonce for a class of molecules or materials and then de-\nployed, ML models, being more \rexible mathematical\nfunctions, should be applied only to molecules or ma-\nterials sampled from the same distribution as the ones\nused to train the model|otherwise, the ML model will\noperate outside of its domain of applicability , result-\ning in uncontrolled and essentially arbitrary errors. For\nthis reason, ML models are often retrained, for example\ndynamically by adding training data \\on-the-\ry\" during\nthe course of a simulation. Deciding when to make a\nprediction and when to do a reference calculation to up-\ndate the model requires uncertainty estimates , that\nis, assessments of the reliability of individual predictions.\nTheroot mean squared error (RMSE) is the\ncanonical measure of how wrong a set of predictions is.\nIt is the RMSE that is minimized by many algorithms\nby default. This typically leads to \\full\" solutions, such\nas all coe\u000ecients in an expansion being non-zero. By\ncontrast, sparsity of solutions, that is, solutions with\nmost coe\u000ecients zero, can be achieved by minimizing the\nmean absolute error (MAE) orL1-norm instead.\nForvalidation of a ML model, the errors reported\nmust always be on out-of-sample data, that is, data\nnot used for training the model, including any pre-\nprocessing steps. An easy way to achieve this is to set\naside a hold-out set in the beginning, to be used only\nfor validation, and only after the ML model's training is\n\n--- 第5页 ---\n5\ncomplete. For small datasets, where this might not be\nfeasible, statistical validation techniques such as cross-\nvalidation can be used. These essentially reuse the data\nby splitting it multiple times into a training and a hold-\nout set, then average over the results.\nThe training or model-building process can include\nsteps such as optimizing free parameters, often called hy-\nperparameters , or,feature selection , where only\nsome of the descriptors or variables used to represent the\ninputs are retained. Hyperparameter optimization\nusually is a non-convex optimization problem, but well-\nbehaved in practice. For few parameters, it can be ad-\ndressed via grid search, minimizing the hold-out RMSE\nover a logarithmic grid; alternatives are maximizing the\nlikelihood of the model given the data, or choosing good\nvalues via heuristics.\nThe out-of-sample error of ML models must decay with\ntraining set size (otherwise it would not be machine learn-\ning). For many models, the leading error term varies as\na=Nb, whereNis number of training data.24{26Learn-\ning curves are plots of the out-of-sample prediction er-\nror as a function of N, usually on a log-log scale.\nIII. SURVEY OF AREAS COVERED\nWe next survey the areas covered by the articles in our\nSpecial Topic. We have organized them according to the\ntype of chemical problem being addressed, as far as is\npossible. This makes it easier to see both the breadth of\nthe problems and which topics have the most interest, as\nwell as to compare di\u000berent MLapproaches to the same\nproblem.\nA. Prediction of energies and other properties throughout\nchemical compound space\nChemical space is astronomically vast.27,28Given some\nmolecule, de\fned by its number of electrons and the set\nof nuclei at their equilibrium geometries, we can typically\npredict its observables with satisfying accuracy using ab\ninitio quantum chemical methods such as CCSD(T) in\na su\u000eciently large basis. This is feasible for smaller\nmolecules, and DFT can be used (less reliably) for larger\nones. But even DFT (or computationally less demanding\nsemi-empirical quantum chemistry methods) is not fast\nenough to search all of chemical compound space, whose\nsize grows combinatorially with the number of atoms\nand distinct elements. Thus, an important problem is\nto search chemical compound space to \fnd new drugs\n(and materials space to \fnd new materials) with desired\nfunctionalities.\nA basic property is the ground-state energy of a\nmolecule. But there are also many other interesting prop-\nerties at the ground-state con\fguration, such as dipole\nmoments, ionization potentials, and vibrational frequen-\ncies. Some of these can be extracted from the same elec-\ntronic structure calculation from which the molecule's en-\nergy was obtained, while others require additional com-\nputation. Given the impossibility of calculating all prop-\nerties of all possible molecules, it is interesting to ask\nif a ML algorithm, trained on known examples, can be\nused to predict the properties of new molecules at muchreduced computational cost.29If so, chemical compound\nspace can be searched orders of magnitude more quickly.\nMany groups are therefore formulating ways to do this.\nNote that often researchers use DFT (or even DFTB)\nresults for both training and testing their algorithms. In\nthose cases, the ML algorithm is tested against the DFT\ncalculations, not experiments or more accurate quantum\nchemical methods. The idea is that, once an algorithm is\nsu\u000eciently robust and useful, it can then be trained on\nmore accurate data and, presumably, work just as well.\nThese days, many ML approaches already produce MAE\nbelow those typical of density functionals.\nYang et al.30introduce a size-independent NNmodel\nof heats of formation trained on small organic molecules\nthat can be applied to large molecules. For these,\nthe MAE from reference B3LYP numbers is reduced to\n1.7 kcal/mol.\nOn the other hand, Eickenberg et al.31introduce a ML\nmodel based on a solid harmonic wavelet scattering rep-\nresentation of organic molecules and demonstrate com-\npetitive performance for predicted atomization energies.\nMeanwhile, Hy et al.76use a new kind of NN, called\na covariant compositional network, to deduce properties\nfrom molecular graphs alone, yielding promising results\non databases of small molecules.\nOften, the e\u000eciency of a ML algorithm depends cru-\ncially on the way the data are represented. Faber\net al.32introduce a many-body representation of atoms\nin their environment and report \\chemical accuracy\"\n(1 kcal/mol) for energies of organic molecules and solids\nwith few thousand training points. Interpolation across\nthe periodic table even enables prediction of energies of\nmolecules with elements that were not included in the\ntraining set.\nLubbers et al.33introduce a hierarchical NN approach\nwith competitive performance for predicting atomiza-\ntion energies of organic molecules, as well as energies\nand forces of thousands of snapshots of benzene, mal-\nonaldehyde, salicyclic acid, and toluene. Their method\ncan also be applied to MD simulations and gives a\nmeasure of model uncertainty automatically. Gastegger\net al.34develop element-speci\fc weighting functions for\natom-centered symmetry function-based representations\nin NNs. Upon use of the weighting functions, they show\nthat less symmetry functions are necessary and the pre-\ndiction error of atomization energies in organic molecules\nis systematically reduced.\nGubaev et al.35conceive a local tensor based ML\napproach which depends on the property being inten-\nsive or extensive, and they combine it with active\nlearning in order to achieve state-of-the-art perfor-\nmance for atomization energies, polarizabilities, and\nHOMO/LUMO eigenvalues in organic molecules. Collins\net al.36show that graph-based molecular representations\npro\ft from inclusion of interatomic distance information\nwhile remaining size-independent, as evinced for compet-\nitive prediction errors of atomization energies in organic\nmolecules.\n\n--- 第6页 ---\n6\nB. Interatomic potentials\nClassical MD simulations with interatomic potentials\ncan handle a million atoms or more and are used to study\ndynamic processes in biology and chemistry. Unfortu-\nnately, the necessary computational e\u000eciency is some-\ntimes obtained only at the expense of predictive power.\nTypically, relying on complex classical force \felds, which\nignore the underlying electronic structure and dynam-\nics, can produce inconsistent answers to important ques-\ntions. This limitation becomes especially acute when co-\nvalent bonds are formed or broken, when atoms vary their\nhybridization state, or during considerable changes in\nchemical environments, as in, for example, molten alloys.\nThen developing and testing force \felds for all possible\ncon\fgurations become an unsurmountable task. Given\nthis challenge, and the relevance of a dynamic descrip-\ntion of atomistic processes throughout the exact sciences,\na large number of articles in the Special Topic are devoted\nto the question if and how interatomic potentials can be\nconstructed via ML, for example by training on (usually)\nDFT calculations.\nBereau et al.37predict parameters for intermolecular\nforce \felds throughout chemical space. These parame-\nters include atomic charges, dipole moments, quadrupole\nmoments, polarizabilities, atomic electron density screen-\ning factors, and normalization constants. Out-of-sample\npredictions on well established van der Waals benchmark\ndatasets indicate errors below or about 1 kcal/mol.\nA crucial consideration for ML methods is the way in\nwhich the inputs are represented, which can have a strong\nimpact on performance. Imbalzano et al.38provide an au-\ntomated protocol for feature selection , showing how\nthis can simplify construction of ML potentials. They il-\nlustrate their procedure on NNpotentials for water and\naluminum ternary alloys, as well as a GPR potential for\nformation energies of molecules.\nGaussian approximation potentials (GAPs) are one of\nthe success stories of ML in chemistry. They provide\nan automated approach to constructing accurate inter-\natomic potentials that recreate the underlying electronic\nstructure energetics at a fraction of the computational\ncost. Fujikake et al.39study the issue of guest atoms in\nhost structures, with the speci\fc case of Li in C, show-\ning how to add the Li interactions to a pre-existing GAP\npotential for C.\nAn important question, usually left to human bias and\nintuition, is the selection of data upon which to train:\nWhen generating an interatomic potential, which sets of\nelectronic structure calculations do you perform to create\nthe database to train on and test against? Smith et al.40\npresent a fully automatic way of generating datasets for\nthe speci\fc purpose of training ML potentials. Query-\nby-committee active learning uses disagreements be-\ntween predictions of di\u000berent models to improve sampling\nand reduce the amount of data needed over random sam-\npling. Results are given on a new COMP6 database of\nsmall organic molecules containing CHNO.Unke and Meuwly41are focused on creating methods\nthat span both con\fgurational space and chemical space.\nTheir method decomposes energy into local atomic con-\ntributions, with prediction errors on atomization energies\non the order of half a kcal/mol after training on 35 000 or-\nganic molecules. They demonstrate predictive capability\non both reactive and non-reactive MD simulations.\nAdvanced deep learning methods are applied by Sch utt\net al.42They present SchNet, a DNN that learns chem-\nically relevant information about atom types across the\nperiodic table. It is general and \rexible and uses deep\nlearning to avoid the need for clever choices of descrip-\ntors. It can be applied to both molecules and materials,\nand has been shown to reduce the computational cost\nof DFT-MD simulations of fullerenes by 3{4 orders of\nmagnitude.\nAnother type of ML method is GPR orKriging , and\nDi Pasquale et al.43use it to predict energies of ions sol-\nvated in water. Energies are based on atomic energies\nobtained from the topological partitioning called inter-\nacting quantum atoms. This method provides accurate\nresults and is part of an advanced force \feld development,\nFFLUX.\nSpectral Neighbor Analysis Potentials (SNAPs) ex-\npress the energy of an atom linearly in terms of bis-\npectrum components of neighboring atoms. Wood and\nThompson44show that accuracy can be improved by in-\ncluding quadratic contributions at a modest increase in\ncost, making it particularly suitable for large-scale MD\nsimulations of materials.\nMetallic nanoclusters are important in many areas\nof chemistry, but realistic simulations are limited by\nthe computational cost of DFT-MD. Zeni et al.45study\nsuch systems via classical n-body potentials derived from\nML (\\M-FFs\") by constructing n-body kernels that can\nbe exactly mapped to non-parametric classical potential\nforms such as 3D splines. This circumvents summing\nover training set entries for predictions, accelerating sim-\nulations by orders of magnitude. They \fnd that 2-body\npotentials are insu\u000eciently accurate to capture the be-\nhavior of Ni clusters, but 3-body potentials are. Choice\nof training data also plays an essential role.\nAnother important question is which regions of con\fg-\nuration space to sample when constructing a ML force\n\feld. Herr et al.46explore application of metadynamics\nto training sets prior to selection for training. Metady-\nnamics avoids the problem of being stuck in the vicinity\nof local minima. In comparison to data retrieved from\nMD or normal-mode analysis based sampling, the result-\ning NN exhibits improved or more e\u000ecient performance.\nFinally, QM/MM schemes are popular in computa-\ntional molecular biology, but often su\u000ber from limitations\nof the MM model and ambiguities at the interface. Zhang\net al.47review this \feld for the speci\fc case of a ML force\n\feld for the MM contribution. They point out both ad-\nvantages and disadvantages of the ML approach.\n\n--- 第7页 ---\n7\nC. Potential energy surfaces of speci\fc molecules\nThis section could arguably be part of the previous\none. But in this section, the molecule is \fxed, and a\nhighly accurate potential energy surface is desired, for a\n\fxed number of atoms.\nA di\u000ecult problem is the simulation of water on ox-\nide surfaces, as measured by infrared spectroscopy of OH\nanharmonic stretches. MD simulations at the DFT level\nshould be su\u000eciently accurate, but are too expensive\ncomputationally. Quaranta et al.48use a NN potential\ntrained on such calculations to perform MD and solve the\nnuclear Schr odinger equation for a large number of con-\n\fgurations to determine vibrational spectra. They \fnd\nthat many di\u000berent species contribute in overlapping re-\ngions of the spectrum and that the stretching frequencies\ndepend strongly on the hydrogen bonding.\nFor many purposes DFT-level calculations su\u000ece, but\nnot for the infrared spectrum of weakly bound dimers.\nThe potential energy surface is a function of all 45 in-\nternuclear distances and must be calculated at CCSD(T)\nlevels of accuracy in order to accurately re\rect the anhar-\nmonic couplings. Qu and Bowman49present a novel \ft\nto the dipole moment and solve the nuclear Schr odinger\nequation using various levels of anharmonic theory to\ngenerate the infrared spectrum.\nNguyen et al.50perform a careful study of the general\nmethodology for constructing interatomic potentials, fo-\ncusing on two- and three-body interactions in water us-\ning coupled-cluster energies. They compare di\u000berent ap-\nproaches: GAP ,NNs , and permutation-invariant poly-\nnomials, \fnding comparable levels of accuracy in the \ft.\nIn a related way, Kamath et al.51study the potential\nenergy surface of formaldehyde, in order to compare NNs\nwith GPR, using exactly the same data. In each case,\nthey calculate vibrational spectra. They \fnd GPR to\nperform better for a \fxed number of data points, with\na relatively accurate spectrum from as few as 300 data\npoints.\nD. Stability of solids\nAnother important \feld is the relative stability of dif-\nferent arrangements of atoms in solids, be they metallic\nalloys or molecular crystals. Searching all possible ar-\nrangements is again a Herculean task, which could be\ntremendously accelerated if the patterns of the output\ncould be machine-learned instead of having to be recal-\nculated over and over.\nArtrith et al.52address the problem of creating atomic\npotentials for alloys. There are a few cases where good\npotentials have been intuited in the past, but the es-\nsentially in\fnite number of possibilities and simulation\nconditions leads to a strong need for automation. Essen-\ntially, direct simulation with \frst-principles methods is\nhopelessly expensive for many problems and properties\nof interest. They use NNs to speed up the sampling for\namorphous and disordered materials, and use the subse-\nquent potential to calculate the phase diagram.\nOn the other hand, Schmidt et al.53scan many materi-\nals, looking speci\fcally at ternary compounds to \fnd themost stable structures. Here they \fnd that ML reduces\nthe calculational cost by about a factor of 4, but the high\naccuracy needed for such predictions limits the bene\fts\nof the ML approach to this problem.\nAn important problem is that of \fnding stable poly-\nmorphs of molecular crystals. Li et al.54introduce Genar-\nris, a Python package that does inexpensive approximate\nDFT calculations and analyzes results with a relative co-\nordinate descriptor developed speci\fcally for this task. It\nuses ML for clustering and can be targeted for various\noutcomes, ranging from random structure generation to\n\fnding a maximally diverse set of structures to seed a\ngenetic algorithm.\nA quite di\u000berent problem is that of grain boundaries\nin materials, where all sorts of non-stoichiometric de-\nfects appear. Kiyohara and Mizoguchi55use a Monte\nCarlo tree search to model grain-boundary segregation\nand test it on silver impurities in copper. They \fnd that\nthe search algorithm reduces the number of evaluations\nby a factor of 100 and yields insight into the nature of\nthe most relevant sites.\nReturning to searching chemical compound space, Seko\net al.56look at all possible inorganic crystals, which is a\nmuch vaster space than those that have been discovered\nso far. They propose descriptors to estimate the rele-\nvance of chemical composition to stability. They train\nand test on experimental databases and also estimate\nphase stability from \frst-principles calculations.\nGraphene is a promising material for future electronic\napplications. Dieb et al.57consider doping graphene with\nboron atoms. High levels of doping have been recently\nmade and measured. Their aim is to \fnd the most stable\nstructures, using \frst principles calculations and ML to\nperform the search. They \fnd useful patterns and predict\nproperties as a function of boron doping.\nE. Finding new density functionals\nDensity functional theory (DFT) calculations are cur-\nrently of limited accuracy and reliability, and often fail\nbadly for materials that are of key technological interest.\nSeveral of the papers in this Special Topic address the\nidea of using ML to improve existing functionals or to\ncreate entirely new ones.\nMardirossian and Head-Gordon58develop ML technol-\nogy to optimize exchange-correlation functionals at dif-\nferent levels on Jacob's ladder59of increasing sophistica-\ntion. Their work is at the highest rung, in which a double-\nhybrid functional is optimized (but not over\ftted) to a\ndataset of nearly 5 000 molecular energies, screening tril-\nlions of possible functionals, but ending up with only 14\nparameters. This might prove an invaluable combination\nof accuracy and computational e\u000eciency.\nAnother place where ML methods can be fruitfully ap-\nplied is to \fnd the exact (or at least a much more accu-\nrate) exchange-correlation functional, without \ftting a\ngiven form of approximation. Nagai et al.60take small\nmodel problems, in which the exact density and energy\nare known, and use inversion techniques to \fnd the ex-\nact Hartree-exchange-correlation energy and potentials.\n\n--- 第8页 ---\n8\nIn the framework of Levy and Zahariev,61they then train\nand test a NN for this object. This work can be classi-\n\fed as going beyond the existing approximations used\ncurrently in DFT.\nOn the other hand, Ji and Jung62use a grid-based local\nrepresentation of various electronic properties to predict\nDFT energies, densities, and exchange-correlation poten-\ntials for 16 small main-group molecules, with errors be-\nlow 1 kcal/mol when trained for each molecule separately.\nThe errors rise only to 4 kcal/mol if a small subset of the\nmolecules is used for training, holding out the promise of\na transferable method sensitive to chemical environment.\nThe work of Hollingsworth et al.63is focused on\nwhether or not simple exact conditions, which have been\nhighly useful in guiding human-based functional design,\nare useful for improving learning curves of ML functional\napproximations. While they examine the question for the\nKohn-Sham kinetic energy of simple models, their results\nshould provide a guide for applications to the exchange-\ncorrelation energy, such as in the work of Nagai et al.60\nThey \fnd that, while exact conditions do improve learn-\ning rates, the improvement is only signi\fcant when there\nis similarity in the densities within the training manifold.\nSeino et al.64work with approximate forms for the en-\nergy density of the Kohn-Sham kinetic energy to improve\nover existing approximations to orbital-free DFT. They\nexpand in higher gradients than are typically included in\nhuman approximations, and use ML to \fnd coe\u000ecients\nand density dependencies, and compare their accuracies\nto many existing orbital-free functionals.\nF. Analyzing molecular dynamics simulations\nEven with classical force \felds, there is tremendous in-\nterest in speeding up speci\fc aspects of MD simulations,\nsuch as rare-event sampling or slow, long-term motions\nof long molecules. A related interest is the extraction of\ninformation from the large amounts of data generated by\nMD simulations.\nThe work of Boninsegna et al.65is focused on \fnding\ncollective variables to determine long-time and coarse-\ngrained motions from MD data. There is substantial\nhistory of ad hoc intuitive approaches to these prob-\nlems, but their Sparse Identi\fcation of Nonlinear Dynam-\nics (SINDy) approach does this automatically, and they\nprove the correctness of their approach in the limit of\nin\fnite data. A similar problem is tackled by Wehmeyer\nand No\u0013 e66using a DNN autoencoder , which \fnds low-\ndimensional features (that is, the slow dynamics of the\nunderlying stochastic processes) embedded in a higher-\ndimensional feature space. They test their methodology\non simple model systems and a 125 \u0016s trajectory of the\nfast-folding peptide villin.\nFinally, Matsunaga and Sugita67approach this topic\nfrom a di\u000berent viewpoint. They construct a Markov\nstate model from MD trajectories and then re\fne that\nmodel using ML methods applied to experimental data.\nThus their methodology attempts to overcome the inher-\nent limitations of the MD force \feld model by comparison\nwith experiment, whereas the other contributions are fo-cused on speeding up a calculation, but entirely within\nthe MD simulation itself.\nG. Everything else\nNot everything \fts into simple categories and that is\nespecially true in this \feld, including attempts to improve\ngeometry optimization, to analyze the statistics behind\nbenchmark datasets, and applications to larger biopoly-\nmers. In fact, there are many, many more possible ap-\nplications of data-enabled chemistry, many of which are\nnot included in this Special Topic and so are beyond the\nscope of this editorial.\nPernot and Savin68perform an in-depth study of the\nmethods currently being used to benchmark approxi-\nmations against datasets, an important topic as ever\nlarger datasets are being generated. They question the\nsummary statistics typically reported, such as RMSE or\nMAE, showing that because the error distributions are\nnot simple, little can be inferred about error probabilities\nfrom these numbers alone. They advocate more informa-\ntive measures and show their usefulness.\nThe position of the LUMO and the width of the op-\ntical gap in polymers for solar cells are important for\npower conversion e\u000eciency. J\u001crgensen et al.69perform\n\frst-principles calculations on about 4 000 monomers and\nshow that a grammar variational autoencoder using a\nsimple string representation makes quite accurate predic-\ntions, reducing the cost of a search by up to a factor of 5.\nAfzal et al.70model the refraction index of organic poly-\nmers by combining \frst-principles calculations with ML\nto predict packing fractions of the bulk polymers.\nAgain, along the lines of solving a material- and\nproperty-speci\fc problem, Pilania et al.71study the e\u000bect\nof lanthanide dopants in inorganic scintillation counter\nmaterials. They use ML on some key experimentally-\nmeasured parameters and combine the results with high-\nthroughput electronic structure calculations to perform\nscreening for materials that exhibit optimized levels of\nthe dopant relative to the gap of the host material.\nAnother important problem is that of geometry opti-\nmization, sometimes at a high level of theory. Schmitz\nand Christiansen72use GPR to optimize geometries using\nnumerical gradients. They use lower levels of electronic\nstructure calculations, such as Hartree-Fock or MP2, and\nthen calculate di\u000berences to higher level theory. The\ninterpolation introduces errors of no more than micro-\nHartrees.\nIn a similar vein, S\u001crensen et al.73also perform ge-\nometry optimization but on materials at an approximate\nDFT level. They \fnd that unsupervised learning can\nbe used to categorize atoms in many diverse partially or-\ndered surface structures of anatase titanium oxide. They\nalso perform gradient-based minimization of a summed\ncluster distance resulting from this analysis which allows\nescape from meta-stable basins and so helps \fnd global\nminima more quickly.\nOn the other hand, in a totally di\u000berent system and\nregime, Botlani et al.74use MD to simulate dynamic\nallostery, in which regulator-induced changes in protein\n\n--- 第9页 ---\n9\nstructure are comparable to thermal changes. Thus the\ndata must be mined to \fnd patterns in a very high di-\nmensional space to identify mechanisms. Unsupervised\nclustering shows that regulator binding strongly alters\nthe protein's signalling network, not by changing connec-\ntions between amino acids as one might naively imagine,\nbut rather by changing the connectivity between clusters.\nAntimicrobial peptides interact with simple phospho-\nlipid membranes, which is relevant for rational drug de-\nsign. Cipcigan et al.75introduce new tools for analyzing\nthek-mer spectrum encoded in antimicrobial databases\nand ways to visualize membrane binding and permeation\nof helical peptides.\nIV. SUMMARY\nWe hope you have found this editorial a useful guide to\nthe important content, the papers in our Special Topic.\nWe end with some remarks about the nature of the \feld.\nML has been scoring some impressive successes in vari-\nous areas of human activity. There is tremendous hope\nfor similar successes in applications to physical sciences.\nHowever, progress in this direction requires discovering\nmore subtle rules than in many other arenas. So it takes\ntime for researchers to \fnd the best ways to apply ML\nto their problems. But practical chemists and materi-\nals scientists can now create a dazzling array of di\u000ber-\nent molecular structures and alloys. Once the progress\nreported here moves beyond development and proof-of-\nprinciple, perhaps we can look forward to new materials\nand drugs designed with ML methods that build on hu-\nman intuition but apply it to more possibilities than a\nhuman could ever imagine. We shall see.\nAcknowledgments\nK.B. acknowledges NSF 1464795. M.R. acknowledges\nfunding from the EU Horizon 2020 program Grant\nNo. 676580, The Novel Materials Discovery (NOMAD)\nLaboratory, a European Center of Excellence. O.A.v.L.\nacknowledges funding from the Swiss National Science\nFoundation (Nos. PP00P2 138932 and 310030 160067).\nThis work was partly supported by the NCCR MAR-\nVEL, funded by the Swiss National Science Foundation.\nThe guest editors sincerely thank the sta\u000b and editors of\nJ. Chem. Phys. for putting this Special Topic together\nand all the authors for their input into this editorial.\n1Zoubin Ghahramani. Probabilistic machine learning and arti\fcial\nintelligence. Nature , 521(7553):452{459, 2015.\n2Michael I. Jordan and Tom M. Mitchell. Machine learning:\nTrends, perspectives, and prospects. Science , 349(6245):255{260,\n2015.\n3Tom M. Mitchell. Machine Learning . McGraw Hill, New York,\n1997.\n4Matthias Rupp. Machine learning for quantum mechanics\nin a nutshell. International Journal of Quantum Chemistry ,\n115(16):1058{1073, 2015.\n5Stuart Russell and Peter Norvig. Arti\fcial Intelligence: A Mod-\nern Approach . Prentice Hall, third edition, 2009.\n6Johann Gasteiger and Thomas Engel, editors. Chemoinformat-\nics. Wiley-VCH, Weinheim, 2003.7Cynthia Selassie and Rajeshwar P. Verma. History of quantita-\ntive structure-activity relationships. In Donald J. Abraham and\nDavid P. Rotella, editors, Burger's Medicinal Chemistry, Drug\nDiscovery and Development , volume 1. Wiley, 7th edition, 2010.\n8Gisbert Schneider. Virtual screening: an endless staircase? Na-\nture Reviews Drug Discovery , 9(7):273{276, 2010.\n9Rampi Ramprasad, Rohit Batra, Ghanshyam Pilania, Arun\nMannodi-Kanakkithodi, and Chiho Kim. Machine learning and\nmaterials informatics: Recent applications and prospects. Nature\nPartner Journals Computational Materials , 3:54, 2017.\n10G okhan Bakir, Thomas Hofmann, Bernhard Sch olkopf, Alexan-\nder Smola, Ben Taskar, and Vishy Vishwanathan, editors. Pre-\ndicting Structured Data . MIT Press, Cambridge, 2007.\n11John Lee and Michel Verleysen. Nonlinear Dimensionality Re-\nduction . Springer, New York, 2007.\n12Burr Settles. Active Learning , volume 18 of Synthesis Lectures on\nArti\fcial Intelligence and Machine Learning . Morgan & Clay-\npool, 2012.\n13Bernhard Sch olkopf and Alexander Smola. Learning with Ker-\nnels. MIT Press, Cambridge, 2002.\n14Thomas Hofmann, Bernhard Sch olkopf, and Alexander Smola.\nKernel methods in machine learning. Annals of Statistics ,\n36(3):1171{1220, 2008.\n15Bernhard Boser, Isabelle Guyon, and Vladimir Vapnik. A train-\ning algorithm for optimal margin classi\fers. In Proceedings of the\n5th Annual ACM Conference on Computational Learning The-\nory (COLT 1992), Pittsburgh, Pennsylvania, USA, July 27{29 ,\npages 144{152, 1992.\n16Bernhard Sch olkopf, Alexander Smola, and Klaus-Robert M uller.\nNonlinear component analysis as a kernel eigenvalue problem.\nNeural Computation , 10(5):1299{1319, 1998.\n17Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The\nElements of Statistical Learning. Data Mining, Inference, and\nPrediction . Springer, New York, 2 edition, 2009.\n18Carl Rasmussen and Christopher Williams. Gaussian Processes\nfor Machine Learning . MIT Press, Cambridge, 2006.\n19Christopher Bishop. Neural Networks for Pattern Recognition .\nOxford University Press, Oxford, 1996.\n20Gr\u0013 egoire Montavon, Genevi\u0013 eve B. Orr, and Klaus-Robert M uller,\neditors. Neural Networks: Tricks of the Trade , volume 7700 of\nLecture Notes in Computer Science . Springer, Berlin, Germany,\nsecond edition, 2012.\n21Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep\nLearning . MIT Press, 2016.\n22Karl Pearson. On lines and planes of closest \ft to systems of\npoints in space. Philosophical Magazine , 2(11):559{572, 1901.\n23Ian Jolli\u000be. Principal Component Analysis . Springer, New York,\nsecond edition, 2004.\n24Corinna Cortes, Lawrence D. Jackel, Sara A. Solla, Vladimir\nVapnik, and John S. Denker. Learning curves: Asymptotic val-\nues and rate of convergence. In Jack D. Cowan, Gerald Tesauro,\nand Joshua Alspector, editors, Advances in Neural Information\nProcessing Systems 6 (NIPS 1993), Denver, Colorado, USA,\nNovember 29{December 2 . Morgan Kaufmann, 1993.\n25Klaus-Robert M uller, Michael Finke, Noboru Murata, Klaus\nSchulten, and Shun-ichi Amari. A numerical study on learn-\ning curves in stochastic multilayer feedforward networks. Neural\nComputation , 8(5):1085{1106, 1996.\n26Vladimir Vapnik. The Nature of Statistical Learning Theory .\nSpringer, second edition, 2001.\n27Asher Mullard. The drug-maker's guide to the galaxy. Nature ,\n549(7673):445{447, 2017.\n28Peter Kirkpatrick and Clare Ellis. Chemical space. Nature ,\n432:823, 2004.\n29O. Anatole von Lilienfeld. Quantum machine learning in chemical\ncompound space. Angewandte Chemie Intl. Ed. , 57(16):4164{\n4169, 2018.\n30GuanYa Yang, Jiang Wu, Shuguang Chen, WeiJun Zhou, Jian\nSun, and GuanHua Chen. Size-independent neural networks\nbased \frst-principles method for accurate prediction of heat of\n\n--- 第10页 ---\n10\nformation of fuels. Journal of Chemical Physics , 148(24):241738,\n2018.\n31Michael Eickenberg, Georgios Exarchakis, Matthew Hirn,\nSt\u0013 ephane Mallat, and Louis Thiry. Solid harmonic wavelet scat-\ntering for predictions of molecule properties. Journal of Chemical\nPhysics , 148(24):241732, 2018.\n32Felix A. Faber, Anders S. Christensen, Bing Huang, and O. Ana-\ntole von Lilienfeld. Alchemical and structural distribution based\nrepresentation for universal quantum machine learning. Journal\nof Chemical Physics , 148(24):241717, 2018.\n33Nicholas Lubbers, Justin S. Smith, and Kipton Barros. Hierarchi-\ncal modeling of molecular energies using a deep neural network.\nJournal of Chemical Physics , 148(24):241715, 2018.\n34Michael Gastegger, Ludwig Schwiedrzik, Marius Bittermann,\nFlorian Berzsenyi, and Philipp Marquetand. WACSF|\nweighted atom-centered symmetry functions as descriptors in\nmachine learning potentials. Journal of Chemical Physics ,\n148(24):241709, 2018.\n35Konstantin Gubaev, Evgeny V. Podryabinkin, and Alexander V.\nShapeev. Machine learning of molecular properties: Locality and\nactive learning. Journal of Chemical Physics , 148(24):241727,\n2018.\n36Christopher R. Collins, Geo\u000brey J. Gordon, O. Anatole von\nLilienfeld, and David J. Yaron. Constant size descriptors for ac-\ncurate machine learning models of molecular properties. Journal\nof Chemical Physics , 148(24):241718, 2018.\n37Tristan Bereau, Robert A. DiStasio, Jr., Alexandre Tkatchenko,\nand O. Anatole von Lilienfeld. Non-covalent interactions across\norganic and biological subsets of chemical space: Physics-based\npotentials parametrized from machine learning. Journal of\nChemical Physics , 148(24):241706, 2018.\n38Giulio Imbalzano, Andrea Anelli, Daniele Giofr\u0013 e, Sinja Klees,\nJ org Behler, and Michele Ceriotti. Automatic selection of atomic\n\fngerprints and reference con\fgurations for machine-learning po-\ntentials. Journal of Chemical Physics , 148(24):241730, 2018.\n39So Fujikake, Volker L. Deringer, Tae Hoon Lee, Marcin Krynski,\nStephen R. Elliott, and G\u0013 abor Cs\u0013 anyi. Gaussian approximation\npotential modeling of lithium intercalation in carbon nanostruc-\ntures. Journal of Chemical Physics , 148(24):241714, 2018.\n40Justin S. Smith, Ben Nebgen, Nicholas Lubbers, Olexandr\nIsayev, and Adrian E. Roitberg. Less is more: sampling chem-\nical space with active learning. Journal of Chemical Physics ,\n148(24):241733, 2018.\n41Oliver T. Unke and Markus Meuwly. A reactive, scalable, and\ntransferable model for molecular energies from a neural net-\nwork approach based on local information. Journal of Chemical\nPhysics , 148(24):241708, 2018.\n42Kristof T. Sch utt, Huziel E. Sauceda, Pieter-Jan Kindermans,\nAlexandre Tkatchenko, and Klaus-Robert M uller. SchNet|a\ndeep learning architecture for molecules and materials. Journal\nof Chemical Physics , 148(24):241722, 2018.\n43Nicodemo Di Pasquale, Stuart J. Davie, and Paul L. A. Popelier.\nThe accuracy of ab initio calculations without ab initio calcula-\ntions for charged systems: Kriging predictions of atomistic prop-\nerties for ions in aqueous solutions. Journal of Chemical Physics ,\n148(24):241724, 2018.\n44Mitchell A. Wood and Aidan P. Thompson. Extending the accu-\nracy of the SNAP interatomic potential form. Journal of Chem-\nical Physics , 148(24):241721, 2018.\n45Claudio Zeni, Kevin Rossi, Aldo Glielmo, Nicola Gaston,\nFrancesca Baletto, and Alessandro De Vita. Building ma-\nchine learning force \felds for nanoclusters. Journal of Chemical\nPhysics , 148(24):241739, 2018.\n46John E. Herr, Kun Yao, Ryker McIntyre, David Toth, and\nJohn Parkhill. Metadynamics for training neural network model\nchemistries: a competitive assessment. Journal of Chemical\nPhysics , 148(24):241710, 2018.\n47Yin-Jia Zhang, Alireza Khorshidi, Georg Kastlunger, and An-\ndrew A. Peterson. The potential for machine learning in\nhybrid QM/MM calculations. Journal of Chemical Physics ,148(24):241740, 2018.\n48Vanessa Quaranta, Matti Hellstr om, J org Behler, Jolla Kullgren,\nPavlin D. Mitev, and Kersti Hermansson. Maximally resolved\nanharmonic OH vibrational spectrum of the water/ZnO(10 \u001610)\ninterface from a high-dimensional neural network potential. Jour-\nnal of Chemical Physics , 148(24):241720, 2018.\n49Chen Qu and Joel M. Bowman. High-dimensional \ftting of\nsparse datasets of CCSD(T) electronic energies and MP2 dipole\nmoments, illustrated for the formic acid dimer and its complex IR\nspectrum. Journal of Chemical Physics , 148(24):241713, 2018.\n50Thuong T. Nguyen, Eszter Sz\u0013 ekely, Giulio Imbalzano, J org\nBehler, G\u0013 abor Cs\u0013 anyi, Michele Ceriotti, Andreas W. G otz, and\nFrancesco Paesani. Comparison of permutationally invariant\npolynomials, neural networks, and Gaussian approximation po-\ntentials in representing water interactions through many-body\nexpansions. Journal of Chemical Physics , 148(24):241725, 2018.\n51Aditya Kamath, Rodrigo A. Vargas-Hern\u0013 andez, Roman V.\nKrems, Tucker Carrington, Jr., and Sergei Manzhos. Neural\nnetworks vs Gaussian process regression for representing poten-\ntial energy surfaces: A comparative study of \ft quality and\nvibrational spectrum accuracy. Journal of Chemical Physics ,\n148(24):241702, 2018.\n52Nongnuch Artrith, Alexander Urban, and Gerbrand Ceder. Con-\nstructing \frst-principles phase diagrams of amorphous Li xSi us-\ning machine-learning-assisted sampling with an evolutionary al-\ngorithm. Journal of Chemical Physics , 148(24):241711, 2018.\n53Jonathan Schmidt, Liming Chen, Silvana Botti, and Miguel A. L.\nMarques. Predicting the stability of ternary intermetallics with\ndensity functional theory and machine learning. Journal of\nChemical Physics , 148(24):241728, 2018.\n54Xiayue Li, Farren S. Curtis, Timothy Rose, Christoph Schober,\nAlvaro Vazquez-Mayagoitia, Karsten Reuter, Harald Oberhofer,\nand Noa Marom. Genarris: Random generation of molecular\ncrystal structures and fast screening with a Harris approximation.\nJournal of Chemical Physics , 148(24):241701, 2018.\n55Shin Kiyohara and Teruyasu Mizoguchi. Searching the segrega-\ntion con\fguration at the grain boundary by a Monte Carlo tree\nsearch. Journal of Chemical Physics , 148(24):241741, 2018.\n56Atsuto Seko, Hiroyuki Hayashi, and Isao Tanaka. Compositional\ndescriptor-based recommender system for the materials discov-\nery. Journal of Chemical Physics , 148(24):241719, 2018.\n57Thaer M. Dieb, Zhufeng Hou, and Koji Tsuda. Structure pre-\ndiction of boron-doped graphene by machine learning. Journal\nof Chemical Physics , 148(24):241716, 2018.\n58Narbe Mardirossian and Martin Head-Gordon. Survival of the\nmost transferable at the top of Jacob's ladder: De\fning and\ntesting the !B97M(2) double hybrid density functional. Journal\nof Chemical Physics , 148(24):241736, 2018.\n59John P. Perdew, Adrienn Ruzsinszky, Jianmin Tao, Viktor N.\nStaroverov, Gustavo E. Scuseria, and G\u0013 abor I. Csonka. Prescrip-\ntion for the design and selection of density functional approxima-\ntions: More constraint satisfaction with fewer \fts. The Journal\nof Chemical Physics , 123(6):062201, 2005.\n60Ryo Nagai, Ryosuke Akashi, Shu Sasaki, and Shinji Tsuneyuki.\nNeural-network Kohn-Sham exchange-correlation potential and\nits out-of-training transferability. Journal of Chemical Physics ,\n148(24):241737, 2018.\n61Mel Levy and Federico Zahariev. Ground-state energy as a simple\nsum of orbital energies in Kohn-Sham theory: A shift in perspec-\ntive through a shift in potential. Phys. Rev. Lett. , 113:113002,\n2014.\n62Hyunjun Ji and Yousung Jung. A local environment descrip-\ntor for machine-learned electronic structure theory. Journal of\nChemical Physics , 148(24):241742, 2018.\n63Jacob Hollingsworth, Li Li, Thomas Baker, and Kieron Burke.\nCan exact conditions improve machine-learned density function-\nals? Journal of Chemical Physics , 148(24):241743, 2018.\n64Junji Seino, Ryo Kageyama, Mikito Fujinami, Yasuhiro Ikabata,\nand Hiromi Nakai. Semi-local machine-learned kinetic energy\ndensity functional with third-order gradients of electron density.\n\n--- 第11页 ---\n11\nJournal of Chemical Physics , 148(24):241705, 2018.\n65Lorenzo Boninsegna, Feliks N uske, and Cecilia Clementi. Sparse\nlearning of stochastic dynamical equations. Journal of Chemical\nPhysics , 148(24):241723, 2018.\n66Christoph Wehmeyer and Frank No\u0013 e. Time-lagged autoencoders:\nDeep learning of slow collective variables for molecular kinetics.\nJournal of Chemical Physics , 148(24):241703, 2018.\n67Yasuhiro Matsunaga and Yuji Sugita. Re\fning Markov state\nmodels for conformational dynamics using ensemble-averaged\ndata and time-series trajectories. Journal of Chemical Physics ,\n148(24):241731, 2018.\n68Pascal Pernot and Andreas Savin. Probabilistic performance es-\ntimators for computational chemistry methods: The empirical\ncumulative distribution function of absolute errors. Journal of\nChemical Physics , 148(24):241707, 2018.\n69Peter B. J\u001crgensen, Murat Mesta, Suranjan Shil, Juan Maria\nGarc\u0013 \u0010a Lastra, Karsten W. Jacobsen, Kristian S. Thygesen, and\nMikkel Schmidt. Machine learning-based screening of complex\nmolecules for polymer solar cells. Journal of Chemical Physics ,\n148(24):241735, 2018.\n70Mohammad Atif Faiz Afzal, Chong Cheng, and Johannes\nHachmann. Combining \frst-principles and data modeling for the\naccurate prediction of the refractive index of organic polymers.\nJournal of Chemical Physics , 148(24):241712, 2018.71Ghanshyam Pilania, Kenneth J. McClellan, Christopher R.\nStanek, and Blas P. Uberuaga. Physics-informed machine learn-\ning for inorganic scintillator discovery. Journal of Chemical\nPhysics , 148(24):241729, 2018.\n72Gunnar Schmitz and Ove Christiansen. Gaussian process regres-\nsion to accelerate geometry optimizations relying on numerical\ndi\u000berentiation. Journal of Chemical Physics , 148(24):241704,\n2018.\n73Knud H. S\u001crensen, Mathias S. J\u001crgensen, Albert Bruix, and\nBj\u001crk Hammer. Accelerating atomic structure search with clus-\nter regularization. Journal of Chemical Physics , 148(24):241734,\n2018.\n74Mohsen Botlani, Ahnaf Siddiqui, and Sameer Varmaa. Machine\nlearning approaches to evaluate correlation patterns in allosteric\nsignaling: A case study of the PDZ2 domain. Journal of Chem-\nical Physics , 148(24):241726, 2018.\n75Flaviu Cipcigan, Anna Paola Carrieri, Edward O. Pyzer-Knapp,\nRitesh Krishna, Ya-Wen Hsiao, Martyn Winn, Maxim G. Ryad-\nnov, Colin Edge, Glenn Martyna, and Jason Crain. Accelerating\nmolecular discovery through data and physical sciences: Appli-\ncations to peptide-membrane interactions. Journal of Chemical\nPhysics , 148(24):241744, 2018.\n76Truong Son Hy, Shubhendu Trivedi, Horace Pan, Brandon M.\nAnderson, and Risi Kondor. Predicting molecular properties with\ncovariant compositional networks. Journal of Chemical Physics ,\n148(24):241745, 2018.",
      "download_time": "2025-12-14T21:50:29.843208",
      "word_count": 60210
    },
    {
      "index": 2,
      "title": "Topic Modelling Meets Deep Neural Networks: A Survey",
      "authors": [
        "He Zhao",
        "Dinh Phung",
        "Viet Huynh",
        "Yuan Jin",
        "Lan Du",
        "Wray Buntine"
      ],
      "published": "2021-02-28",
      "source": "arxiv",
      "url": "http://arxiv.org/abs/2103.00498v1",
      "pdf_url": "http://arxiv.org/pdf/2103.00498v1.pdf",
      "pdf_path": "papers\\02_Topic Modelling Meets Deep Neural Networks_ A Survey.pdf",
      "txt_path": "papers\\02_Topic Modelling Meets Deep Neural Networks_ A Survey.txt",
      "content": "--- 第1页 ---\narXiv:2103.00498v1  [cs.LG]  28 Feb 2021Topic Modelling Meets Deep Neural Networks: A Survey\nHe Zhao ,Dinh Phung ,Viet Huynh ,Yuan Jin ,Lan Du ,Wray Buntine\nDepartment of Data Science and Artiﬁcial Intelligence, Mon ash University, Australia\n{ethan.zhao, dinh.phung, viet.huynh, yuan.jin, lan.du, wr ay.buntine }@monash.edu\nAbstract\nTopic modelling has been a successful technique\nfor text analysis for almost twenty years. When\ntopic modelling met deep neural networks, there\nemerged a new and increasingly popular research\narea, neural topic models , with over a hundred\nmodels developed and a wide range of applica-\ntions in neural language understanding such as text\ngeneration, summarisation and language models.\nThere is a need to summarise research develop-\nments and discuss open problems and future direc-\ntions. In this paper, we provide a focused yet com-\nprehensive overview of neural topic models for in-\nterested researchers in the AI community, so as to\nfacilitate them to navigate and innovate in this fast-\ngrowing research area. To the best of our knowl-\nedge, ours is the ﬁrst review focusing on this spe-\nciﬁc topic.\n1 Introduction\nA powerful technique for text analysis, topic modelling has\nenjoyed success in various applications in machine learnin g,\nnatural language processing (NLP), and data mining for al-\nmost two decades. A topic model is applied to a collection of\ndocuments and aims to discover a set of latent topics, each of\nwhich describes an interpretable semantic concept. Bayesi an\nprobabilistic topic models (BPTMs) have been the most pop-\nular and successful series of models, with latent Dirichlet\nallocation (LDA) the best known representative. A BPTM\nusually speciﬁes a probabilistic generative model that gen er-\nates the data of a document with a structure of latent vari-\nables sampled from pre-speciﬁed distributions connected b y\nBayes’ theorem. Topics are captured by these latent variabl es.\nLike other Bayesian models, the learning of a BPTM is done\nby a (Bayesian) inference process (e.g. variational infere nce\n(VI) and Monte Carlo Markov Chain sampling).\nDespite their success, conventional BPTMs started to show\nsigns of fatigue in the era of big data and deep learning: 1)\nGiven a speciﬁc BPTM, its inference process usually needs\nto be customised accordingly and the inference complexity\nmay grow signiﬁcantly as the model complexity grows. Un-\nfortunately, it is also hard to automate the design of the inf er-\nence processes. 2)The inference processes for conventionalBPTMs can be hard to scale efﬁciently on large text collec-\ntions or to leverage parallel computing facilities like GPU s.\n3)It is usually inconvenient to integrate BPTMs with other\ndeep neural networks (DNNs) for joint training.\nWith the recent developments in DNNs and deep genera-\ntive models, there has been an emerging research direction\nwhich aims to leverage DNNs to boost performance, efﬁ-\nciency, and usability of topic modelling, named neural topic\nmodels (NTMs). With appealing ﬂexibility and scalability,\nNTMs have gained a huge research following, with more\nthan a hundred models and variants developed to date. More-\nover, NTMs have been used in important NLP tasks including\ntext generation, document summarisation, and translation , ar-\neas to which conventional topic models are harder to apply.\nTherefore, it is important to properly summarise research d e-\nvelopments, categorise existing approaches, identify rem ain-\ning issues, and discuss open problems and future directions .\nTo the best of our knowledge, a comprehensive review specif-\nically focusing on NTMs has not been published. In this pa-\nper, we would like to ﬁll this gap by providing an overview for\ninterested researchers who want to develop new NTMs and/or\nto apply NTMs in their domains. The notable contributions of\nour paper can be summarised as follows: 1)We propose a tax-\nonomy of NTMs where we categorise existing models based\non their backbone framework. 2)We ﬁrst provide an infor-\nmative discussion and overview of the background and eval-\nuation methods for NTMs and conduct a focused yet com-\nprehensive review, offering detailed comparisons of the va ri-\nants in different categories of NTMs with applications. 3)We\nidentify the limitations of existing methods and analyse th e\npossible future research directions for NTMs.\nThe rest of this paper is organised as follows. Section 2\nintroduces the background, deﬁnitions, and evaluations. S ec-\ntion 3 and 4 review NTMs with various backbone frame-\nworks. Section 5 discusses the applications. The current ch al-\nlenges and future directions are discussed in Section 6. Not e\nthat given the two page limit of references, we may only keep\nthe most relevant papers to NTMs.\n2 Background, Deﬁnition, and Evaluation\n2.1 Background and Deﬁnition\nThe most important idea of a topic model is the modelling of\nthe three key entities: document ,word , and topic .\n\n--- 第2页 ---\nNotations of Data A topic model works on a corpus (i.e.,\na collection of documents), where a document, by its nature,\ncan be represented as a sequence of words, which can be de-\nnoted by a vector of natural numbers, s∈NL, whereLis\nthe length of the document and sj∈ {1,···,V}is the in-\ndex in the vocabulary (with the size of V) of the token for\nthejth(j∈ {1,···,L})word. A more common represen-\ntation in topic modelling is the bag-of-words model, which\nrepresents a document by a vector of word counts, b∈ZV\n≥0,\nwherebvindicates the occurrences of the vocabulary token\nv∈ {1,···,V}in the document. One can readily obtain b\nfor a document from its word sequence vector s.\nNotations of Latent Variables A central concept is a topic ,\nwhich is usually interpreted as a cluster of words, describi ng\na speciﬁc semantic meaning. A topic is or can be normalised\ninto a distribution over the tokens in the vocabulary, named\nword distribution ,t∈∆V, where∆Vis aVdimensional\nsimplex and tvindicates the weight or relevance of token v\nunder this topic. Usually, a document’s semantic content is\nassumed to be captured or generated by one or more topics\nshared across the corpus. Therefore, a document is com-\nmonly associated with a distribution (or a vector that can be\nnormalised into a distribution) over K(K≥1) topics, named\ntopic distribution ,z∈∆K, wherezkindicates the weight of\nthekthtopic for this document. We further use D,Z, andTto\ndenote the corpus with all the document data, the collection s\nof topic distributions of all the documents, and the collect ions\nof word distributions of all the topics, respectively.\nNotations of Architectures and Learning With these no-\ntations, the task for a topic model is to learn the latent vari -\nables ofTandZfrom the observed data D. More formally,\na topic model learns a projection parameterised by θfrom\na document’s data to its topic distribution: z=θ(b)and a\nset of global variables for the word distributions of the top -\nics:T. In order to learn these parameters, one generates or\nreconstructs a document’s BoW data from its topic distribu-\ntion, which is modelled by another projection parameterise d\nbyφ:˜b=φ(z,T). Note that the majority of topic mod-\nels belong to the category of probabilistic generative mode ls,\nwherezandbare latent and observed random variables as-\nsumed to be generated from certain distributions respectiv ely.\nThe projection from the latent variables to the observed one s\nis named the generative process, which we further denote as:\n˜b∼pb\nφ(z,T)wherezis sampled from the prior distribution\nz∼pz. While the inverse projection is named the inference\nprocess, denoted as z∼qz\nθ(b), whereqzis the posterior dis-\ntribution of z. For NTMs, these probabilities are typically\nparameterised by deep neural networks.\n2.2 Evaluation\nIt is still challenging to comprehensively evaluate and com -\npare the performance of topic models including NTMs.\nBased on the nature and applications of topic models, the\ncommonly-used metrics are as follows.\nPredictive accuracy It has been common to measure the\nlog-likelihood of a model on held-out test documents, i.e.,\nthe predictive accuracy. A more popular metric based onlog-likelihood is perplexity, which captures how surprise d\na model is of new (test) data and is inversely proportional\nto average log-likelihood per word. Although log-likeliho od\nor perplexity gives a straight numerical comparison betwee n\nmodels, there remain issues: 1)As topic models are not for\npredicting unseen data but learning interpretable topics a nd\nrepresentations of seen data, predictive accuracy does not re-\nﬂect the main use of topic models. 2)Predictive accuracy\ndoes not capture topic quality. Predictive accuracy and hum an\njudgement on topic quality are often not correlated [7], and\neven sometimes slightly anti-correlated. 3)The estimation of\nthe predictive probability is usually intractable for Baye sian\nmodels and different papers may apply different sampling or\napproximation techniques. For NTMs, the computation of\nlog-likelihood is even more inconsistent, making it harder to\ncompare the results across different papers.\nTopic Coherence Experiments show topic coherence (TC)\ncomputed with the coherence between a topic’s most rep-\nresentative words (e.g, top 10 words) is inline with human\nevaluation of topic interpretability [32]. Various formulations\nhave been proposed to compute TC, we refer readers to [49]\nfor more details. Most formulations require to compute the\ngeneral coherence between two words, which are estimated\nbased on word co-occurrence counts in a reference corpus.\nRegarding TC: 1)The ranking of TC scores may vary un-\nder different formulations. Therefore, it is encouraged to re-\nport TC scores of different formulations or report the avera ge\nscore. 2)The choice of the reference corpus can also affect\nthe TC scores, due to the change of lexical usage, i.e, the\nshift of word distribution. For example, computing TC for\na machine learning paper collection with a tweet dataset as\nreference may generate inaccurate results. Popular choice s of\nthe reference corpus are the target corpus itself or an exter nal\ncorpus such as a large dump of Wikipedia. 3)To exclude less\ninterpretable “background” topics, one can select the topi cs\n(e.g., top 50%) with the highest TC and report the average\nscore over those selected topics [69]or to vary the proportion\nof the selected topics (e.g, from 10% to 100%) and plot TC\nscore at each proportion [70].\nTopic Diversity Topic diversity (TD), as its name implies,\nmeasures how diverse the discovered topics are. It is prefer -\nable that the topics discovered by a model describe differen t\nsemantic topical meanings. Speciﬁcally, [11]deﬁnes topic\ndiversity to be the percentage of unique words in the top 25\nwords.\nDownstream Application Performance The topic distri-\nbutionzof a document learned by a topic model can be used\nas the semantic representation of the document, which can\nbe used in document classiﬁcation, clustering, retrieval, vi-\nsualisation, and elsewhere. For document classiﬁcation, o ne\ncan train a classiﬁcation model with the topic distribution s\nlearned by a topic model as features and report the classiﬁca -\ntion performance to compare different topic models. Docu-\nment clustering can be conducted by two strategies: 1)Simi-\nlar to classiﬁcation, one can perform a clustering model (e. g.\nK-means with different numbers of clusters) on the topic dis -\ntributions, such as in [70];2)Alternatively, topics can be\nviewed as “soft” clusters of documents. Thus, one can use the\n\n--- 第3页 ---\nmost signiﬁcant topic of a document (i.e., the topic with the\nlargest weight in the topic distribution) as the cluster ass ign-\nment, such as in [42]. For document retrieval, we can use the\ndistance of the topic distributions of two documents as thei r\nsemantic distance and report retrieval accuracy as a metric\nof topic modelling [30]. For qualitative analysis, a popular\nchoice is to use visualisation techniques (e.g., t-SNE) on z.\n3 Neural Topic Models with Amortised\nVariational Inference\nThe recent success of deep generative models such as varia-\ntional autoencoders (V AEs) and amortised variational infe r-\nence (A VI) [28; 48 ]has shed light on extending the genera-\ntive process and amortising the inference process of BPTMs,\nwhich is the most popular framework for NTMs. We name\nthis series of models V AE-NTMs. The basic framework of\na V AE follows the description in Section 2.1, where band\nzare the observed and latent variables respectively and the\ngenerative and inference processes are modelled by the DNN-\nbased decoder and encoder respectively. Following [28;\n48], one can learn a V AE model by maximising the Evidence\nLower BOund (ELBO) of the marginal likelihood of the\nBoW data bin terms of θ,φ, andT:Ez∼qz[logp(b|z)]−\nKL[qz/bardblpz],where the RHS term is the Kullback-Leiber\n(KL) divergence. To compute/estimate gradients, tricks li ke\nreparameterisations are usually used to back-propagate gr a-\ndients through the expectation in the LHS term and approx-\nimations are applied when the analytical form of the KL di-\nvergence is unavailable.\nTo adapt the V AE framework for topic modelling, there are\ntwo key questions to be answered: 1)Different from other\napplications, the input data of topic modelling has its uniq ue\nproperties, i.e., bis a high-dimensional, sparse, count-valued\nvector and sis a variable-length sequential data. How to\ndeal with such data is the ﬁrst question for designing a V AE\ntopic model. 2)Interpretability of topics is extremely im-\nportant in topic modelling. When it comes to a V AE model,\nhow to explicitly or implicitly incorporate the word distri -\nbutions of topics (i.e., T) to interpret the latent representa-\ntions or each dimension remains another question. [37]pro-\nposes the ﬁrst answers to the above questions, where the de-\ncoder is developed by specifying the data distribution pbas:\npb:=Multi/parenleftbig\nsoftmax/parenleftbig\nTTz+c/parenrightbig/parenrightbig\n. Herez∈RKmod-\nels the topic distribution of a document, T∈RK×Vmod-\nels the words distributions of the topics, and c∈RVis the\nbias. That is to say, φ:={c}1andT:={T}. For the en-\ncoder which takes bas input and outputs (the samples of) z,\nthe paper follows the original V AE: pz:=N(0,diagK(1));\nqz:=N(µ,diagK(σ2)), whereπ=θ0(b),µ=θ1(π), and\nlogσ=θ2(π). Here,θ:={θ0,θ1,θ2}, all of which are\nmulti-layer perceptrons (MLPs). To better address the abov e\nquestions, various conﬁgurations of the prior distributio npz,\ndata distribution pb, posterior distribution qz, as well as dif-\nferent architectures of the decoder φ, encoder θ, word distri-\nbutions of the topics T, have been proposed for V AE-NTMs.\n1With a slight abuse of notation, we use θandφto denote the\nprojections or the parameters of the projections.Figure 1 shows the taxonomy of V AE-NTMs.\n3.1 Variants of Distributions\nGiven the knowledge and experience of BPTMs, z’s prior\nplays an important role in the quality of topics and document\nrepresentations in topic models. Thus, various constructi ons\nof the prior distributions and their corresponding posteri or\ndistributions have been proposed for V AE-NTMs, aiming to\nbe better alternatives to the normal distributions used in t he\noriginal models.\nVariants of Prior Distributions for z.Note that the appli-\ncation of Dirichlet is one of the key successes of LDA for\nencouraging topic smoothness and sparsity. For V AE-NTMs,\none can apply: pz:=Dir(α0)andqz:=Dir(θ(b)). How-\never, it is difﬁcult to develop an effective reparameterisa tion\nfunction (RF) for Dirichlet, making it hard to compute the\ngradient of the expectation in ELBO. Therefore, various ap-\nproximations have been proposed. For example, [52]uses\nthe Laplace approximation, where Dirichlet samples are ap-\nproximated by these sampled from a logistic normal distri-\nbution, whose mean and co-variance are speciﬁcally conﬁg-\nured. Recall that the Dirichlet distribution can be simulat ed\nby normalising gamma variables, which still do not have non-\ncentral differentiable RF but are easier to approximate. Se v-\neral works have been proposed in this line, such as using the\nWeibull distribution as the approximation of gamma in [68],\napproximating the cumulative distribution function of gam ma\nwith an auxiliary uniform variable in [25], and leveraging the\nproposal function of a rejection sampler of the gamma distri -\nbution as the RF in [4]. Recently, [55]proposes to tackle this\nchallenge by using the so-called rounded RF, which approx-\nimates Dirichlet samples by those drawn from the rounded\nposterior distribution. In addition to the above methods th at\nare speciﬁc to topic modelling or V AEs, other general ap-\nproaches for distributions without RF can also be used in\nV AE-NTMs, such as those in [50; 38 ]. Other than Dirichlet,\n[36]introduces a Gaussian softmax (GSM) function in the en-\ncoder:qz:=softmax/parenleftbig\nN(µ,diagK(σ2))/parenrightbig\nand[51]proposes\nto use a logistic-normal mixture distribution for the prior of\nz. To further enhance the sparsity in z,[34]introduces to use\nthe sparsemax function to replace the softmax in GSM.\nNonparametric Prior for z.Bayesian Nonparametrics\nsuch as the Dirichlet processes and Indian Buffet Processes\nhave been successfully applied in Bayesian topic modelling ,\nenabling to automatically infer the number of topics (i.e., K).\nAs a ﬂexible construction of Dirichlet processes, the stick -\nbreaking process (SBP) is able to generate probability vect ors\nwith inﬁnite dimensions, which has been used to the prior of\nzin V AE-NTMs. Given z∼SBP(α0), we have z1=v1and\nzk=vk/producttext\nj<k(1−vj)fork >1, wherevk∼Beta(1,α0).\nThis procedure can be viewed as iteratively breaking a lengt h-\none stick into multiple ones and the kthiteration breaks the\nstick at the point of vk. Although not for NTMs, [39]pro-\nposes to use SBP to generate zfor V AEs, where VI is done\nby various approximations to the beta distribution of vkwith\ntruncation. [43]adapts this SBP construction for V AE-NTMs\nand also proposes to impose an SBP on the corpus level,\nwhich serves as the prior for the document-level SBP, form-\n\n--- 第4页 ---\nFrameworks of NTMs\nV AE-NTMs\n(Section 3)\nVariants of\nPriors for Topics\n(Section 3.1)\n[52; 68; 25; 4;\n55; 36; 51; 34 ]Nonparametric\npriors for\nTopics\n(Section 3.1)\n[39; 43;\n36; 62 ]Variants of\nBoW Data\nDistributions\n(Section 3.1)\n[71]Variants\nof Word\nDistributions\n(Section 3.1)\n[26; 11; 13 ]Correlated\nand Structured\nTopics\n(Section 3.2)\n[35; 24;\n68; 14 ]NTMs with\nMeta-data\n(Section 3.3)\n[6; 29;\n60; 2 ]NTMs for\nShort Text\n(Section 3.4)\n[67; 74;\n33; 15;\n63; 21 ]Sequential\nNTMs\n(Section 3.5)\n[40; 66;\n12; 44; 47 ]NTMs with\nPre-trained\nLanguage\nModels\n(Section 3.6)\n[3; 54;\n8; 22 ]DocNADE-\nNTMs\n(Section 4.1)\n[30; 18;\n20; 19 ]GAN-NTMs\n(Section 4.2)\n[57; 56; 23 ]GNN-NTMs\n(Section 4.3)\n[74; 65; 73 ]Other\nFrameworks\n(Section 4.4)\n[5; 9;\n45; 16;\n41; 70 ]\nFigure 1: A taxonomy of the papers regarding neural topic mod els\ning into a hierarchical model. In [36], the break points vk\nare generated from a posterior modelled by a recurrent neu-\nral network (RNN) with normal noises as input, making the\nmodel able to automatically infer Kin a truncation-free man-\nner. Recently, [62]uses the truncated (gamma) negative bi-\nnomial process to generate discrete vectors for z(i.e. each\nentry ofzis equivalently generated by an independent Pois-\nson distribution), which gives the model certain ability to be\nnonparametric.\nVariants of Data Distribution pb.In addition to impos-\ning different distributions on z,[71]proposes to replace\nthe multinomial data distribution used in other NTMs with\nthe negative-binomial distribution to capture overdisper sion:\nb∼NB(φ0(z),φ1(z)), where two separate decoders φ0\nandφ1are proposed to generate the two parameters of the\nnegative-binomial distribution from z.\nVariants of Word Distributions T.Conventionally, the\ncollection of the word distributions of the topics Tis aK×V\nmatrix, i.e., T∈RK×VwithKV free parameters to learn. In\nNTMs, it has been popular to factorise the matrix into a prod-\nuct of topic and word embeddings, meaning that the relevance\nbetween a topic and a word is captured by their distance in the\nembedding space. This construction has been studied in de-\ntails in [26; 11; 13 ].\n3.2 Correlated and Structured Topics\nTopics discovered by conventional topic models like LDA are\nusually independent. An important research direction is to\nexplicitly capture topic correlations (e.g. pairwise rela tions\nbetween topics) or structures (e.g. tree structures of topi cs),\nwhich has been studied in NTMs as well. Following the\nframework of V AE with Householder ﬂow, which enables to\ndrawzfrom the normal posterior with a non-diagonal covari-\nance matrix, [35]develops a more efﬁcient centralised trans-\nformation ﬂow for NTMs, which is able to discover pairwise\ntopic correlations by the covariance matrix. In terms of tre e-\nstructured topics, [24]introduces to generate a series of topics\nfrom the root to the leaf of a topic tree with a doubly-recurre nt\nneural network [1]. When applied in topic modelling, the\ngamma belief network (GBN) can be viewed as a Bayesian\nmodel that also discovers three-structured topics, whose i n-\nference is done by Gibbs sampling. [68]introduces the NTM\ncounterpart of GBN, which leverages A VI as the inferenceprocess and signiﬁcantly improves the test time of GBN. [14]\nproposes an structured V AE-NTM that discovers topics with\nrespect to different aspects, specialising in modelling us er re-\nviews.\n3.3 NTMs with Meta-data\nConventionally, topic models learn from documents in an un-\nsupervised way. However, documents are usually associated\nwith rich sets of meta-data on both document and word lev-\nels, such as document labels, authorships, and pre-trained\nword embeddings, which can be used to improve topic qual-\nity or document representation quality for supervised task s\n(e.g., accuracy of predicting document meta-data). [6]pro-\nposes a V AE-NTM that is able to incorporate various kinds\nof meta-data, where the BoW data bof a document and its la-\nbels (e.g., sentiment) are generated with a joint process co ndi-\ntioned on the document’s covariates (e.g., publication yea r) in\nthe decoder and the encoder generates zby conditioning on\nall types of data of the document: BoW, covariates, and la-\nbels. Instead of specifying the generative model as a direct ed\nnetwork as in most of topic models, [29]introduces the logis-\ntic LDA model whose generative process can be viewed as an\nundirected graph. In addition to the BoW data, a document’s\nlabel is also an observed variable in the graph. Following\na few assumptions of factorisation in the generative proces s,\nthe paper manually speciﬁes the complete conditional distr i-\nbutions in the graph with the interactions between the laten t\nvariables captured by neural networks. The inference is don e\nby the mean-ﬁeld VI and zin the model is further trained to\nbe more discriminative for the classiﬁcation of labels. Giv en a\nset of documents with labels, [60]uses a V AE-NTM to model\na document’s BoW data and an RNN classiﬁer to predict a\ndocument’s label based on its sequential data in a joint trai n-\ning process. The paper combines the two models by intro-\nducing an attention mechanism in the RNN which takes doc-\numents’ topics into account. [2]proposes to incorporate rela-\ntional graphs (e.g. citation graph) of documents into NTMs,\nwhere the topic distributions of two document are fed into a\nnetwork with MLPs to predict whether they should be con-\nnected.\n3.4 NTMs for Short Texts\nTexts generated on the internet (e.g., tweets, news headlin es\nand product reviews) can be short, meaning that each indi-\n\n--- 第5页 ---\nvidual document contains insufﬁcient word co-occurrence i n-\nformation. This results in degraded performance for both\nBPTMs and NTMs. To tackle this issue, one can limit a\nmodel’s capacity and to enhance the contextual information\nof short texts. [67]proposes a combination of an NTM and a\nmemory network for short text classiﬁcation in a similar spi rit\nto[60]. The main difference is the memory network instead\nof RNN is responsible for classiﬁcation, which is informed\nby the topic distributions learned by the NTM. To enhance\nthe contextual information of short documents, [74]proposes\nan NTM whose encoder is a graph neural network (GNN)\ntaking the biterms graph of the words in sampled documents\nas inputs and outputting the topic distribution for the whol e\ncorpus. The model also learns a decoder that reconstructs\nthe input biterms graph. Despite the novel idea, the model\nmight not be able to generate the topic distribution of an in-\ndividual document. To limit a short document to focus on\nseveral salient topics, [33]introduces to use the Archimedean\ncopulas to regularise the discreteness of topic distributi ons for\nshort texts. [15]proposes an NTM with reinforced content by\nlimiting the number of the active topics for each short docu-\nment and informing the word distributions of the topics by\nusing pretrained word embeddings. [63]introduces an NTM\nwith vector quantisation over z, i.e., a document’s topic dis-\ntribution can only be one vector in the learned dictionary in\nthe vector quantisation process. In addition to maximising\nthe likelihood of the input documents, the paper introduces\nto minimise the likelihood of the negatively-sampled “fake\ndocuments”. Although not directly addressing the short tex t\nproblem for topic modelling, [21]introduces NTMs for mod-\nelling microblog conversations, by leveraging their uniqu e\nmeta data and structures.\n3.5 Sequential NTMs\nThe ﬂexibility of V AE-NTMs enables to leverage various\nneural network architectures for the encoder and decoder.\nWith the help of sequential networks like RNNs, unlike other\nNTMs working with BoW data (i.e., b), sequential NTMs\n(SNTMs) usually take sequences of words of documents (i.e.,\ns) and are able to capture the orders of words, sentences, and\ntopics. [40]proposes an SNTM working with s, which sam-\nples a topic for each sentence of an input document accord-\ning tozand then generates the word sequence of the sentence\nwith an RNN conditioned on the sentence’s topic. Note that z\nis attached to a document and shared across all its sentences .\nIn[66], givens, a word’s topic is conditioned on its previ-\nous word’s and this order dependency is captured by a long\nshort-term memory (LSTM) model. At the similar period of\ntime, [12]independently proposes an SNTM whose gener-\native process is similar to [66], with an additional variable\nmodelling stop words and several variants in the inference\nprocess. Recently, [44]proposes to use an LSTM with at-\ntentions as the encoder taking sas input, where the attention\nincorporates topical information with a context vector tha t is\nconstructed by topic embeddings and document embeddings.\n[47]introduces an SNTM that is related to [12], where instead\nof marginalising out the discrete topic assignments, the pa per\nproposes to generate them from an RNN model. This helps\nto avoid using reparameterisation tricks in the variationa l in-ference.\n3.6 NTMs with Pre-trained Language Models\nRecently, pre-trained transformer-based language models\nsuch as BERT are becoming ubiquitous in NLP. Pre-trained\non large corpora, such models usually have a ﬁne-grained\nability to capture aspects of linguistic context, which can be\npartially represented by contextual word embeddings. Thes e\ncontextual word embeddings can provide richer context info r-\nmation than BoW or sequential data, which has been recently\nused to assist the training of topic models. Instead of using\nthe BoW or sequential data of a document as the input of the\nencoder, [3]proposes to use the document embedding vector\ngenerated by Sentence-BERT [46]and to keep the remaining\npart of an NTM the same as [52].[54]shows that the clusters\nobtained by performing clustering algorithms (e.g., Kmean s)\non the contextual word embeddings generated by various\npre-trained models can be interpreted as topics, similar to\nthose discovered by LDA. Having similar ideas with [67;\n60],[8]proposes to combine an NTM with a ﬁne-tuned BERT\nmodel by concatenating the topic distribution and the learn ed\nBERT embedding of a document as the features for docu-\nment classiﬁcation. [22]proposes an NTM learned by distill-\ning knowledge from a pre-trained BERT model. Speciﬁcally,\ngiven a document, the BERT model generates the predicted\nprobability for each word then the paper introduces to aver-\nage those probabilities to generate a pseudo BoW vector for\nthe document. An NTM following [6]is used to reconstruct\nboth the actual and pseudo BoW data.\n4 NTMs based on Other Frameworks\nBesides V AE-NTMs, there are other frameworks for NTMs\nthat also draw research attention.\n4.1 NTMs based on Autoregressive Models\nV AE-NTMs gained popularity after V AEs were invented. Be-\nfore that, NTMs based on the autoregressive framework had\nbeen studied. Speciﬁcally, [30]proposes an autoregressive\nNTM, named DocNADE, similar to the spirit of RNNs, where\nthe predictive probability of a word in a document is condi-\ntioned on its hidden state, which is further conditioned on\nthe previous words. A hidden unit can be interpreted as a\ntopic and a document’s hidden states capture its topic distr i-\nbution. The learning is done by maximising the likelihood\nof the input documents. Recently, [18]extends DocNADE\nby introducing a structure similar to the bi-directional RN N,\nwhich allows to model bi-directional dependencies between\nwords. [19]combines DocNADE with an LSTM for incorpo-\nrating external knowledge. [20]extends DocNADE into the\nlife long learning settings.\n4.2 NTMs based on Generative Adversarial Nets\nBesides V AEs, generative adversarial networks (GANs) are\nanother popular series of deep generative models. Recently ,\nthere are a few attempts on adapting the GAN framework for\ntopic modelling. [57]proposes a GAN generator that takes\na random sample of the Dirichlet distribution as a topic dis-\ntribution ˜zand generates the word distributions of a “fake”\n\n--- 第6页 ---\ndocument conditioning on ˜z. A discriminator is introduced\nto distinguish between generated word distributions and re al\nword distributions obtained by normalising the TF-IDF vec-\ntors of real documents. Although the proposed model is able\nto discover interpretable topics, it cannot learn topic dis tribu-\ntions for documents. To address this issue, [56]introduces an\nadditional encoder that learns zfor a given document. More-\nover,zis concatenated with the word distribution of a doc-\nument as a real datum and ˜zis concatenated with the gener-\nated word distribution as a fake datum. The discriminator is\ndesigned to distinguish between the real and fake ones. [23]\nfurther extends the above model with a CycleGAN frame-\nwork.\n4.3 NTMs based on Graph Neural Networks\nInstead of viewing a document as a sequence or bag of words,\none can consider the graph presentations of a corpus of docu-\nments. This perspective enables leveraging a variety of GNN s\nto discover latent topics. As discussed in Section 3.4, [74]\nviews a collection of documents as a biterm word graph.\nWhile [65; 73 ]model a corpus by a bipartite graph with doc-\numents and words as two separate parties and connected by\nthe occurrences of words in documents. For the former, it di-\nrectly uses the word occurrences of documents as the weights\nof the connections between them and for the latter, it uses\nTF-IDF values instead.\n4.4 NTMs based on Other Frameworks\nIn addition to the above frameworks, other kinds of NTMs\nhave also been developed. An NTM is developed in [5]that\ntakes n-gram embeddings (obtained from word embeddings)\nand a document index as input and then predicts whether an n-\ngram is in the document. [9]proposes an autoencoder model\nfor NTMs where the neurons in the hidden layer of the au-\ntoencoder compete with each other, focusing them to be spe-\ncialised in recognising speciﬁc data patterns. [45]proposes\nan NTM based on matrix factorisation. [16]proposes a rein-\nforcement learning framework for NTMs, where the encoder\nand decoder of an NTM are kept. In addition, an agent takes\nactions to select the topical-coherent words from a documen t\nand uses the selected words as the input document for the\nencoder. The reward to the agent is the topic coherence of\nthe reconstructed document from the decoder. [41]adapts\nthe framework of Wasserstein auto-encoders (WAEs), which\nminimises the Wasserstein distance between reconstructed\ndocuments from the decoder and real documents, similarly to\nV AE-NTMs. [70]recently introduces a NTM based on opti-\nmal transport, which directly minimises the optimal transp ort\ndistance between the topic distribution learned by an encod er\nand the word distribution of a document.\n5 Applications of NTMs\nAlthough just recently developed, NTMs have been actively\nused in various applications. Compared with conventional\ntopic models, NTMs have the appealing advantages of ﬂex-\nibility: 1)NTMs are ﬂexible in representing topic distribu-\ntions of documents and word distributions of topics with ei-\nther probability vectors or embeddings, and are more easilyincorporated into broader models. 2)The inference process\nof NTMs can usually be formulated as an optimisation pro-\ncess with gradients, which is more conveniently integrated\nwith other DNN models for joint training.\nMany DNN models used for language such as RNNs,\ntransformers, and attention might not be able to capture lon g-\nrange dependency well. On the contrary, working on BoW\ndata, NTMs are good at learning global semantic representa-\ntions for long texts, which can serve complementary informa -\ntion to the above models. This leads to a wide range of ap-\nplications of NTMs in NLP such as language models [31; 58;\n64; 17; 27 ], text generation [53; 59 ], and summarisation [10;\n72; 61 ]. Due to the space limit of the references, a detailed\nlist of application papers are omitted.\n6 Discussion\nIn this paper, we have reviewed neural topic models, the most\npopular research trend of topic modelling in the deep learn-\ning era. A variety of NTMs based on different frameworks\nhave been developed and due to the appealing ﬂexibility, ef-\nfectiveness, and efﬁciency, NTMs show a promising poten-\ntial in a range of applications. In addition to providing an\noverview of existing approaches of NTMs, we would like to\ndiscuss the following challenges and opportunities. 1) Bet-\nter evaluation: As stated in Section 2.2, evaluation of topic\nmodels is challenging. This is mainly because there has not\nbeen a uniﬁed system of evaluation metrics, making the com-\nparisons across different NTMs harder due to the variety of\nframeworks, architectures and datasets. For example, V AE-\nNTMs calculate perplexity using the ELBO, attached to the\nmodels with variational inference, which cannot be compare d\nwith models without ELBO. Also for topic coherence and\ndownstream performance, the evaluation processes, metric s,\nsettings usually vary in different papers. As a topic model\nshould be evaluated with comprehensive metrics, it could be\ntendentious to only use one kind of metric (e.g., topic co-\nherence), which can reﬂect just one aspect of a model. There-\nfore, uniﬁed platforms and benchmarks for NTMs are needed.\n2) Richer architectures and applications: Compared to\nBPTMs, NTMs offer better ﬂexibility for representing topic\ndistributions for documents and word distributions for top ics.\nParticularly, projecting documents, topics, and words int o a\nuniﬁed embedding space transforms the thinking of the re-\nlationships between the three. Given this ﬂexibility, NTMs\nare expected to get integrated with the most recent neural ar -\nchitectures and play a unique role in richer applications. 3)\nMore external knowledge: With the development of topic\nmodels including NTMs, people have not stopped seeking to\nleverage external knowledge to help the learning, from doc-\nument meta-data to pre-trained word embeddings. Recently-\nproposed pre-trained language models (e.g., BERT) provide\nmore advanced, ﬁner-grained, and higher-level representa -\ntions of semantic knowledge (e.g., contextual word embed-\ndings over global embeddings), which can be leveraged in\nNTMs to boost performance. Although the marriage between\nNTMs and language models is still an emerging area, we ex-\npect to see more developments in this important direction.\n\n--- 第7页 ---\nReferences\n[1]D. Alvarez-Melis and T. S. Jaakkola. Tree-structured\ndecoding with doubly-recurrent neural networks. In\nICLR , 2017.\n[2]H. Bai, Z. Chen, M. R. Lyu, I. King, and Z. Xu. Neural\nrelational topic models for scientiﬁc article analysis. In\nCIKM , 2018.\n[3]F. Bianchi, S. Terragni, and D. Hovy. Pre-training is\na hot topic: Contextualized document embeddings im-\nprove topic coherence. arXiv , 2020.\n[4]S. Burkhardt and S. Kramer. Decoupling sparsity\nand smoothness in the Dirichlet variational autoencoder\ntopic model. JMLR , 2019.\n[5]Z. Cao, S. Li, Y . Liu, W. Li, and H. Ji. A novel neu-\nral topic model and its supervised extension. In AAAI ,\n2015.\n[6]D. Card, C. Tan, and N. A. Smith. Neural models for\ndocuments with metadata. In ACL, 2018.\n[7]J. Chang, J. Boyd-Graber, C. Wang, S. Gerrish, and\nD. M. Blei. Reading tea leaves: How humans interpret\ntopic models. In NeurIPS , 2009.\n[8]Y . Chaudhary, P. Gupta, K. Saxena, V . Kulkarni, T. Run-\nkler, and H. Sch¨ utze. TopicBERT for energy efﬁcient\ndocument classiﬁcation. In EMNLP , 2020.\n[9]Y . Chen and M. J. Zaki. KATE: K-competitive autoen-\ncoder for text. In SIGKDD , 2017.\n[10]P. Cui, L. Hu, and Y . Liu. Enhancing extractive text\nsummarization with topic-aware graph neural networks.\nInCOLING , 2020.\n[11]A. B. Dieng, F. J. Ruiz, and D. M. Blei. Topic modeling\nin embedding spaces. TACL , 2020.\n[12]A. B. Dieng, C. Wang, J. Gao, and J. Paisley. Topi-\ncRNN: A recurrent neural network with long-range se-\nmantic dependency. In ICLR , 2017.\n[13]R. Ding, R. Nallapati, and B. Xiang. Coherence-aware\nneural topic modeling. In EMNLP , 2018.\n[14]B. Esmaeili, H. Huang, B. Wallace, and J.-W. van de\nMeent. Structured neural topic models for reviews. In\nAISTATS , 2019.\n[15]J. Feng, Z. Zhang, C. Ding, Y . Rao, and H. Xie. Context\nreinforced neural topic modeling over short texts. arXiv ,\n2020.\n[16]L. Gui, J. Leng, G. Pergola, R. Xu, and Y . He. Neural\ntopic model with reinforcement learning. In EMNLP-\nIJCNLP , 2019.\n[17]D. Guo, B. Chen, R. Lu, and M. Zhou. Recurrent hier-\narchical topic-guided RNN for language generation. In\nICML , 2020.\n[18]P. Gupta, Y . Chaudhary, F. Buettner, and H. Sch¨ utze.\nDocument informed neural autoregressive topic models\nwith distributional prior. In AAAI , 2019.[19]P. Gupta, Y . Chaudhary, F. Buettner, and H. Sch¨ utze.\nTexttovec: Deep contextualized neural autoregressive\ntopic models of language with distributed compositional\nprior. In ICLR , 2019.\n[20]P. Gupta, Y . Chaudhary, T. Runkler, and H. Schuetze.\nNeural topic modeling with continual lifelong learning.\nInICML , 2020.\n[21]R. He, X. Zhang, D. Jin, L. Wang, J. Dang, and X. Li.\nInteraction-aware topic model for microblog conversa-\ntions through network embedding and user attention. In\nCOLING , 2018.\n[22]A. M. Hoyle, P. Goel, and P. Resnik. Improving neural\ntopic models using knowledge distillation. In EMNLP ,\n2020.\n[23]X. Hu, R. Wang, D. Zhou, and Y . Xiong. Neural topic\nmodeling with cycle-consistent adversarial training. In\nEMNLP , 2020.\n[24]M. Isonuma, J. Mori, D. Bollegala, and I. Sakata. Tree-\nstructured neural topic model. In ACL, 2020.\n[25]W. Joo, W. Lee, S. Park, and I.-C. Moon. Dirichlet vari-\national autoencoder. Pattern Recognition , 2020.\n[26]N. Jung and H. I. Choi. Continuous semantic topic em-\nbedding model using variational autoencoder. arXiv ,\n2017.\n[27]N. Kawamae. Topic structure-aware neural language\nmodel: Uniﬁed language model that maintains word and\ntopic ordering by their embedded representations. In\nWWW , 2019.\n[28]D. P. Kingma and M. Welling. Auto-encoding varia-\ntional Bayes. In ICLR , 2014.\n[29]I. Korshunova, H. Xiong, M. Fedoryszak, and L. Theis.\nDiscriminative topic modeling with logistic LDA. In\nNeurIPS , 2019.\n[30]H. Larochelle and S. Lauly. A neural autoregressive\ntopic model. NeurIPS , 2012.\n[31]J. H. Lau, T. Baldwin, and T. Cohn. Topically driven\nneural language model. In ACL, 2017.\n[32]J. H. Lau, D. Newman, and T. Baldwin. Machine read-\ning tea leaves: Automatically evaluating topic coher-\nence and topic model quality. In ACL, 2014.\n[33]L. Lin, H. Jiang, and Y . Rao. Copula guided neural topic\nmodelling for short texts. In SIGIR , 2020.\n[34]T. Lin, Z. Hu, and X. Guo. Sparsemax and relaxed\nWasserstein for topic sparsity. In WSDM , 2019.\n[35]L. Liu, H. Huang, Y . Gao, Y . Zhang, and X. Wei. Neural\nvariational correlated topic modeling. In WWW , 2019.\n[36]Y . Miao, E. Grefenstette, and P. Blunsom. Discovering\ndiscrete latent topics with neural variational inference.\nInICML , 2017.\n[37]Y . Miao, L. Yu, and P. Blunsom. Neural variational in-\nference for text processing. In ICML , 2016.\n\n--- 第8页 ---\n[38]C. Naesseth, F. Ruiz, S. Linderman, and D. Blei. Repa-\nrameterization gradients through acceptance-rejection\nsampling algorithms. In AISTATS , 2017.\n[39]E. Nalisnick and P. Smyth. Stick-breaking variational\nautoencoders. In ICLR , 2017.\n[40]R. Nallapati, I. Melnyk, A. Kumar, and B. Zhou.\nSengen: Sentence generating neural variational topic\nmodel. arXiv , 2017.\n[41]F. Nan, R. Ding, R. Nallapati, and B. Xiang. Topic mod-\neling with Wasserstein autoencoders. In ACL, 2019.\n[42]D. Q. Nguyen, R. Billingsley, L. Du, and M. Johnson.\nImproving topic models with latent feature word repre-\nsentations. TACL , 2015.\n[43]X. Ning, Y . Zheng, Z. Jiang, Y . Wang, H. Yang,\nJ. Huang, and P. Zhao. Nonparametric topic modeling\nwith neural inference. Neurocomputing , 2020.\n[44]M. Panwar, S. Shailabh, M. Aggarwal, and B. Krishna-\nmurthy. TAN-NTM: Topic attention networks for neural\ntopic modeling. arXiv , 2020.\n[45]M. Peng, Q. Xie, Y . Zhang, H. Wang, X. J. Zhang,\nJ. Huang, and G. Tian. Neural sparse topical coding.\nInACL, 2018.\n[46]N. Reimers and I. Gurevych. Sentence-BERT: Sen-\ntence embeddings using siamese BERT-networks. In\nEMNLP-IJCNLP , 2019.\n[47]M. Rezaee and F. Ferraro. A discrete variational re-\ncurrent topic model without the reparametrization trick.\nNeurIPS , 2020.\n[48]D. J. Rezende, S. Mohamed, and D. Wierstra. Stochas-\ntic backpropagation and approximate inference in deep\ngenerative models. In ICML , 2014.\n[49]M. R¨ oder, A. Both, and A. Hinneburg. Exploring the\nspace of topic coherence measures. In WSDM , 2015.\n[50]F. Ruiz, M. Titsias, and D. Blei. The generalized repa-\nrameterization gradient. NeurIPS , 2016.\n[51]D. Silveira, A. Carvalho, M. Cristo, and M.-F. Moens.\nTopic modeling using variational auto-encoders with\nGumbel-softmax and logistic-normal mixture distribu-\ntions. In IJCNN , 2018.\n[52]A. Srivastava and C. Sutton. Autoencoding variational\ninference for topic models. In ICLR , 2017.\n[53]H. Tang, M. Li, and B. Jin. A topic augmented text gen-\neration model: Joint learning of semantics and structural\nfeatures. In EMNLP-IJCNLP , 2019.\n[54]L. Thompson and D. Mimno. Topic modeling with con-\ntextualized word representation clusters. arXiv , 2020.\n[55]R. Tian, Y . Mao, and R. Zhang. Learning V AE-\nLDA models with rounded reparameterization trick. In\nEMNLP , 2020.\n[56]R. Wang, X. Hu, D. Zhou, Y . He, Y . Xiong, C. Ye, and\nH. Xu. Neural topic modeling with bidirectional adver-\nsarial training. In ACL, 2020.[57]R. Wang, D. Zhou, and Y . He. ATM: Adversarial-neural\ntopic model. Information Processing & Management ,\n2019.\n[58]W. Wang, Z. Gan, W. Wang, D. Shen, J. Huang, W. Ping,\nS. Satheesh, and L. Carin. Topic compositional neural\nlanguage model. In AISTATS , 2018.\n[59]W. Wang, Z. Gan, H. Xu, R. Zhang, G. Wang, D. Shen,\nC. Chen, and L. Carin. Topic-guided variational auto-\nencoder for text generation. In NAACL , pages 166–177,\n2019.\n[60]X. Wang and Y . Yang. Neural topic model with attention\nfor supervised learning. In AISTATS , 2020.\n[61]Z. Wang, Z. Duan, H. Zhang, C. Wang, L. Tian,\nB. Chen, and M. Zhou. Friendly topic assistant for trans-\nformer based abstractive summarization. In EMNLP ,\n2020.\n[62]J. Wu, Y . Rao, Z. Zhang, H. Xie, Q. Li, F. L. Wang, and\nZ. Chen. Neural mixed counting models for dispersed\ntopic discovery. In ACL, 2020.\n[63]X. Wu, C. Li, Y . Zhu, and Y . Miao. Short text topic\nmodeling with topic distribution quantization and nega-\ntive sampling decoder. In EMNLP , 2020.\n[64]Y . Xiao, T. Zhao, and W. Y . Wang. Dirichlet variational\nautoencoder for text modeling. arXiv , 2018.\n[65]L. Yang, F. Wu, J. Gu, C. Wang, X. Cao, D. Jin, and\nY . Guo. Graph attention topic modeling network. In\nWWW , 2020.\n[66]M. Zaheer, A. Ahmed, and A. J. Smola. Latent LSTM\nallocation: Joint clustering and non-linear dynamic\nmodeling of sequence data. In ICML , 2017.\n[67]J. Zeng, J. Li, Y . Song, C. Gao, M. R. Lyu, and I. King.\nTopic memory networks for short text classiﬁcation. In\nEMNLP , 2018.\n[68]H. Zhang, B. Chen, D. Guo, and M. Zhou. Whai:\nWeibull hybrid autoencoding inference for deep topic\nmodeling. In ICLR , 2018.\n[69]H. Zhao, L. Du, W. Buntine, and M. Zhou. Dirichlet\nbelief networks for topic structure learning. In NeurIPS ,\n2018.\n[70]H. Zhao, D. Phung, V . Huynh, T. Le, and W. Buntine.\nNeural topic model via optimal transport. In ICLR ,\n2020.\n[71]H. Zhao, P. Rai, L. Du, W. Buntine, D. Phung, and\nM. Zhou. Variational autoencoders for sparse and\noverdispersed discrete data. In AISTATS , 2020.\n[72]C. Zheng, K. Zhang, H. J. Wang, and L. Fan. Topic-\naware abstractive text summarization. arXiv , 2020.\n[73]D. Zhou, X. Hu, and R. Wang. Neural topic modeling by\nincorporating document relationship graph. In EMNLP ,\n2020.\n[74]Q. Zhu, Z. Feng, and X. Li. Graphbtm: Graph en-\nhanced autoencoded variational inference for biterm\ntopic model. In EMNLP , 2018.",
      "download_time": "2025-12-14T21:50:32.335400",
      "word_count": 45861
    },
    {
      "index": 4,
      "title": "A Topic Model Approach to Multi-Modal Similarity",
      "authors": [
        "Rasmus Troelsgård",
        "Bjørn Sand Jensen",
        "Lars Kai Hansen"
      ],
      "published": "2014-05-27",
      "source": "arxiv",
      "url": "http://arxiv.org/abs/1405.6886v1",
      "pdf_url": "http://arxiv.org/pdf/1405.6886v1.pdf",
      "pdf_path": "papers\\04_A Topic Model Approach to Multi-Modal Similarity.pdf",
      "txt_path": "papers\\04_A Topic Model Approach to Multi-Modal Similarity.txt",
      "content": "--- 第1页 ---\nA Topic Model Approach to Multi-Modal Similarity\nRasmus Troelsg ˚ard, Bjørn Sand Jensen and Lars Kai Hansen\nDepartment of Applied Mathematics and Computer Science\nTechnical University of Denmark\nMatematiktorvet 303B, 2800 Kgs. Lyngby\nfrast,bjje,lkai g@dtu.dk\nAbstract\nCalculating similarities between objects deﬁned by many heterogeneous data\nmodalities is an important challenge in many multimedia applications. We use a\nmulti-modal topic model as a basis for deﬁning such a similarity between objects.\nWe propose to compare the resulting similarities from different model realizations\nusing the non-parametric Mantel test. The approach is evaluated on a music dataset.\n1 Introduction\nCalculating similarity between objects linked to multiple data sources is more urgent than ever. A\nprime example is the typical multimedia application of music services where users face a virtually\ninﬁnite pool of songs to choose from. Here choices are based on many different information sources\nincluding the audio/sound, meta-data like genre, and social inﬂuences [ 1], hence, attempts of modeling\nthe geometry of music navigation have taken on a multi-modal perspective. In fusing heterogeneous\nmodalities like audio, genre, and user generated tags it is both a challenge to establish a combined\nmodel in a ’symmetric’ manner so that one modality do not dominate others and it is challenging to\nevaluate the quality of the resulting geometric representation. Here, we focus on the latter issue by\ntesting the consistency of derived inter-song (dis-)similarity by means of direct comparison between\nsimilarities using the Mantel permutation test.\nTopic models have previously been used to infer geometry in the image and music domain, e.g. by [ 2]\ncombining audio features and listening histories. In [ 3] images and tags were analyzed, also by means\nof a multi-modal topic model. In [ 4] music similarity is inferred with a nonparametric Bayesian\nmodel, and [ 5] describe multiple multi-modal extensions to basic LDA models and evaluate the\nmodels on an image information retrieval task. Furthermore, topic model induced similarities among\ndocuments have been put to use in a navigation application [ 6], and different similarity estimates are\nalso discussed in relation to a content-based image retrieval problem [7].\n2 Model & Inference\nTo be able to measure similarities between objects, a representation of these objects is needed. In this\nwork we use a version of Latent Dirichlet Allocation that incorporates multiple sources of information\ninto a joint object representation similar to [ 5]. In [ 8], this model was applied to a multilingual corpus.\nEach object is represented by a multinomial distribution over topics which is common for all of\nthe modalities composing the object. Each topic is deﬁned by a set of multinomial distributions\nover features, each of which is deﬁned on the vocabulary speciﬁc for a modality. To explain the\ncharacteristics of the model, the assumed generative process for objects is outlined in ﬁgure 1\ntogether with a graphical representation of the model. The difference from a number of individual\nLDA models, each deﬁned on a separate modality, is that each object is described by a single,\nshared distribution over topics, which potentially induces strong dependencies between the feature\ndistributions representing the same topic in the individual modalities.\n1arXiv:1405.6886v1  [cs.IR]  27 May 2014\n\n--- 第2页 ---\n\u000fFor each topic indexed by t2[1;T]in each modality indexed\nbym2[1;M]\nDraw \u001e(m)\nt\u0018Dirichlet (\f(m))\nThis is the parameters of the tthtopic’s distribution over\nvocabulary [1;V(m)]of modality m.\n\u000fFor each document indexed by d2[1;D]\n–Draw \u0012d\u0018Dirichlet (\u000b)\nThis is the parameters of the dthdocuments’s distribu-\ntion over topics [1;T].\n–For each modality m2[1;M]\n\u0003For each word win the mthmodality of document\nd\n\u0001Draw a speciﬁc topic z(m)\u0018Categorical (\u0012d)\n\u0001Draw a word w(m)\u0018Categorical (\u001e(m)\nz(m))\n(a) Generative process\n(b) The multi-modal Latent Dirich-\nlet Allocation model represented as\na probabilistic graphical model.\nFigure 1\nPerforming inference in the model amounts to estimation of the posterior distributions over the latent\nvariables. We use a Gibbs sampler inspired by the sparsity improvements proposed by [ 9]. For\nevaluation (see section 4), we use point estimates \u0012sand\u001esderived from a sample zsfrom the\nMarkov chain, by taking the expectations of the respective posterior Dirichlet distributions deﬁned by\nzs. In this work we choose the state of the chain with the highest model evidence within the last 50\nout of 4000 iterations. Hyper-parameters are optimized using ﬁxed point updates [ 10,11]. The prior\non the document topic distributions is an asymmetric Dirichlet with parameter \u000b, and the priors over\nthe vocabularies of the respective modalities are symmetric Dirichlet distributions with parameters\n\f(m).\n3 Similarities in Topic Models\nAs already hinted, there are many ways to deﬁne and calculate similarities in topic models; both\nbetween topics and documents. In this paper we focus on the latter. Most methods in literature\nare based solely on the distributions of topics in the documents, \u0012, e.g. [ 4] measures the Kullback-\nLeibler divergence between two such distributions, while [ 7] also mentions inner products and cosine\nsimilarities as candidates. With focus on visualization, [ 6], introduces the yet another dissimilarity\nmeasure based on topic proportions. [ 7] promotes a measure based on the predictive likelihood of the\ndocument contents, and this approach is the basis of the method chosen here; The similarity of two\ndocuments AandBis given by the mean per-word log-likelihood of the words of document Agiven\nthe topic distribution of document B(and the vocabulary distributions).\nlogp(wAj\u0012s\nB;\u001es)\nPM\nm=1N(m)\nA;where p(wAj\u0012s\nB;\u001es) =MY\nm=1N(m)\nAY\ni=1TX\nt=1(\u001e(m)\nt;w(m)\nAi)>\u0012t;B (1)\nWe use this approach to calculate a non-symmetric similarity matrix between all objects in the\nheld-out cross-validation fold, for which the topic proportions have been estimated using “fold-in”.\n1While this similarity measure is more computationally demanding than e.g. the KL-divergence,\nwhen the number of topics Tused in the model increases, it might happen that some topics have\nvocabulary distributions that are very alike and only differ on a few words. Thus two documents with\nmainly the same type of content may have large proportions of different topics, causing them to be\nvery dissimilar according to a topic proportion based measure. For a non-parametric topic model\nsuch as [ 4], this might not be a large concern, however, for parametric topic models, this should be\ntaken into consideration. Generally, most of the discussed similarity measures are not proper metrics\n1For the few held-out documents that do not contain any words in the modalities used for model estimation,\nwe chose to simulate a uniform distribution of words in such an empty document by one occurrence of every\nword in the vocabulary.\n2\n\n--- 第3页 ---\nin the geometric sense, but for (dis-)similarity purposes the exact properties might not be important,\ndepending on the application.\nComparing Similarities - the Mantel test\nAn important aspect of this work is the ability to assess the relations between different similarities\ninduced by models estimated from multiple, possibly different, heterogeneous data sources. To\ncompare such similarities we look at the correlation between the deﬁned similarities. For testing\nthe signiﬁcance of the correlations we can apply a Mantel style test [ 12]. The Mantel test is a\nnon-parametric test to assess the relation between two (dis-)similarity matrices. The null hypothesis\nis that the two matrices are unrelated, and the null distribution is approximated by calculating the\ntest statistic for a large number of random permutations of the two matrices (excluding the diagonal\nelements); permuting rows and columns together to maintain the distribution of (dis-)similarities for\neach object. In this work we use Spearman’s correlation coefﬁcient as the test statistic.\n4 Experimental Results: Music Similarity\nIn this preliminary study we examine induced similarities in a subset of the Million Song Dataset [ 13],\nconsisting of 30.000 tracks with equal proportions of 15 different genres. Each track is composed\nof data from a number of different sources: Open vocabulary tags from users (last.fm), Lyrics\n(musiXmatch.com), Editorial artist tags (allmusic.com), Artist tags (musicBrainz), User listening\nhistory (echonest), Genre and style (allmusic), and Audio Features (echonest). All modalities—\nbesides the audio features—are naturally occurring as counts of words and for the audio we turn\nto an audio word approach, where the continuous features are vector quantized into a total of 2144\nwords. For this pilot study we estimate topic models on combinations of groups of modalities from\nthe mentioned list, respectively consisting of the ﬁrst 5, the genre and style labels, and the audio.\nTo be able to assess the model stability of the similarities, we estimate each model ﬁve times from\ndifferent random initialisations of the Markov chain. This is done for every training set of a 10-fold\ncross-validation split. The correlations between all combinations of the 5 similarity matrices resulting\nfrom each held-out fold are then calculated, and the resulting distributions of correlation coefﬁcients\nare shown in ﬁgure 2a. Figure 3a shows the distributions of correlations between similarities based\non audio and on the larger modality group. The correlations are evidently much smaller than for\nidentical models, but a Mantel test with 100 permutations suggest that the null hypothesis of no\ncorrelation can be rejected at a signiﬁcance level of at least 1% for all three model complexities.\n5 Discussion & Conclusion\nThe issue of stability is relevant for similarities induced by topic models using approximate inference\ntechniques. The correlations between similarities from identical but randomly initialized models,\n(a) Boxplots of Spearman’s correlation be-\ntween similarity matrices obtained using the\nsame parameters and data, but different ran-\ndom initialisation. Each row correspond to a\nmodality group.\n(b) Scatter plot of two speciﬁc similarities ob-\ntained by random initialisations of the same\nmodel estimated from the larger group of\nmodalities. The colors indicate within- and\nbetween-genre similarities.\nFigure 2\n3\n\n--- 第4页 ---\n(a) Boxplots of Spearman’s correlation be-\ntween similarity matrices obtained using the\nlarger modality group and the audio.\n(b) Scatter plot of the two examples of simi-\nlarities obtained from topic models with same\nparameters, but two mutually exclusive modal-\nities of data.\nFigure 3\ncan be used as a tool to gain some insight into this matter. From the preliminary results on the\nmusic example we ﬁnd the induced similarities (ﬁg. 2) to be highly stable. Furthermore, inspecting\nthe similarities obtained from different data types; ﬁgure 3, we observe that while the audio model\nin itself does not seem to provide higher intra- than inter-genre similarity, it is still signiﬁcantly\npositively correlated to the other modality group which does possess some discriminative power in\nterms of genre labels. Moreover, it seems that an increasing number of topics causes the correlation\nbetween similarities from models estimated on different modality groups to decrease. We speculate\nthat this is linked to the speciﬁc topic model variant, for which [ 5] also note that the model describes\nthe joint distribution of different modalities well, but does not model the relations between them.\nIn conclusion, we have proposed the multi-modal LDA as a method to deﬁne similarities in multimedia\napplications with multiple heterogeneous data sources based on the predictive-likelihood. This was\nextended with the Mantel test allowing direct evaluation of the consistency and correspondence of\nthe resulting similarities.\nAcknowledgment\nThis work was supported in part by the Danish Council for Strategic Research of the Danish Agency\nfor Science Technology and Innovation under the CoSound project, case number 11-115328. This\npublication only reﬂects the authors’ views.\nReferences\n[1]M.J. Salganik, P. Sheridan Dodds, and D.J. Watts. Experimental study of inequality and unpredictability in\nan artiﬁcial cultural market. science , 311(5762):854–856, 2006.\n[2]Kazuyoshi Yoshii, M. Goto, K. Komatani, R. Ogata, and H.G. Okuno. Hybrid collaborative and content-\nbased music recommendation using probabilistic model with latent user preferences. In Proceedings of the\n7th International Conference on Music Information Retrieval (ISMIR , pages 296–301, 2006.\n[3]Rainer Lienhart, Stefan Romberg, and Eva H ¨orster. Multilayer pLSA for multimodal image retrieval. In\nProceedings of the ACM International Conference on Image and Video Retrieval , pages 9:1–9:8, New\nYork, New York, USA, 2009. ACM Press.\n[4]M. Hoffman, D. Blei, and P Cook. Content-based musical similarity computation using the hierarchical\ndirichlet process. ISMIR 2008 - 9th International Conference on Music Information Retrieval , pages\n349–354, 2008.\n[5]D.M. Blei and M.I. Jordan. Modeling annotated data. Annual ACM Conference on Research and\nDevelopment in Information Retrieval , pages 127–134, 2003.\n[6]A.J.B. Chaney and D.M. Blei. Visualizing Topic Models. International AAAI Conference on Social Media\nand Weblogs , 2012.\n[7]E. H ¨orster, R. Lienhart, and M. Slaney. Image retrieval on large-scale image databases. In Proceedings of\nthe 6th ACM international conference on Image and video retrieval - CIVR ’07 , pages 17–24, New York,\nNew York, USA, 2007. ACM Press.\n4\n\n--- 第5页 ---\n[8]David Mimno, Hanna M. Wallach, Jason Naradowsky, David A. Smith, and Andrew McCallum. Polylingual\ntopic models. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language\nProcessing , volume 2, 2009.\n[9]L. Yao, D. Mimno, and A. McCallum. Efﬁcient methods for topic model inference on streaming document\ncollections. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery\nand data mining , KDD ’09, pages 937–946, New York, NY , USA, 2009. ACM.\n[10] T. P. Minka. Estimating a Dirichlet distribution. Annals of Physics , 2000(8):1–14, 2012.\n[11] H. M Wallach. Structured Topic Models for Language . PhD thesis, University of Cambridge, 2008.\n[12] N. Mantel. The detection of disease clustering and a generalized regression approach. Cancer Research ,\n27(2):209–220, 1967.\n[13] T. Bertin-Mahieux, D. P.W. Ellis, B. Whitman, and P. Lamere. The million song dataset. In the 12th\nInternational Society for Music Information Retrieval Conference (ISMIR 2011) , 2011.\n5",
      "download_time": "2025-12-14T21:51:09.414734",
      "word_count": 14728
    },
    {
      "index": 5,
      "title": "Evaluating topic coherence measures",
      "authors": [
        "Frank Rosner",
        "Alexander Hinneburg",
        "Michael Röder",
        "Martin Nettling",
        "Andreas Both"
      ],
      "published": "2014-03-25",
      "source": "arxiv",
      "url": "http://arxiv.org/abs/1403.6397v1",
      "pdf_url": "http://arxiv.org/pdf/1403.6397v1.pdf",
      "pdf_path": "papers\\05_Evaluating topic coherence measures.pdf",
      "txt_path": "papers\\05_Evaluating topic coherence measures.txt",
      "content": "--- 第1页 ---\nEvaluating topic coherence measures\nF. Rosner\u0003, A. Hinneburg\u0003\nMartin-Luther-University\nHalle-Wittenberg, GermanyM. R ¨oder\u0003\nLeipzig University and\nUnister GmbH, GermanyM. Nettling, A. Both\nUnister GmbH, Germany\nAbstract\nTopic models extract representative word sets—called topics—from word counts\nin documents without requiring any semantic annotations. Topics are not guaran-\nteed to be well interpretable, therefore, coherence measures have been proposed\nto distinguish between good and bad topics. Studies of topic coherence so far are\nlimited to measures that score pairs of individual words. For the ﬁrst time, we\ninclude coherence measures from scientiﬁc philosophy that score pairs of more\ncomplex word subsets and apply them to topic scoring.\n1 Introduction\nTopic model inference searches for a representation of word count distributions of documents as\ncombination of some topic distributions based on word counts. It is not considered how well derived\ntopics (i.e., the top words) can be interpreted by humans. Recently, coherence measures [7, 9]\nhave been proposed to distinguish between good and bad topics based on top words with respect to\ninterpretability. Topic models have been combined with coherence measures by introducing speciﬁc\npriors on topic distributions [7, 8].\nIt is interesting to note that all coherence measures evaluated so far take a set of words as input\nand compute a sum of scores over pairs of words from the input set [15]. This falls short for\nexamples likefbow, tie, match, deck gwhere the word pairs have semantic relations (e.g., fbow,\ndeckgas parts of a ship or fbow, tiegas terms of music or clothing) but the set as a whole is\nnot interpretable. However, a coherence measure based on word pairs would assign a good score.\nIn scientiﬁc philosophy measures have been proposed that compare pairs of more complex word\nsubsets instead of just word pairs. The main contribution of this paper is to compare coherence\nmeasures of different complexity with human ratings. Furthermore, we include in our study not\njust word sets generated from topics found by some topic model, but we examine word sets derived\nby direct optimization of a coherence measure. This tests whether a coherence measure speciﬁes a\nuseful optimization goal on its own terms.\nIn Section 2, we brieﬂy review coherence measures proposed in different scientiﬁc communities:\nNLP, computational linguistics and scientiﬁc philosophy. Section 3 shows the setup and results of\nour evaluation study and in Section 4 we discuss the results and conclude the paper.\n2 Coherence Measures\nCoherence measures have been proposed in the NLP community to evaluate topics constructed by\nsome topic model. In a more general setting, coherence measures have been discussed in scientiﬁc\nphilosophy as a formalism to quantify the hanging and ﬁtting together of information pieces [3].\nTopic coherence has been proposed as an intrinsic evaluation method for topic models [9, 10].\nIt is deﬁned as average or median of pairwise word similarities formed by top words of a given\ntopic. Word similarity is grounded on external data not used during topic modeling [8]. The UCI-\ncoherence uses point wise mutual information (PMI) and word cooccurrence counts collected from\n\u0003These authors contributed equally to this work.\nThis work has been presented at the Topic Models: Computation, Application and Evaluation workshop at the\nNeural Information Processing Systems conference 2013.\n1arXiv:1403.6397v1  [cs.LG]  25 Mar 2014\n\n--- 第2页 ---\nWikipedia based on a boolean window model1. This word similarity measure induces orderings\nfrom bad to good topics that come closest to human coherence judgements.\nIn [6] inferred posterior distributions of topics are visually analyzed how well they ﬁt the real ob-\nservations. However, no coherence measure is proposed to automattically judge interpretability of\nword sets. The coherence measure proposed in [7] is also based on cooccurrences of word pairs.\nGiven an ordered list of words T=hw1;:::;w nitheUMass-coherence is deﬁned as\nCUMass(T) =MX\nm=2m\u00001X\nl=1logp(wm;wl) +1\nD\np(wl)(1)\nA boolean document model is assumed to estimate word probabilities p, i.e.,p(wm;wl)is the ratio\nof number of documents containing both words wm;wland the total number of documents in the\ncorpusD. The smoothing count 1=Dis added to avoid calculating the logarithm of zero.\nFormalized coherence measures have been proposed in scientiﬁc philosophy [2, 14, 12, 13]. The\nframework proposed in [3] uniﬁes several concepts and introduces general qualitative and quanti-\ntative coherence measures. We describe the general framework in the context of word sets. Let\nW=fw1;:::;w ngbe a set of words and W0\u0012W. According to boolean document model, let\np(W0)be the ratio of the number of documents containing all words of W0divided by the number\nof documents D. The following three different qualitative coherence notions check whether cer-\ntain subsets of Wincrease the conditional probability of other subsets of W. This is formalized\nby deﬁning pairs of word subsets (W0;W\u0003)withW0;W\u0003\u0012Wand requiring that W\u0003supports\nW0, i.e.,p(W0jW\u0003)> p(W0)holds for all of the required pairs. One-all coherence requires that\neach wordwis supported by the complement Wnfwg. The more complex one-any coherence\nchecks that each word wis supported by all subsets W\u0003\u0012Wnfwg. The most restrictive any-\nany coherence requires that each possible, non-empty subset W0\u001aWis supported by all other,\nnon-overlapping subsets W\u0003\u0012WnW0. Formally, the sets of word set pairs are deﬁned as:\nSone-all(W) =\b\n(W0;W\u0003):W0=fwg;w2W;W\u0003=WnW0\t\nSone-any (W) =\b\n(W0;W\u0003):W0=fwg;w2W;W\u0003\u0012WnW0\t\nSany-any (W) =\b\n(W0;W\u0003):W0\u001aW;W\u0003\u0012WnW0\t\nQuantitative coherence measures Cd;x(W)are derived by averaging some conﬁrmation measure\nd(\u0001;\u0001), which quantiﬁes how strong W\u0003supportsW0, over all pairs of subsets Sx(W); x2\nfone-all;one-any;any-anyg, depending on the coherence type x. Following [3], we use differ-\nence measure (a.k.a. interest in association rule mining literature) as conﬁrmation measure. Given\ntwo non-overlapping subsets W0;W\u0003\u0012Wthis measure is deﬁned as:\nd(W0;W\u0003) =p(W0jW\u0003)\u0000p(W0)\nTo avoid artifacts, conditional probabilities are neglected for quantitative coherence, when they are\ncomputed on a very small subset of the corpus, i.e., the condition speciﬁes a subset of ten or less\ndocuments. Note that the framework is very ﬂexible. Both, probability estimation and conﬁrmation\nmeasure could be substituted by boolean window model or PMI as in [9] respectively.\nRun time complexities of all proposed coherence measures depend on the number of word set pairs.\nOne-all coherence is linear, UMass and UCI are quadratic and one-any as well as any-any coherence\nare exponential in size of word set W. Despite, the latter two coherences have exponential running\ntimes, we assume that their application in practice is possible. Many techniques from mining fre-\nquent item sets may be borrowed that exploit sparsity in text data. However, a detailed discussion is\nbeyond the scope of this paper.\n3 Evaluation\nWe evaluated coherence measures from Section 2 in three experiments on word sets generated from\nEnglish and German Wikipedia articles. The two corpora used consist of articles containing the\nterms “movie” and the German translation “ﬁlm” respectively to ensure that the human raters are\n1We removed stopwords and used a sliding window size of ten words.\n2\n\n--- 第3页 ---\nCoherenceGerman English\ngood (%) neutral (%) bad (%) good (%) neutral (%) bad (%)\nAny-any 66 27 7 52 41 7\nOne-any 63 30 7 59 35 6\nOne-all 44 36 20 59 31 10\nUMass 2 37 61 0 22 78\nRandom 0 12 88 0 0 100\nTable 1: Human ratings of word sets (Exp. I).\nCoherenceCoh. Word Sets (Exp. II) LDA Topics (Exp. III)\nEnglish German English German\nAny-any 0.557 0.568 0.239 0.379\nOne-any 0.592 0.583 0.242 0.376\nOne-all 0.561 0.578 0.215 0.337\nUMass 0.074 0.279 0.066 0.243\nUCI 0.224 0.380 0.219 0.371\nTable 2: Kendall’s tau rank correlations of coherences and average human ratings (Exp. II & III).\nfamliar with the subject. Preprocessing removed redirection and disambiguation pages, portal and\ncategory articles as well as articles about single years. Only nouns have been retained in lemma-\ntized form, except common ﬁrst names. Furthermore, frequent ( \u001560%) and rare nouns (\u00141%\nof documents) have been removed. The resulting English (German) corpus has 125.410 (71.134)\ndocuments, 21.370.741 (6.958.206) tokens and 2.888 (1.885) unique terms.2\nThe ﬁrst experiment evaluates whether a coherence measure speciﬁes a useful optimization goal on\nits own terms. The ability of the coherence measures to mimic human judgements is tested in the\nsecond experiment. The third experiment investigates the applicability of the coherences to topic\nmodeling.\nExperiment I . We generated word sets by directly optimizing coherence using heuristic beam search\n[11]. A beam search is initialized with a word set comprising of a single word. Then the algorithm\nevaluates all possible extensions by another word. It keeps the kword sets which have the largest\ncoherences. Those are recursively extended by the same principle until a predeﬁned word set length\nlis reached. Thus, kl\u00001word sets are generated for a given initial word. For both corpora, the 20\ntop TF-IDF terms have been selected as initial terms. Given some coherence measure and choosing\nbeamwidthk= 3and length of word sets l= 5,35\u00001= 81 word sets are generated for each initial\nword, thus, 81\u000320 = 1620 words sets in total. Except UCI coherence – which does not rely on\nthe given corpus – for every coherence we randomly sampled 100 of these word sets for human\nrating. Additionally we created 100 word sets randomly as baseline, thus, 500 word sets in total.\nEach word set was rated by at least three different human volunteers3regarding its interpretability as\neither good (all ﬁve words are related to each other), neutral (three or four words are related) or bad\n(at most two words are related). The kappa statistics [4] about the agreements among the volunteers\nare\u0014= 0:595and\u0014= 0:49for German and English data respectively. The smaller kappa for\nEnglish word sets might be due to the volunteers’ lesser faculty of speech in the foreign language\nthan in their mother tongue. Table 1 shows the percentage of word sets regarding the ratings of the\nmajority per coherence measure used for construction.\nExperiment II . For each of the 500 word sets of the ﬁrst experiment, all ﬁve coherence measures\nare derived. In Table 2, second and third columns show the rank correlations between the orderings\nof the topics by average score of human ratings and the respective coherence measure.\nExperiment III . For each corpus we generated 100 topics using Latent Dirichlet Allocation (LDA)\n[1]4. Each topic has been rated by human volunteers ( \u0014= 0:45for German and \u0014= 0:29for\n2corpora and results are available at http://topics.labs.bluekiwi.de/data/nips2013\n3All 19 volunteers that participated are German native speakers and ﬂuent in English.\n4We used Mallet[5] with \u000b= 0:5and\f= 0:01.\n3\n\n--- 第4页 ---\nEnglisch topics). In Table 2, columns four and ﬁve report the rank correlations between the orderings\nof the topics by average score of human ratings and the respective coherence measure.\n4 Discussion and Conclusion\nThe results of the ﬁrst experiment show that if we are using the one-any, any-any and one-all co-\nherences directly for optimization they are leading to meaningful word sets. The second experiment\nshows that these coherence measures are able to outperform the UCI coherence as well as the UMass\ncoherence on these generated word sets. For evaluating LDA topics any-any and one-any coherences\nperform slightly better than the UCI coherence. The correlation of the UMass coherence and the hu-\nman ratings is not as high as for the other coherences.\nOur results clearly show that comparing just word pairs via conformation measures can lead to poor\nperforming coherence measures. This indicates that evaluating word pairs is not enough to mimic\nhuman ratings.\nOur results might give rise to the development of new priors for topic models. However, directly\noptimizing coherence may lead to other meaningful word sets that maybe missed by topic models.\nTherefore, it is worth to explore multi-criteria optimization for learning topic models instead of\ncombining coherence measures as prior distributions with topic model inference. Additional future\nwork should focus on exploring the characteristics of the different coherences. This should include\nthe different conformation measures, the document model used and possible requirements which are\nimposed on the corpus by the coherences.\nAcknowledgments\nWe thank all volunteers who participated in the evaluation. Parts of\nthis work were supported by the ESF and the Free State of Saxony.\nReferences\n[1] D.M. Blei, A.Y . Ng, and M.I. Jordan. Latent dirichlet allocation. The Journal of Machine Learning\nResearch , 3:993–1022, 2003.\n[2] L. Bovens and S. Hartmann. Bayesian Epistemology . Oxford University Press, 2003.\n[3] I. Douven and W. Meijs. Measuring coherence. Synthese , 156:405–425, 2007.\n[4] J.R. Landis and G.G. Koch. The measurement of observer agreement for categorical data. biometrics ,\npages 159–174, 1977.\n[5] A. McCallum. Mallet: A machine learning for language toolkit. 2002.\n[6] D. Mimno and D. Blei. Bayesian checking for topic models. In Proceedings of the Conference on\nEmpirical Methods in Natural Language Processing , EMNLP 2011, pages 227–237.\n[7] D. Mimno, H.M. Wallach, E. Talley, M. Leenders, and A. McCallum. Optimizing semantic coherence in\ntopic models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing ,\nEMNLP 2011, pages 262–272.\n[8] D. Newman, E.V . Bonilla, and W. Buntine. Improving topic coherence with regularized topic models. In\nAdvances in Neural Information Processing Systems 24 , pages 496–504. 2011.\n[9] D. Newman, J.H. Lau, K. Grieser, and T. Baldwin. Automatic evaluation of topic coherence. In Human\nLanguage Technologies: The 2010 Annual Conference of the North American Chapter of the Association\nfor Computational Linguistics , HLT 2010, pages 100–108.\n[10] D. Newman, Y . Noh, E. Talley, S. Karimi, and T. Baldwin. Evaluating topic models for digital libraries.\nInProceedings of the 10th annual joint conference on Digital libraries , JCDL 2010, pages 215–224.\n[11] P. Norvig. Paradigms of artiﬁcial intelligence programming : case studies in Common LISP . Morgan\nKaufmann, 1992.\n[12] S. Schubert. Coherence reasoning and reliability: a defense of the shogenji measure. Synthese , 187:305–\n319, 2012.\n[13] S. Schubert and E.J. Olsson. On the coherence of higher-order beliefs. The Southern Journal of Philoso-\nphy, 50(1):112–135, 2012.\n[14] T. Shogenji. Coherence of the contents and the transmission of probabilistic support. Synthese , pages\n1–21, 2011.\n[15] K. Stevens, P. Kegelmeyer, D. Andrzejewski, and D. Buttler. Exploring topic coherence over many models\nand many topics. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language\nProcessing and Computational Natural Language Learning , EMNLP-CoNLL 2012, pages 952–961.\n4",
      "download_time": "2025-12-14T21:51:12.476737",
      "word_count": 15171
    }
  ],
  "paper_analysis": {
    "total_papers": 4,
    "total_words": 135970,
    "papers": [
      {
        "title": "Guest Editorial: Special Topic on Data-enabled Theoretical Chemistry",
        "authors": [
          "Matthias Rupp",
          "O. Anatole von Lilienfeld",
          "Kieron Burke"
        ],
        "key_concepts": [
          "machine learning",
          "deep learning",
          "neural network",
          "algorithm",
          "model",
          "training",
          "optimization",
          "classification",
          "regression",
          "clustering"
        ],
        "research_methods": [
          "experiment",
          "analysis",
          "evaluation",
          "comparison",
          "benchmark"
        ],
        "findings": [
          "1, we show papers being published involving\nmachine learning and chemistry or materials over the last\nthree decades",
          "Model building\nUnlike classical potentials, which are parametrized\nonce for a class of molecules or materials and then de-\nployed, ML models, being more \rexible mathematical\nfunctions, should be applied only to molecules or ma-\nterials sampled from the same distribution as the ones\nused to train the model|otherwise, the ML model will\noperate outside of its domain of applicability , result-\ning in uncontrolled and essentially arbitrary errors",
          "These essentially reuse the data\nby splitting it multiple times into a training and a hold-\nout set, then average over the results",
          "Note that often researchers use DFT (or even DFTB)\nresults for both training and testing their algorithms",
          "31introduce a ML\nmodel based on a solid harmonic wavelet scattering rep-\nresentation of organic molecules and demonstrate com-\npetitive performance for predicted atomization energies"
        ],
        "future_directions": [
          "II C\nEAM Embedded Atom Model/Method, an inter-\natomic potential\nGAP Gaussian Approximation Potential, a ma-\nchine learning potential\nHOMO Highest Occupied Molecular Orbital\nKRR Kernel Ridge regression, see Sec",
          "II D\nSINDy Sparse Identi\fcation of Nonlinear Dynamics,\na machine learning method\nSNAP Spectral Neighbor Analysis Potential, a ma-\nchine learning potential\nSVM Support Vector Machine, see Sec",
          "ML Method QM Method Systems Keywords\n30 III A NN DFT Hydrocarbon molecules Size-independence\n31 III A Multilinear regression DFT Small organic molecules Representation, wavelets\n32 III A KRR DFT Organic molecules, water, solids Representation, many-body terms\n33 III A NN DFT Small organic molecules NN architecture\n34 III A NN DFT Small organic molecules Representation, symmetry func-\ntions\n35 III A Regression DFT Small organic molecules Polynomial \ft, active learning\n36 III A KRR DFT Small organic molecules Graph-based representation\n76 III A NN DFT Organic molecules Covariant compositional networks\n37 III B KRR DFT, CCSD(T)Dimers, hydrogen-bonded\ncomplexes, and othersNon-covalent interactions\n38 III B GPR, NN DFTLiquid water, Al-Si-Mg alloy,\norganic moleculesFeature selection\n39 III B GPR DFT Li-C guest-host systems Combination of potentials\n40 III B NN DFT Small organic molecules Active learning\n41 III B NN DFT Small organic molecules Molecular properties\n42 III B DNN DFTOrganic molecules, bulk\ncrystals, C 20-fullereneDNN architecture\n43 III B GPR DFT, force \feld Na+, Cl\u0000ion-water clusters Ion-water interactions\n44 III BRegularized linear\nregressionDFT Tantalum Bispectrum quadratic terms\n45 III B GPR DFT Ni nanoclusters Interatomic forces, k-body kernels\n46 III B NN DFT Nicotine, water cluster Sampling, meta-dynamics\n47 III B NN DFT Cu surface grain boundaries Hybrid QM/ML models\n48 III C NN DFT Water/ZnO(10 \u001610) interface Anharmonic vibrational spectra\n49 III C Linear regression CCSD(T) Formic acid dimer Dipole moment surface, infrared\nspectrum\n50 III C NN, GPR CCSD(T) Water (ice, liquid, clusters) Representation, invariant polyno-\nmials\n51 III C NN, GPR Force \feld Formaldehyde Comparison, vibrational spectra\n52 III D NN, genetic algorithm DFT Li xSi alloysPhase diagrams of amorphous\nmaterials\n53 III D Regression trees DFT AB 2C2ternary intermetallics Stable compound search\n54 III D Clustering Harris approximation Rigid-molecule crystals Crystal structure prediction\n55 III D Monte Carlo tree search EAM Ag, Co grain boundaries Segregation\n56 III D Binary classi\fcation trees DFT Inorganic crystals Recommender system\n57 III DMonte Carlo tree search,\nGPRDFT Boron-doped graphene Stable structure search\n58 III ESubset selection, outlier\ndetectionDFT Main group chemistry Doubly hybrid functional\n60 III E NN DFT Model systemsHartree-exchange-correlation\npotential\n62 III E KRR DFT Organic molecules Representation\n63 III E KRR DFT Model systems Exact conditions\n64 III E NN DFT Atoms and molecules Kinetic energy density functional\n65 III F Sparse regression Analytic potential Model systems Stochastic dynamical equations\n66 III F Time-lagged autoencoder Force \feld Model systems, villin peptideSlow dynamics, dimensionality\nreduction\n67 III F Markov state model,tICA Force \feld Dye-labeled polyproline-20 Dynamics, transition probabilities\n68 III G None DFT Various (G3/99 test set) Error statistics\n69 III G Autoencoder, NN DFT Donor-acceptor polymers Screening, solar cells\n70 III G SVM DFT Organic polymers Refraction index\n71 III G KRR DFTPerovskite oxides, elpasolite\nhalidesLanthanide-doped scintillators\n72 III G GPR CCSD(T) Small organic molecules Geometry optimization\n73 III G Clustering DFTB Anatase TiO 2(001) Global structure optimization\n74 III G SVM, graph analysis Force \feld Tyrosine phosphatase 1E Proteins, dynamic allostery\n75 III G Data analysis Force \feld Antimicrobial peptides Visualization\n\n--- 第3页 ---\n3\nIn Fig"
        ],
        "summary": "--- 第1页 ---\nGuest Editorial: Special Topic on Data-enabled Theoretical Chemistry\nMatthias Rupp,1,a)O. Anatole von Lilienfeld,2,b)and Kieron Burke3,c)\n1)Fritz Haber Institute of the Max Planck Society, Faradayweg 4{6, 14195 Berlin, Germany\n2)Department of Chemistry, Institute of Physical Chemistry and National Center for Computational\nDesign and Discovery of Novel Materials, University of Basel, 4056 Basel, Switzerland\n3)Departments of Chemistry and of Physics, University of California, Irvine, CA 92697, USA\n(Published in The Journal of Chemical Physics 148(24): 241401, 2018. 5043213)\nA survey of the contributions to the Special Topic on Data-enabled Theoretical Chemistry is given, including\na glossary of relevant machine learning terms."
      },
      {
        "title": "Topic Modelling Meets Deep Neural Networks: A Survey",
        "authors": [
          "He Zhao",
          "Dinh Phung",
          "Viet Huynh",
          "Yuan Jin",
          "Lan Du",
          "Wray Buntine"
        ],
        "key_concepts": [
          "machine learning",
          "deep learning",
          "neural network",
          "algorithm",
          "model",
          "training",
          "clustering",
          "supervised learning",
          "reinforcement learning"
        ],
        "research_methods": [
          "experiment",
          "analysis",
          "evaluation",
          "comparison",
          "benchmark"
        ],
        "findings": [
          "Despite their success, conventional BPTMs started to show\nsigns of fatigue in the era of big data and deep learning: 1)\nGiven a speciﬁc BPTM, its inference process usually needs\nto be customised accordingly and the inference complexity\nmay grow signiﬁcantly as the model complexity grows",
          "For NTMs, the computation of\nlog-likelihood is even more inconsistent, making it harder to\ncompare the results across different papers",
          "Topic Coherence Experiments show topic coherence (TC)\ncomputed with the coherence between a topic’s most rep-\nresentative words (e",
          "For example, computing TC for\na machine learning paper collection with a tweet dataset as\nreference may generate inaccurate results",
          "Figure 1 shows the taxonomy of V AE-NTMs"
        ],
        "future_directions": [
          "There is a need to summarise research develop-\nments and discuss open problems and future direc-\ntions",
          "With the recent developments in DNNs and deep genera-\ntive models, there has been an emerging research direction\nwhich aims to leverage DNNs to boost performance, efﬁ-\nciency, and usability of topic modelling, named neural topic\nmodels (NTMs)",
          "Therefore, it is important to properly summarise research d e-\nvelopments, categorise existing approaches, identify rem ain-\ning issues, and discuss open problems and future directions"
        ],
        "summary": "--- 第1页 ---\narXiv:2103. 00498v1  [cs. LG]  28 Feb 2021Topic Modelling Meets Deep Neural Networks: A Survey\nHe Zhao ,Dinh Phung ,Viet Huynh ,Yuan Jin ,Lan Du ,Wray Buntine\nDepartment of Data Science and Artiﬁcial Intelligence, Mon ash University, Australia\n{ethan. phung, viet."
      },
      {
        "title": "A Topic Model Approach to Multi-Modal Similarity",
        "authors": [
          "Rasmus Troelsgård",
          "Bjørn Sand Jensen",
          "Lars Kai Hansen"
        ],
        "key_concepts": [
          "model",
          "training",
          "regression",
          "clustering"
        ],
        "research_methods": [
          "experiment",
          "evaluation",
          "comparison"
        ],
        "findings": [
          "We propose to compare the resulting similarities from different model realizations\nusing the non-parametric Mantel test",
          "In fusing heterogeneous\nmodalities like audio, genre, and user generated tags it is both a challenge to establish a combined\nmodel in a ’symmetric’ manner so that one modality do not dominate others and it is challenging to\nevaluate the quality of the resulting geometric representation",
          "4 Experimental Results: Music Similarity\nIn this preliminary study we examine induced similarities in a subset of the Million Song Dataset [ 13],\nconsisting of 30",
          "The correlations between all combinations of the 5 similarity matrices resulting\nfrom each held-out fold are then calculated, and the resulting distributions of correlation coefﬁcients\nare shown in ﬁgure 2a",
          "Figure 3a shows the distributions of correlations between similarities based\non audio and on the larger modality group"
        ],
        "future_directions": [
          "The difference from a number of individual\nLDA models, each deﬁned on a separate modality, is that each object is described by a single,\nshared distribution over topics, which potentially induces strong dependencies between the feature\ndistributions representing the same topic in the individual modalities"
        ],
        "summary": "--- 第1页 ---\nA Topic Model Approach to Multi-Modal Similarity\nRasmus Troelsg ˚ard, Bjørn Sand Jensen and Lars Kai Hansen\nDepartment of Applied Mathematics and Computer Science\nTechnical University of Denmark\nMatematiktorvet 303B, 2800 Kgs. Lyngby\nfrast,bjje,lkai g@dtu. dk\nAbstract\nCalculating similarities between objects deﬁned by many heterogeneous data\nmodalities is an important challenge in many multimedia applications. We use a\nmulti-modal topic model as a basis for deﬁning such a similarity between objects. We propose to compare the resulting similarities from different model realizations\nusing the non-parametric Mantel test."
      },
      {
        "title": "Evaluating topic coherence measures",
        "authors": [
          "Frank Rosner",
          "Alexander Hinneburg",
          "Michael Röder",
          "Martin Nettling",
          "Andreas Both"
        ],
        "key_concepts": [
          "machine learning",
          "algorithm",
          "model",
          "optimization"
        ],
        "research_methods": [
          "experiment",
          "evaluation",
          "review"
        ],
        "findings": [
          "Section 3 shows the setup and results of\nour evaluation study and in Section 4 we discuss the results and conclude the paper",
          "The resulting English (German) corpus has 125",
          "Table 1 shows the percentage of word sets regarding the ratings of the\nmajority per coherence measure used for construction",
          "In Table 2, second and third columns show the rank correlations between the orderings\nof the topics by average score of human ratings and the respective coherence measure",
          "Each topic has been rated by human volunteers ( \u0014= 0:45for German and \u0014= 0:29for\n2corpora and results are available at http://topics"
        ],
        "future_directions": [
          "Preprocessing removed redirection and disambiguation pages, portal and\ncategory articles as well as articles about single years",
          "Additional future\nwork should focus on exploring the characteristics of the different coherences"
        ],
        "summary": "--- 第1页 ---\nEvaluating topic coherence measures\nF. Hinneburg\u0003\nMartin-Luther-University\nHalle-Wittenberg, GermanyM. R ¨oder\u0003\nLeipzig University and\nUnister GmbH, GermanyM. Nettling, A."
      }
    ],
    "key_concepts": [
      "deep learning",
      "neural network",
      "model",
      "algorithm",
      "machine learning",
      "clustering",
      "optimization",
      "classification",
      "regression",
      "reinforcement learning",
      "supervised learning",
      "training"
    ],
    "research_methods": [
      "benchmark",
      "analysis",
      "evaluation",
      "comparison",
      "experiment",
      "review"
    ],
    "findings": [
      "1, we show papers being published involving\nmachine learning and chemistry or materials over the last\nthree decades",
      "Despite their success, conventional BPTMs started to show\nsigns of fatigue in the era of big data and deep learning: 1)\nGiven a speciﬁc BPTM, its inference process usually needs\nto be customised accordingly and the inference complexity\nmay grow signiﬁcantly as the model complexity grows",
      "We propose to compare the resulting similarities from different model realizations\nusing the non-parametric Mantel test",
      "Topic Coherence Experiments show topic coherence (TC)\ncomputed with the coherence between a topic’s most rep-\nresentative words (e",
      "Each topic has been rated by human volunteers ( \u0014= 0:45for German and \u0014= 0:29for\n2corpora and results are available at http://topics",
      "Note that often researchers use DFT (or even DFTB)\nresults for both training and testing their algorithms",
      "For NTMs, the computation of\nlog-likelihood is even more inconsistent, making it harder to\ncompare the results across different papers",
      "These essentially reuse the data\nby splitting it multiple times into a training and a hold-\nout set, then average over the results",
      "Table 1 shows the percentage of word sets regarding the ratings of the\nmajority per coherence measure used for construction",
      "In Table 2, second and third columns show the rank correlations between the orderings\nof the topics by average score of human ratings and the respective coherence measure",
      "Figure 3a shows the distributions of correlations between similarities based\non audio and on the larger modality group",
      "In fusing heterogeneous\nmodalities like audio, genre, and user generated tags it is both a challenge to establish a combined\nmodel in a ’symmetric’ manner so that one modality do not dominate others and it is challenging to\nevaluate the quality of the resulting geometric representation",
      "Model building\nUnlike classical potentials, which are parametrized\nonce for a class of molecules or materials and then de-\nployed, ML models, being more \rexible mathematical\nfunctions, should be applied only to molecules or ma-\nterials sampled from the same distribution as the ones\nused to train the model|otherwise, the ML model will\noperate outside of its domain of applicability , result-\ning in uncontrolled and essentially arbitrary errors",
      "Section 3 shows the setup and results of\nour evaluation study and in Section 4 we discuss the results and conclude the paper",
      "The correlations between all combinations of the 5 similarity matrices resulting\nfrom each held-out fold are then calculated, and the resulting distributions of correlation coefﬁcients\nare shown in ﬁgure 2a",
      "31introduce a ML\nmodel based on a solid harmonic wavelet scattering rep-\nresentation of organic molecules and demonstrate com-\npetitive performance for predicted atomization energies",
      "4 Experimental Results: Music Similarity\nIn this preliminary study we examine induced similarities in a subset of the Million Song Dataset [ 13],\nconsisting of 30",
      "The resulting English (German) corpus has 125",
      "For example, computing TC for\na machine learning paper collection with a tweet dataset as\nreference may generate inaccurate results",
      "Figure 1 shows the taxonomy of V AE-NTMs"
    ],
    "future_directions": [
      "II D\nSINDy Sparse Identi\fcation of Nonlinear Dynamics,\na machine learning method\nSNAP Spectral Neighbor Analysis Potential, a ma-\nchine learning potential\nSVM Support Vector Machine, see Sec",
      "The difference from a number of individual\nLDA models, each deﬁned on a separate modality, is that each object is described by a single,\nshared distribution over topics, which potentially induces strong dependencies between the feature\ndistributions representing the same topic in the individual modalities",
      "Therefore, it is important to properly summarise research d e-\nvelopments, categorise existing approaches, identify rem ain-\ning issues, and discuss open problems and future directions",
      "With the recent developments in DNNs and deep genera-\ntive models, there has been an emerging research direction\nwhich aims to leverage DNNs to boost performance, efﬁ-\nciency, and usability of topic modelling, named neural topic\nmodels (NTMs)",
      "ML Method QM Method Systems Keywords\n30 III A NN DFT Hydrocarbon molecules Size-independence\n31 III A Multilinear regression DFT Small organic molecules Representation, wavelets\n32 III A KRR DFT Organic molecules, water, solids Representation, many-body terms\n33 III A NN DFT Small organic molecules NN architecture\n34 III A NN DFT Small organic molecules Representation, symmetry func-\ntions\n35 III A Regression DFT Small organic molecules Polynomial \ft, active learning\n36 III A KRR DFT Small organic molecules Graph-based representation\n76 III A NN DFT Organic molecules Covariant compositional networks\n37 III B KRR DFT, CCSD(T)Dimers, hydrogen-bonded\ncomplexes, and othersNon-covalent interactions\n38 III B GPR, NN DFTLiquid water, Al-Si-Mg alloy,\norganic moleculesFeature selection\n39 III B GPR DFT Li-C guest-host systems Combination of potentials\n40 III B NN DFT Small organic molecules Active learning\n41 III B NN DFT Small organic molecules Molecular properties\n42 III B DNN DFTOrganic molecules, bulk\ncrystals, C 20-fullereneDNN architecture\n43 III B GPR DFT, force \feld Na+, Cl\u0000ion-water clusters Ion-water interactions\n44 III BRegularized linear\nregressionDFT Tantalum Bispectrum quadratic terms\n45 III B GPR DFT Ni nanoclusters Interatomic forces, k-body kernels\n46 III B NN DFT Nicotine, water cluster Sampling, meta-dynamics\n47 III B NN DFT Cu surface grain boundaries Hybrid QM/ML models\n48 III C NN DFT Water/ZnO(10 \u001610) interface Anharmonic vibrational spectra\n49 III C Linear regression CCSD(T) Formic acid dimer Dipole moment surface, infrared\nspectrum\n50 III C NN, GPR CCSD(T) Water (ice, liquid, clusters) Representation, invariant polyno-\nmials\n51 III C NN, GPR Force \feld Formaldehyde Comparison, vibrational spectra\n52 III D NN, genetic algorithm DFT Li xSi alloysPhase diagrams of amorphous\nmaterials\n53 III D Regression trees DFT AB 2C2ternary intermetallics Stable compound search\n54 III D Clustering Harris approximation Rigid-molecule crystals Crystal structure prediction\n55 III D Monte Carlo tree search EAM Ag, Co grain boundaries Segregation\n56 III D Binary classi\fcation trees DFT Inorganic crystals Recommender system\n57 III DMonte Carlo tree search,\nGPRDFT Boron-doped graphene Stable structure search\n58 III ESubset selection, outlier\ndetectionDFT Main group chemistry Doubly hybrid functional\n60 III E NN DFT Model systemsHartree-exchange-correlation\npotential\n62 III E KRR DFT Organic molecules Representation\n63 III E KRR DFT Model systems Exact conditions\n64 III E NN DFT Atoms and molecules Kinetic energy density functional\n65 III F Sparse regression Analytic potential Model systems Stochastic dynamical equations\n66 III F Time-lagged autoencoder Force \feld Model systems, villin peptideSlow dynamics, dimensionality\nreduction\n67 III F Markov state model,tICA Force \feld Dye-labeled polyproline-20 Dynamics, transition probabilities\n68 III G None DFT Various (G3/99 test set) Error statistics\n69 III G Autoencoder, NN DFT Donor-acceptor polymers Screening, solar cells\n70 III G SVM DFT Organic polymers Refraction index\n71 III G KRR DFTPerovskite oxides, elpasolite\nhalidesLanthanide-doped scintillators\n72 III G GPR CCSD(T) Small organic molecules Geometry optimization\n73 III G Clustering DFTB Anatase TiO 2(001) Global structure optimization\n74 III G SVM, graph analysis Force \feld Tyrosine phosphatase 1E Proteins, dynamic allostery\n75 III G Data analysis Force \feld Antimicrobial peptides Visualization\n\n--- 第3页 ---\n3\nIn Fig",
      "II C\nEAM Embedded Atom Model/Method, an inter-\natomic potential\nGAP Gaussian Approximation Potential, a ma-\nchine learning potential\nHOMO Highest Occupied Molecular Orbital\nKRR Kernel Ridge regression, see Sec",
      "There is a need to summarise research develop-\nments and discuss open problems and future direc-\ntions",
      "Additional future\nwork should focus on exploring the characteristics of the different coherences",
      "Preprocessing removed redirection and disambiguation pages, portal and\ncategory articles as well as articles about single years"
    ]
  },
  "collaborative_content": {},
  "execution_end": "2025-12-14T21:51:12.507643",
  "total_duration": 6.058538
}