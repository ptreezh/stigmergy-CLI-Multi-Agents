#!/usr/bin/env python3
"""
è®ºæ–‡ä¸‹è½½å™¨ - ä¸‹è½½å­¦æœ¯è®ºæ–‡åˆ°æœ¬åœ°å¹¶è§£æå†…å®¹
"""

import os
import requests
import json
from typing import List, Dict, Any
from datetime import datetime
import PyPDF2
from io import BytesIO

class PaperDownloader:
    """è®ºæ–‡ä¸‹è½½å™¨"""
    
    def __init__(self, download_dir: str = "papers"):
        self.download_dir = download_dir
        os.makedirs(download_dir, exist_ok=True)
    
    def download_papers(self, search_results: List[Dict[str, Any]], topic: str) -> List[Dict[str, Any]]:
        """ä¸‹è½½è®ºæ–‡åˆ°æœ¬åœ°"""
        print(f"      ğŸ“¥ å¼€å§‹ä¸‹è½½è®ºæ–‡åˆ°æœ¬åœ°...")
        
        downloaded_papers = []
        
        for i, result in enumerate(search_results[:5], 1):  # ä¸‹è½½å‰5ç¯‡
            paper_info = self._download_single_paper(result, topic, i)
            if paper_info:
                downloaded_papers.append(paper_info)
                print(f"         âœ“ ä¸‹è½½å®Œæˆ: {paper_info['title']}")
        
        print(f"      ğŸ“Š æˆåŠŸä¸‹è½½ {len(downloaded_papers)} ç¯‡è®ºæ–‡")
        return downloaded_papers
    
    def _download_single_paper(self, result: Dict[str, Any], topic: str, index: int) -> Dict[str, Any]:
        """ä¸‹è½½å•ç¯‡è®ºæ–‡"""
        try:
            title = result.get('title', f'paper_{index}')
            url = result.get('url', '')
            
            if not url:
                return None
            
            # æ¸…ç†æ ‡é¢˜ä½œä¸ºæ–‡ä»¶å
            safe_title = self._sanitize_filename(title)
            
            # å°è¯•ä¸‹è½½PDF
            pdf_url = self._get_pdf_url(url)
            if not pdf_url:
                print(f"         âš ï¸ æœªæ‰¾åˆ°PDFé“¾æ¥: {title}")
                return None
            
            # ä¸‹è½½PDFæ–‡ä»¶
            response = requests.get(pdf_url, timeout=30)
            response.raise_for_status()
            
            # ä¿å­˜PDFæ–‡ä»¶
            pdf_filename = f"{index:02d}_{safe_title}.pdf"
            pdf_path = os.path.join(self.download_dir, pdf_filename)
            
            with open(pdf_path, 'wb') as f:
                f.write(response.content)
            
            # è§£æPDFå†…å®¹
            content = self._extract_pdf_content(response.content)
            
            # ä¿å­˜è§£æçš„æ–‡æœ¬
            txt_filename = f"{index:02d}_{safe_title}.txt"
            txt_path = os.path.join(self.download_dir, txt_filename)
            
            with open(txt_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            paper_info = {
                'index': index,
                'title': title,
                'authors': result.get('authors', []),
                'published': result.get('published', ''),
                'source': result.get('source', ''),
                'url': url,
                'pdf_url': pdf_url,
                'pdf_path': pdf_path,
                'txt_path': txt_path,
                'content': content,
                'download_time': datetime.now().isoformat(),
                'word_count': len(content)
            }
            
            return paper_info
            
        except Exception as e:
            print(f"         âŒ ä¸‹è½½å¤±è´¥: {title} - {e}")
            return None
    
    def _get_pdf_url(self, arxiv_url: str) -> str:
        """è·å–PDFä¸‹è½½é“¾æ¥"""
        if 'arxiv.org/abs/' in arxiv_url:
            # å°† abs/ æ›¿æ¢ä¸º pdf/
            pdf_url = arxiv_url.replace('/abs/', '/pdf/') + '.pdf'
            return pdf_url
        return None
    
    def _extract_pdf_content(self, pdf_data: bytes) -> str:
        """ä»PDFä¸­æå–æ–‡æœ¬å†…å®¹"""
        try:
            pdf_file = BytesIO(pdf_data)
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            
            content = ""
            for page_num, page in enumerate(pdf_reader.pages):
                try:
                    page_text = page.extract_text()
                    if page_text.strip():
                        content += f"\n--- ç¬¬{page_num + 1}é¡µ ---\n"
                        content += page_text + "\n"
                except Exception as e:
                    print(f"         âš ï¸ é¡µé¢ {page_num + 1} è§£æå¤±è´¥: {e}")
                    continue
            
            return content.strip()
            
        except Exception as e:
            print(f"         âŒ PDFè§£æå¤±è´¥: {e}")
            return ""
    
    def _sanitize_filename(self, filename: str) -> str:
        """æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦"""
        # ç§»é™¤æˆ–æ›¿æ¢éæ³•å­—ç¬¦
        illegal_chars = ['<', '>', ':', '"', '/', '\\', '|', '?', '*']
        for char in illegal_chars:
            filename = filename.replace(char, '_')
        
        # é™åˆ¶é•¿åº¦
        if len(filename) > 100:
            filename = filename[:100]
        
        return filename.strip()
    
    def analyze_downloaded_papers(self, downloaded_papers: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æä¸‹è½½çš„è®ºæ–‡"""
        print(f"      ğŸ§  å¼€å§‹å­¦ä¹ å’Œæ¶ˆåŒ–è®ºæ–‡å†…å®¹...")
        
        analysis = {
            'total_papers': len(downloaded_papers),
            'total_words': sum(p['word_count'] for p in downloaded_papers),
            'papers': [],
            'key_concepts': [],
            'research_methods': [],
            'findings': [],
            'future_directions': []
        }
        
        for paper in downloaded_papers:
            paper_analysis = self._analyze_single_paper(paper)
            analysis['papers'].append(paper_analysis)
            
            # æå–å…³é”®æ¦‚å¿µ
            analysis['key_concepts'].extend(paper_analysis['key_concepts'])
            analysis['research_methods'].extend(paper_analysis['research_methods'])
            analysis['findings'].extend(paper_analysis['findings'])
            analysis['future_directions'].extend(paper_analysis['future_directions'])
        
        # å»é‡å’Œæ•´ç†
        analysis['key_concepts'] = list(set(analysis['key_concepts']))
        analysis['research_methods'] = list(set(analysis['research_methods']))
        analysis['findings'] = list(set(analysis['findings']))
        analysis['future_directions'] = list(set(analysis['future_directions']))
        
        print(f"      ğŸ“š å­¦ä¹ å®Œæˆï¼Œæå–äº† {len(analysis['key_concepts'])} ä¸ªå…³é”®æ¦‚å¿µ")
        
        return analysis
    
    def _analyze_single_paper(self, paper: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå•ç¯‡è®ºæ–‡"""
        content = paper.get('content', '')
        
        # ç®€å•çš„å†…å®¹åˆ†æï¼ˆå®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„NLPæŠ€æœ¯ï¼‰
        analysis = {
            'title': paper['title'],
            'authors': paper['authors'],
            'key_concepts': self._extract_key_concepts(content),
            'research_methods': self._extract_research_methods(content),
            'findings': self._extract_findings(content),
            'future_directions': self._extract_future_directions(content),
            'summary': self._generate_summary(content)
        }
        
        return analysis
    
    def _extract_key_concepts(self, content: str) -> List[str]:
        """æå–å…³é”®æ¦‚å¿µ"""
        # ç®€å•çš„å…³é”®è¯æå–ï¼ˆåŸºäºå¸¸è§å­¦æœ¯æœ¯è¯­ï¼‰
        common_concepts = [
            'machine learning', 'deep learning', 'neural network', 'algorithm',
            'model', 'training', 'optimization', 'classification', 'regression',
            'clustering', 'feature extraction', 'data analysis', 'prediction',
            'supervised learning', 'unsupervised learning', 'reinforcement learning'
        ]
        
        concepts = []
        content_lower = content.lower()
        
        for concept in common_concepts:
            if concept in content_lower:
                concepts.append(concept)
        
        return concepts[:10]  # è¿”å›å‰10ä¸ª
    
    def _extract_research_methods(self, content: str) -> List[str]:
        """æå–ç ”ç©¶æ–¹æ³•"""
        method_keywords = [
            'experiment', 'analysis', 'evaluation', 'comparison', 'benchmark',
            'simulation', 'case study', 'survey', 'review', 'empirical study',
            'theoretical analysis', 'statistical analysis', 'quantitative analysis'
        ]
        
        methods = []
        content_lower = content.lower()
        
        for method in method_keywords:
            if method in content_lower:
                methods.append(method)
        
        return methods[:5]
    
    def _extract_findings(self, content: str) -> List[str]:
        """æå–ç ”ç©¶å‘ç°"""
        findings = []
        
        # æŸ¥æ‰¾åŒ…å«"find", "result", "conclusion"ç­‰å…³é”®è¯çš„å¥å­
        sentences = content.split('.')
        for sentence in sentences:
            sentence_lower = sentence.lower()
            if any(keyword in sentence_lower for keyword in ['find', 'result', 'conclusion', 'show', 'demonstrate']):
                if len(sentence.strip()) > 20:  # è¿‡æ»¤çŸ­å¥å­
                    findings.append(sentence.strip())
        
        return findings[:5]
    
    def _extract_future_directions(self, content: str) -> List[str]:
        """æå–æœªæ¥æ–¹å‘"""
        directions = []
        
        # æŸ¥æ‰¾åŒ…å«"future", "direction", "next", "potential"ç­‰å…³é”®è¯çš„å¥å­
        sentences = content.split('.')
        for sentence in sentences:
            sentence_lower = sentence.lower()
            if any(keyword in sentence_lower for keyword in ['future', 'direction', 'next', 'potential', 'opportunity']):
                if len(sentence.strip()) > 20:
                    directions.append(sentence.strip())
        
        return directions[:3]
    
    def _generate_summary(self, content: str) -> str:
        """ç”Ÿæˆè®ºæ–‡æ‘˜è¦"""
        # ç®€å•çš„æ‘˜è¦ç”Ÿæˆï¼ˆå–å‰å‡ å¥è¯ï¼‰
        sentences = content.split('.')
        summary_sentences = []
        
        for sentence in sentences[:5]:  # å–å‰5å¥è¯
            sentence = sentence.strip()
            if len(sentence) > 10:
                summary_sentences.append(sentence)
        
        return '. '.join(summary_sentences) + '.' if summary_sentences else "æ‘˜è¦ç”Ÿæˆå¤±è´¥"

def main():
    """æµ‹è¯•å‡½æ•°"""
    # æµ‹è¯•æ•°æ®
    test_results = [
        {
            'title': 'Test Paper on Machine Learning',
            'url': 'http://arxiv.org/abs/1806.02690v2',
            'authors': ['Test Author'],
            'published': '2023-01-01'
        }
    ]
    
    downloader = PaperDownloader()
    papers = downloader.download_papers(test_results, "machine learning")
    
    if papers:
        analysis = downloader.analyze_downloaded_papers(papers)
        print(f"åˆ†æç»“æœ: {json.dumps(analysis, indent=2, ensure_ascii=False)}")

if __name__ == "__main__":
    main()