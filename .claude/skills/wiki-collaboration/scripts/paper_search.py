#!/usr/bin/env python3
"""
è®ºæ–‡æœç´¢å’Œä¸‹è½½ç³»ç»Ÿ - ä¸ºWikiåˆ›å»ºæä¾›å­¦æœ¯èµ„æºæ”¯æŒ
"""

import requests
import json
import re
import time
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import os
from urllib.parse import quote

@dataclass
class Paper:
    """è®ºæ–‡ä¿¡æ¯"""
    title: str
    authors: List[str]
    abstract: str
    year: int
    venue: str
    url: str
    doi: Optional[str] = None
    citations: int = 0
    pdf_url: Optional[str] = None
    keywords: List[str] = None
    
    def __post_init__(self):
        if self.keywords is None:
            self.keywords = []

class AcademicSearchEngine:
    """å­¦æœ¯æœç´¢å¼•æ“"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.arxiv_base_url = "http://export.arxiv.org/api/query"
        self.semantic_scholar_url = "https://api.semanticscholar.org/graph/v1/paper/search"
    
    def search_papers(self, query: str, max_results: int = 10, 
                     year_range: Optional[Tuple[int, int]] = None) -> List[Paper]:
        """æœç´¢è®ºæ–‡"""
        print(f"ğŸ” æ­£åœ¨æœç´¢è®ºæ–‡: {query}")
        
        all_papers = []
        
        # ä»arXivæœç´¢
        arxiv_papers = self._search_arxiv(query, max_results // 2, year_range)
        all_papers.extend(arxiv_papers)
        
        # ä»Semantic Scholaræœç´¢
        ss_papers = self._search_semantic_scholar(query, max_results - len(arxiv_papers), year_range)
        all_papers.extend(ss_papers)
        
        # å»é‡å¹¶æ’åº
        unique_papers = self._deduplicate_papers(all_papers)
        sorted_papers = sorted(unique_papers, key=lambda p: p.citations, reverse=True)
        
        print(f"âœ… æ‰¾åˆ° {len(sorted_papers)} ç¯‡ç›¸å…³è®ºæ–‡")
        return sorted_papers[:max_results]
    
    def _search_arxiv(self, query: str, max_results: int, 
                      year_range: Optional[Tuple[int, int]]) -> List[Paper]:
        """ä»arXivæœç´¢"""
        try:
            params = {
                'search_query': f'all:{query}',
                'start': 0,
                'max_results': max_results,
                'sortBy': 'relevance',
                'sortOrder': 'descending'
            }
            
            response = self.session.get(self.arxiv_base_url, params=params, timeout=10)
            response.raise_for_status()
            
            papers = self._parse_arxiv_response(response.text)
            
            # å¹´ä»½è¿‡æ»¤
            if year_range:
                papers = [p for p in papers if year_range[0] <= p.year <= year_range[1]]
            
            return papers
            
        except Exception as e:
            print(f"âš ï¸ arXivæœç´¢å¤±è´¥: {e}")
            return []
    
    def _parse_arxiv_response(self, xml_text: str) -> List[Paper]:
        """è§£æarXivå“åº”"""
        import xml.etree.ElementTree as ET
        
        papers = []
        root = ET.fromstring(xml_text)
        
        # å®šä¹‰å‘½åç©ºé—´
        namespaces = {
            'atom': 'http://www.w3.org/2005/Atom',
            'arxiv': 'http://arxiv.org/schemas/atom'
        }
        
        for entry in root.findall('atom:entry', namespaces):
            try:
                title = entry.find('atom:title', namespaces).text.strip()
                
                # æå–ä½œè€…
                authors = []
                for author in entry.findall('atom:author', namespaces):
                    name = author.find('atom:name', namespaces).text
                    authors.append(name)
                
                # æå–æ‘˜è¦
                abstract = entry.find('atom:summary', namespaces).text.strip()
                
                # æå–å¹´ä»½
                published = entry.find('atom:published', namespaces).text
                year = int(published.split('-')[0])
                
                # æå–é“¾æ¥
                url = entry.find('atom:id', namespaces).text
                
                # æå–PDFé“¾æ¥
                pdf_link = entry.find('atom:link[@title="pdf"]', namespaces)
                pdf_url = pdf_link.get('href') if pdf_link is not None else None
                
                # æå–arXiv IDä½œä¸ºDOI
                arxiv_id = url.split('/')[-1]
                doi = f"arXiv:{arxiv_id}"
                
                # æå–ç±»åˆ«ä½œä¸ºå…³é”®è¯
                categories = []
                for category in entry.findall('arxiv:primary_category', namespaces):
                    categories.append(category.get('term'))
                
                paper = Paper(
                    title=title,
                    authors=authors,
                    abstract=abstract,
                    year=year,
                    venue="arXiv",
                    url=url,
                    doi=doi,
                    pdf_url=pdf_url,
                    keywords=categories
                )
                
                papers.append(paper)
                
            except Exception as e:
                print(f"âš ï¸ è§£æè®ºæ–‡å¤±è´¥: {e}")
                continue
        
        return papers
    
    def _search_semantic_scholar(self, query: str, max_results: int, 
                                year_range: Optional[Tuple[int, int]]) -> List[Paper]:
        """ä»Semantic Scholaræœç´¢"""
        try:
            params = {
                'query': query,
                'limit': min(max_results, 100),  # APIé™åˆ¶
                'fields': 'title,authors,abstract,year,venue,citationCount,url,doi,paperId'
            }
            
            response = self.session.get(self.semantic_scholar_url, params=params, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            papers = []
            
            for item in data.get('data', []):
                try:
                    # æå–ä½œè€…
                    authors = []
                    for author in item.get('authors', []):
                        authors.append(author.get('name', ''))
                    
                    # æå–å…³é”®è¯ï¼ˆä»æ ‡é¢˜ä¸­ï¼‰
                    keywords = self._extract_keywords_from_title(item.get('title', ''))
                    
                    paper = Paper(
                        title=item.get('title', ''),
                        authors=authors,
                        abstract=item.get('abstract', ''),
                        year=item.get('year', 0),
                        venue=item.get('venue', ''),
                        url=item.get('url', ''),
                        doi=item.get('doi', ''),
                        citations=item.get('citationCount', 0),
                        keywords=keywords
                    )
                    
                    # å¹´ä»½è¿‡æ»¤
                    if year_range:
                        if year_range[0] <= paper.year <= year_range[1]:
                            papers.append(paper)
                    else:
                        papers.append(paper)
                
                except Exception as e:
                    print(f"âš ï¸ è§£æSemantic Scholarè®ºæ–‡å¤±è´¥: {e}")
                    continue
            
            return papers
            
        except Exception as e:
            print(f"âš ï¸ Semantic Scholaræœç´¢å¤±è´¥: {e}")
            return []
    
    def _extract_keywords_from_title(self, title: str) -> List[str]:
        """ä»æ ‡é¢˜ä¸­æå–å…³é”®è¯"""
        # ç®€å•çš„å…³é”®è¯æå–
        keywords = []
        
        # å¸¸è§çš„æŠ€æœ¯æœ¯è¯­
        tech_terms = [
            'Machine Learning', 'Deep Learning', 'Neural Network', 'Algorithm',
            'Artificial Intelligence', 'Data Science', 'Computer Vision',
            'Natural Language Processing', 'Reinforcement Learning',
            'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'ç¥ç»ç½‘ç»œ', 'ç®—æ³•', 'äººå·¥æ™ºèƒ½',
            'æ•°æ®ç§‘å­¦', 'è®¡ç®—æœºè§†è§‰', 'è‡ªç„¶è¯­è¨€å¤„ç†', 'å¼ºåŒ–å­¦ä¹ '
        ]
        
        for term in tech_terms:
            if term.lower() in title.lower():
                keywords.append(term)
        
        return keywords
    
    def _deduplicate_papers(self, papers: List[Paper]) -> List[Paper]:
        """å»é‡è®ºæ–‡"""
        seen = set()
        unique_papers = []
        
        for paper in papers:
            # ä½¿ç”¨æ ‡é¢˜å’Œå¹´ä»½ä½œä¸ºå»é‡æ ‡è¯†
            identifier = (paper.title.lower(), paper.year)
            if identifier not in seen:
                seen.add(identifier)
                unique_papers.append(paper)
        
        return unique_papers
    
    def download_paper(self, paper: Paper, download_dir: str = "downloads") -> Optional[str]:
        """ä¸‹è½½è®ºæ–‡PDF"""
        if not paper.pdf_url:
            print(f"âš ï¸ è®ºæ–‡ {paper.title} æ²¡æœ‰PDFé“¾æ¥")
            return None
        
        try:
            # åˆ›å»ºä¸‹è½½ç›®å½•
            os.makedirs(download_dir, exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶å
            safe_title = re.sub(r'[^\w\s-]', '', paper.title)[:50]
            filename = f"{safe_title}_{paper.year}.pdf"
            filepath = os.path.join(download_dir, filename)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if os.path.exists(filepath):
                print(f"ğŸ“„ è®ºæ–‡å·²å­˜åœ¨: {filename}")
                return filepath
            
            # ä¸‹è½½æ–‡ä»¶
            print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½: {paper.title}")
            response = self.session.get(paper.pdf_url, stream=True, timeout=30)
            response.raise_for_status()
            
            # ä¿å­˜æ–‡ä»¶
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            print(f"âœ… ä¸‹è½½å®Œæˆ: {filename}")
            return filepath
            
        except Exception as e:
            print(f"âš ï¸ ä¸‹è½½å¤±è´¥: {e}")
            return None

class PaperAnalyzer:
    """è®ºæ–‡åˆ†æå™¨"""
    
    def __init__(self):
        self.stop_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
            'çš„', 'äº†', 'å’Œ', 'æ˜¯', 'åœ¨', 'æœ‰', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'ä»¬', 'è¿™', 'é‚£'
        }
    
    def analyze_paper(self, paper: Paper) -> Dict[str, Any]:
        """åˆ†æè®ºæ–‡å†…å®¹"""
        analysis = {
            "paper_id": paper.doi or paper.url,
            "title": paper.title,
            "key_contributions": self._extract_key_contributions(paper.abstract),
            "methodology": self._extract_methodology(paper.abstract),
            "findings": self._extract_findings(paper.abstract),
            "limitations": self._identify_limitations(paper.abstract),
            "future_work": self._extract_future_work(paper.abstract),
            "relevance_score": self._calculate_relevance_score(paper),
            "quality_indicators": self._assess_quality(paper)
        }
        
        return analysis
    
    def _extract_key_contributions(self, abstract: str) -> List[str]:
        """æå–å…³é”®è´¡çŒ®"""
        contributions = []
        
        # å¯»æ‰¾è´¡çŒ®ç›¸å…³çš„å¥å­
        contribution_patterns = [
            r'(?:we|this paper|our work) (?:propose|present|introduce|develop) (.+?)(?:\.|$)',
            r'(?:contribution|contribute|novelty) (.+?)(?:\.|$)',
            r'(?:main contribution|key contribution) (.+?)(?:\.|$)'
        ]
        
        for pattern in contribution_patterns:
            matches = re.findall(pattern, abstract, re.IGNORECASE)
            for match in matches:
                contributions.append(match.strip())
        
        return contributions[:3]  # è¿”å›æœ€å¤š3ä¸ªè´¡çŒ®
    
    def _extract_methodology(self, abstract: str) -> List[str]:
        """æå–æ–¹æ³•è®º"""
        methodology = []
        
        # å¯»æ‰¾æ–¹æ³•ç›¸å…³çš„è¯æ±‡
        method_patterns = [
            r'(?:method|approach|technique|algorithm) (.+?)(?:\.|$)',
            r'(?:using|by|through) (.+?)(?:\.|$)',
            r'(?:we use|we employ|we apply) (.+?)(?:\.|$)'
        ]
        
        for pattern in method_patterns:
            matches = re.findall(pattern, abstract, re.IGNORECASE)
            for match in matches:
                methodology.append(match.strip())
        
        return methodology[:3]
    
    def _extract_findings(self, abstract: str) -> List[str]:
        """æå–å‘ç°"""
        findings = []
        
        # å¯»æ‰¾ç»“æœç›¸å…³çš„å¥å­
        finding_patterns = [
            r'(?:result|finding|show|demonstrate|indicate) (.+?)(?:\.|$)',
            r'(?:we find|we show|we demonstrate) (.+?)(?:\.|$)',
            r'(?:experiment|evaluation) (.+?)(?:\.|$)'
        ]
        
        for pattern in finding_patterns:
            matches = re.findall(pattern, abstract, re.IGNORECASE)
            for match in matches:
                findings.append(match.strip())
        
        return findings[:3]
    
    def _identify_limitations(self, abstract: str) -> List[str]:
        """è¯†åˆ«å±€é™æ€§"""
        limitations = []
        
        # å¯»æ‰¾é™åˆ¶ç›¸å…³çš„è¯æ±‡
        limitation_patterns = [
            r'(?:limitation|limit|challenge) (.+?)(?:\.|$)',
            r'(?:however|but|although) (.+?)(?:\.|$)',
            r'(?:future work|future direction) (.+?)(?:\.|$)'
        ]
        
        for pattern in limitation_patterns:
            matches = re.findall(pattern, abstract, re.IGNORECASE)
            for match in matches:
                limitations.append(match.strip())
        
        return limitations[:2]
    
    def _extract_future_work(self, abstract: str) -> List[str]:
        """æå–æœªæ¥å·¥ä½œ"""
        future_work = []
        
        # å¯»æ‰¾æœªæ¥å·¥ä½œç›¸å…³çš„å¥å­
        future_patterns = [
            r'(?:future work|future direction|future research) (.+?)(?:\.|$)',
            r'(?:in the future|next steps) (.+?)(?:\.|$)',
            r'(?:plan to|will) (.+?)(?:\.|$)'
        ]
        
        for pattern in future_patterns:
            matches = re.findall(pattern, abstract, re.IGNORECASE)
            for match in matches:
                future_work.append(match.strip())
        
        return future_work[:2]
    
    def _calculate_relevance_score(self, paper: Paper) -> float:
        """è®¡ç®—ç›¸å…³æ€§è¯„åˆ†"""
        score = 0.5  # åŸºç¡€åˆ†æ•°
        
        # åŸºäºå¼•ç”¨æ•°
        if paper.citations > 100:
            score += 0.2
        elif paper.citations > 10:
            score += 0.1
        
        # åŸºäºå¹´ä»½ï¼ˆè¾ƒæ–°çš„è®ºæ–‡æ›´æœ‰ä»·å€¼ï¼‰
        current_year = datetime.now().year
        if current_year - paper.year <= 2:
            score += 0.2
        elif current_year - paper.year <= 5:
            score += 0.1
        
        # åŸºäºæœŸåˆŠ/ä¼šè®®è´¨é‡
        high_quality_venues = [
            'Nature', 'Science', 'Cell', 'NeurIPS', 'ICML', 'ICLR', 'AAAI',
            'IJCAI', 'CVPR', 'ICCV', 'ECCV', 'ACL', 'EMNLP'
        ]
        if any(venue in paper.venue for venue in high_quality_venues):
            score += 0.1
        
        return min(score, 1.0)
    
    def _assess_quality(self, paper: Paper) -> Dict[str, Any]:
        """è¯„ä¼°è®ºæ–‡è´¨é‡"""
        quality = {
            "citation_impact": "high" if paper.citations > 100 else "medium" if paper.citations > 10 else "low",
            "recency": "recent" if datetime.now().year - paper.year <= 2 else "moderate",
            "venue_quality": "high" if any(q in paper.venue for q in ['Nature', 'Science', 'NeurIPS']) else "medium",
            "author_count": "adequate" if 2 <= len(paper.authors) <= 6 else "needs_review"
        }
        
        return quality

def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python paper_search.py <æœç´¢å…³é”®è¯> [æœ€å¤§ç»“æœæ•°]")
        print("ç¤ºä¾‹: python paper_search.py æœºå™¨å­¦ä¹  5")
        return
    
    query = sys.argv[1]
    max_results = int(sys.argv[2]) if len(sys.argv) > 2 else 10
    
    # åˆ›å»ºæœç´¢å¼•æ“
    search_engine = AcademicSearchEngine()
    
    # æœç´¢è®ºæ–‡
    papers = search_engine.search_papers(query, max_results)
    
    if not papers:
        print(f"âŒ æœªæ‰¾åˆ°å…³äº'{query}'çš„è®ºæ–‡")
        return
    
    print(f"\nğŸ“š æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡:")
    print("=" * 60)
    
    # åˆ†æè®ºæ–‡
    analyzer = PaperAnalyzer()
    
    for i, paper in enumerate(papers, 1):
        print(f"\n{i}. {paper.title}")
        print(f"   ä½œè€…: {', '.join(paper.authors[:3])}{'...' if len(paper.authors) > 3 else ''}")
        print(f"   å¹´ä»½: {paper.year} | å¼•ç”¨: {paper.citations}")
        print(f"   æœŸåˆŠ: {paper.venue}")
        print(f"   æ‘˜è¦: {paper.abstract[:150]}...")
        
        # åˆ†æè®ºæ–‡
        analysis = analyzer.analyze_paper(paper)
        print(f"   ç›¸å…³æ€§: {analysis['relevance_score']:.2f}")
        print(f"   å…³é”®è´¡çŒ®: {len(analysis['key_contributions'])} é¡¹")
        
        # ä¸‹è½½é€‰é¡¹
        if paper.pdf_url:
            print(f"   PDF: {paper.pdf_url}")
    
    # ä¿å­˜ç»“æœ
    result = {
        "query": query,
        "timestamp": datetime.now().isoformat(),
        "papers": [
            {
                "title": p.title,
                "authors": p.authors,
                "year": p.year,
                "venue": p.venue,
                "abstract": p.abstract,
                "citations": p.citations,
                "url": p.url,
                "doi": p.doi,
                "pdf_url": p.pdf_url,
                "keywords": p.keywords
            }
            for p in papers
        ]
    }
    
    output_file = f"papers_{query}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“„ æœç´¢ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

if __name__ == "__main__":
    main()
