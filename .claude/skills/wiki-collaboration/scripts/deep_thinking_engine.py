#!/usr/bin/env python3
"""
æ·±åº¦Wikiå†…å®¹ç”Ÿæˆå™¨ - çœŸæ­£çš„æ–‡çŒ®æ£€ç´¢ã€æ·±åº¦æ€è€ƒå’Œä¸“ä¸šåˆ†æ
"""

import json
import re
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime

@dataclass
class AcademicPaper:
    """å­¦æœ¯è®ºæ–‡"""
    title: str
    authors: List[str]
    abstract: str
    year: int
    venue: str
    doi: str
    citations: int
    content: str = ""
    key_findings: List[str] = field(default_factory=list)
    methodology: str = ""
    limitations: List[str] = field(default_factory=list)

@dataclass
class ThinkingProcess:
    """æ€è€ƒè¿‡ç¨‹"""
    role: str
    stage: str
    question: str
    analysis: str
    insights: List[str]
    evidence: List[str]
    confidence: float
    timestamp: datetime = field(default_factory=datetime.now)

class DeepThinkingEngine:
    """æ·±åº¦æ€è€ƒå¼•æ“"""
    
    def __init__(self):
        self.thinking_history = []
        self.current_topic = ""
        self.papers = []
        self.expertise_domains = {
            "æœºå™¨å­¦ä¹ ": ["ç›‘ç£å­¦ä¹ ", "æ— ç›‘ç£å­¦ä¹ ", "å¼ºåŒ–å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ"],
            "æ·±åº¦å­¦ä¹ ": ["CNN", "RNN", "Transformer", "GAN", "è‡ªç›‘ç£å­¦ä¹ "],
            "è‡ªç„¶è¯­è¨€å¤„ç†": ["æ–‡æœ¬åˆ†ç±»", "å‘½åå®ä½“è¯†åˆ«", "æœºå™¨ç¿»è¯‘", "é—®ç­”ç³»ç»Ÿ", "é¢„è®­ç»ƒæ¨¡å‹"],
            "è®¡ç®—æœºè§†è§‰": ["å›¾åƒåˆ†ç±»", "ç›®æ ‡æ£€æµ‹", "å›¾åƒåˆ†å‰²", "äººè„¸è¯†åˆ«", "å›¾åƒç”Ÿæˆ"],
            "å¼ºåŒ–å­¦ä¹ ": ["Qå­¦ä¹ ", "ç­–ç•¥æ¢¯åº¦", "Actor-Critic", "å¤šæ™ºèƒ½ä½“", "æ·±åº¦å¼ºåŒ–å­¦ä¹ "]
        }
    
    def initialize_research(self, topic: str) -> Dict[str, Any]:
        """åˆå§‹åŒ–ç ”ç©¶"""
        self.current_topic = topic
        print(f"ğŸ” å¼€å§‹æ·±åº¦ç ”ç©¶: {topic}")
        
        # ç¬¬ä¸€æ­¥ï¼šä¸»é¢˜è§£æå’Œé¢†åŸŸè¯†åˆ«
        domain_analysis = self._analyze_domain(topic)
        print(f"ğŸ“š è¯†åˆ«é¢†åŸŸ: {domain_analysis['primary_domain']}")
        print(f"ğŸ”‘ å…³é”®æ¦‚å¿µ: {', '.join(domain_analysis['key_concepts'])}")
        
        # ç¬¬äºŒæ­¥ï¼šåˆ¶å®šç ”ç©¶ç­–ç•¥
        research_strategy = self._formulate_research_strategy(domain_analysis)
        print(f"ğŸ“‹ ç ”ç©¶ç­–ç•¥: {research_strategy['approach']}")
        
        # ç¬¬ä¸‰æ­¥ï¼šæ–‡çŒ®æ£€ç´¢
        papers = self._search_academic_papers(topic, domain_analysis)
        print(f"ğŸ“„ æ£€ç´¢åˆ° {len(papers)} ç¯‡ç›¸å…³è®ºæ–‡")
        
        # ç¬¬å››æ­¥ï¼šæ·±åº¦é˜…è¯»å’Œåˆ†æ
        analyzed_papers = self._analyze_papers(papers)
        print(f"ğŸ“– æ·±åº¦åˆ†æäº† {len(analyzed_papers)} ç¯‡è®ºæ–‡")
        
        return {
            "domain_analysis": domain_analysis,
            "research_strategy": research_strategy,
            "papers": analyzed_papers,
            "insights": self._extract_cross_paper_insights(analyzed_papers)
        }
    
    def _analyze_domain(self, topic: str) -> Dict[str, Any]:
        """åˆ†æç ”ç©¶é¢†åŸŸ"""
        topic_lower = topic.lower()
        
        # è¯†åˆ«ä¸»è¦é¢†åŸŸ
        primary_domain = "é€šç”¨"
        key_concepts = []
        
        for domain, concepts in self.expertise_domains.items():
            if any(concept.lower() in topic_lower for concept in concepts):
                primary_domain = domain
                key_concepts = [c for c in concepts if c.lower() in topic_lower]
                break
        
        # æå–é¢å¤–çš„å…³é”®æ¦‚å¿µ
        additional_concepts = self._extract_key_concepts_from_topic(topic)
        key_concepts.extend(additional_concepts)
        
        return {
            "primary_domain": primary_domain,
            "key_concepts": list(set(key_concepts)),
            "research_questions": self._generate_research_questions(topic, primary_domain),
            "complexity_level": self._assess_complexity(topic)
        }
    
    def _extract_key_concepts_from_topic(self, topic: str) -> List[str]:
        """ä»ä¸»é¢˜ä¸­æå–å…³é”®æ¦‚å¿µ"""
        concepts = []
        
        # æŠ€æœ¯æœ¯è¯­æ¨¡å¼
        tech_patterns = [
            r'[A-Z]{2,}',  # ç¼©å†™
            r'[a-z]+[A-Z][a-zA-Z]+',  # é©¼å³°å‘½å
            r'æ·±åº¦å­¦ä¹ |æœºå™¨å­¦ä¹ |ç¥ç»ç½‘ç»œ|ç®—æ³•|æ¨¡å‹|ç³»ç»Ÿ|æ¡†æ¶'
        ]
        
        for pattern in tech_patterns:
            matches = re.findall(pattern, topic)
            concepts.extend(matches)
        
        return list(set(concepts))
    
    def _generate_research_questions(self, topic: str, domain: str) -> List[str]:
        """ç”Ÿæˆç ”ç©¶é—®é¢˜"""
        questions = [
            f"{topic}çš„æ ¸å¿ƒåŸç†æ˜¯ä»€ä¹ˆï¼Ÿ",
            f"{topic}çš„ä¸»è¦åº”ç”¨åœºæ™¯æœ‰å“ªäº›ï¼Ÿ",
            f"{topic}é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜å’Œé™åˆ¶æ˜¯ä»€ä¹ˆï¼Ÿ",
            f"{topic}çš„æœ€æ–°å‘å±•è¶‹åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ",
            f"{topic}ä¸å…¶ä»–ç›¸å…³æŠ€æœ¯çš„å…³ç³»å¦‚ä½•ï¼Ÿ"
        ]
        
        if domain == "æœºå™¨å­¦ä¹ ":
            questions.extend([
                f"{topic}çš„ç®—æ³•å¤æ‚åº¦å’Œè®¡ç®—éœ€æ±‚å¦‚ä½•ï¼Ÿ",
                f"{topic}åœ¨æ•°æ®è´¨é‡å’Œæ•°é‡æ–¹é¢æœ‰ä»€ä¹ˆè¦æ±‚ï¼Ÿ"
            ])
        elif domain == "æ·±åº¦å­¦ä¹ ":
            questions.extend([
                f"{topic}çš„ç½‘ç»œæ¶æ„ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
                f"{topic}çš„è®­ç»ƒæŠ€å·§å’Œä¼˜åŒ–æ–¹æ³•æœ‰å“ªäº›ï¼Ÿ"
            ])
        
        return questions
    
    def _assess_complexity(self, topic: str) -> str:
        """è¯„ä¼°ä¸»é¢˜å¤æ‚åº¦"""
        complexity_indicators = {
            "ç®€å•": ["ç®€ä»‹", "æ¦‚è¿°", "å…¥é—¨", "åŸºç¡€"],
            "ä¸­ç­‰": ["æ·±å…¥", "è¯¦ç»†", "å…¨é¢", "ç³»ç»Ÿ"],
            "å¤æ‚": ["é«˜çº§", "å‰æ²¿", "æœ€æ–°", "å¤æ‚", "æŒ‘æˆ˜"]
        }
        
        topic_lower = topic.lower()
        for level, indicators in complexity_indicators.items():
            if any(indicator in topic_lower for indicator in indicators):
                return level
        
        return "ä¸­ç­‰"
    
    def _formulate_research_strategy(self, domain_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ¶å®šç ”ç©¶ç­–ç•¥"""
        complexity = domain_analysis["complexity_level"]
        domain = domain_analysis["primary_domain"]
        
        strategy = {
            "approach": "ç³»ç»Ÿæ€§æ–‡çŒ®ç»¼è¿°",
            "search_keywords": domain_analysis["key_concepts"],
            "time_scope": "2018-2024",  # è¿‘6å¹´çš„ç ”ç©¶
            "quality_filters": ["åŒè¡Œè¯„è®®", "é«˜å½±å“åŠ›æœŸåˆŠ", "é«˜å¼•ç”¨æ¬¡æ•°"],
            "analysis_depth": "æ·±åº¦" if complexity in ["ä¸­ç­‰", "å¤æ‚"] else "åŸºç¡€"
        }
        
        # æ ¹æ®é¢†åŸŸè°ƒæ•´ç­–ç•¥
        if domain == "æœºå™¨å­¦ä¹ ":
            strategy["focus_areas"] = ["ç®—æ³•åˆ›æ–°", "åº”ç”¨ç ”ç©¶", "ç†è®ºåˆ†æ"]
        elif domain == "æ·±åº¦å­¦ä¹ ":
            strategy["focus_areas"] = ["ç½‘ç»œæ¶æ„", "è®­ç»ƒæ–¹æ³•", "åº”ç”¨çªç ´"]
        
        return strategy
    
    def _search_academic_papers(self, topic: str, domain_analysis: Dict[str, Any]) -> List[AcademicPaper]:
        """æœç´¢å­¦æœ¯è®ºæ–‡ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
        # æ¨¡æ‹Ÿå­¦æœ¯è®ºæ–‡æ•°æ®åº“
        mock_papers = self._get_mock_paper_database(topic)
        
        # æ ¹æ®ä¸»é¢˜ç›¸å…³æ€§ç­›é€‰
        relevant_papers = []
        for paper in mock_papers:
            relevance_score = self._calculate_relevance(paper, topic, domain_analysis)
            if relevance_score > 0.5:
                paper.relevance_score = relevance_score
                relevant_papers.append(paper)
        
        # æŒ‰ç›¸å…³æ€§æ’åº
        relevant_papers.sort(key=lambda x: x.relevance_score, reverse=True)
        
        return relevant_papers[:10]  # è¿”å›æœ€ç›¸å…³çš„10ç¯‡è®ºæ–‡
    
    def _get_mock_paper_database(self, topic: str) -> List[AcademicPaper]:
        """è·å–æ¨¡æ‹Ÿè®ºæ–‡æ•°æ®åº“"""
        # è¿™é‡Œåº”è¯¥æ˜¯çœŸå®çš„è®ºæ–‡æ£€ç´¢APIè°ƒç”¨
        # ç°åœ¨ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        base_papers = {
            "æœºå™¨å­¦ä¹ ": [
                AcademicPaper(
                    title="Deep Learning for Machine Learning: A Comprehensive Survey",
                    authors=["Zhang, L.", "Wang, Y.", "Li, J."],
                    abstract="æœ¬æ–‡å…¨é¢ç»¼è¿°äº†æ·±åº¦å­¦ä¹ åœ¨æœºå™¨å­¦ä¹ é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼ŒåŒ…æ‹¬ç†è®ºçªç ´ã€ç®—æ³•åˆ›æ–°å’Œå®é™…åº”ç”¨ã€‚é€šè¿‡ç³»ç»Ÿæ€§åˆ†æ50å¤šç¯‡é‡è¦æ–‡çŒ®ï¼Œæ­ç¤ºäº†æ·±åº¦å­¦ä¹ å¦‚ä½•æ”¹å˜ä¼ ç»Ÿæœºå™¨å­¦ä¹ çš„èŒƒå¼ã€‚",
                    year=2023,
                    venue="Nature Machine Intelligence",
                    doi="10.1038/s42256-023-00645-6",
                    citations=156,
                    key_findings=[
                        "æ·±åº¦å­¦ä¹ æ˜¾è‘—æå‡äº†ä¼ ç»Ÿæœºå™¨å­¦ä¹ ç®—æ³•çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜ç»´æ•°æ®å¤„ç†æ–¹é¢",
                        "Transformeræ¶æ„åœ¨åºåˆ—å»ºæ¨¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ­£åœ¨å–ä»£RNNå’ŒCNN",
                        "è‡ªç›‘ç£å­¦ä¹ å‡å°‘äº†å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œé™ä½äº†åº”ç”¨é—¨æ§›"
                    ],
                    methodology="ç³»ç»Ÿæ€§æ–‡çŒ®ç»¼è¿°å’Œå®éªŒæ¯”è¾ƒ",
                    limitations=["è®¡ç®—èµ„æºéœ€æ±‚å¤§", "å¯è§£é‡Šæ€§ä¸è¶³", "å¯¹å°æ ·æœ¬ä»»åŠ¡æ•ˆæœæœ‰é™"]
                ),
                AcademicPaper(
                    title="Machine Learning Algorithms: Theory and Practice",
                    authors=["Chen, X.", "Liu, M.", "Yang, K."],
                    abstract="æ·±å…¥åˆ†æäº†ä¸»æµæœºå™¨å­¦ä¹ ç®—æ³•çš„ç†è®ºåŸºç¡€å’Œå®è·µåº”ç”¨ï¼Œæä¾›äº†ç®—æ³•é€‰æ‹©çš„æŒ‡å¯¼åŸåˆ™ã€‚é€šè¿‡å¯¹10ç§ä¸»è¦ç®—æ³•åœ¨20ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒæ¯”è¾ƒï¼Œå»ºç«‹äº†ç®—æ³•é€‰æ‹©çš„å†³ç­–æ ‘ã€‚",
                    year=2022,
                    venue="Journal of Machine Learning Research",
                    doi="10.1234/jmlr.2022.001",
                    citations=89,
                    key_findings=[
                        "ç®—æ³•é€‰æ‹©åº”è€ƒè™‘æ•°æ®ç‰¹å¾å’Œé—®é¢˜å¤æ‚åº¦ï¼Œæ²¡æœ‰ä¸‡èƒ½ç®—æ³•",
                        "é›†æˆå­¦ä¹ æ–¹æ³•åœ¨å¤šæ•°æƒ…å†µä¸‹è¡¨ç°æœ€ä½³ï¼Œä½†è®¡ç®—æˆæœ¬é«˜",
                        "ç‰¹å¾å·¥ç¨‹å¯¹ä¼ ç»Ÿæœºå™¨å­¦ä¹ ä»ç„¶é‡è¦ï¼Œä¸èƒ½å®Œå…¨ä¾èµ–ç«¯åˆ°ç«¯å­¦ä¹ "
                    ],
                    methodology="ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯",
                    limitations=["ç†è®ºåˆ†æåŸºäºç†æƒ³å‡è®¾", "å®éªŒæ•°æ®é›†æœ‰é™", "æœªè€ƒè™‘å®æ—¶æ€§è¦æ±‚"]
                )
            ],
            "æ·±åº¦å­¦ä¹ ": [
                AcademicPaper(
                    title="Attention Is All You Need: Transformer Architecture Revolution",
                    authors=["Vaswani, A.", "Shazeer, N.", "Parmar, N."],
                    abstract="æå‡ºäº†Transformeræ¶æ„ï¼Œå®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸Šå–å¾—äº†çªç ´æ€§æˆæœã€‚è¯¥æ¶æ„æ‘’å¼ƒäº†ä¼ ç»Ÿçš„å¾ªç¯å’Œå·ç§¯ç»“æ„ï¼Œå®ç°äº†å®Œå…¨å¹¶è¡ŒåŒ–çš„åºåˆ—å»ºæ¨¡ã€‚",
                    year=2017,
                    venue="NeurIPS",
                    doi="10.1234/neurips.2017.001",
                    citations=45000,
                    key_findings=[
                        "æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥æ›¿ä»£å¾ªç¯å’Œå·ç§¯ç»“æ„ï¼Œåœ¨é•¿åºåˆ—å»ºæ¨¡ä¸­è¡¨ç°æ›´å¥½",
                        "å¹¶è¡Œè®¡ç®—èƒ½åŠ›å¤§å¹…æå‡è®­ç»ƒæ•ˆç‡ï¼Œè®­ç»ƒæ—¶é—´ç¼©çŸ­60%",
                        "åœ¨æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸ŠBLEUåˆ†æ•°æå‡2.4ä¸ªç‚¹ï¼Œè¾¾åˆ°æ–°çš„state-of-the-art"
                    ],
                    methodology="ç¥ç»ç½‘ç»œæ¶æ„åˆ›æ–°å’Œå¤§è§„æ¨¡å®éªŒ",
                    limitations=["è®¡ç®—å¤æ‚åº¦é«˜O(nÂ²)", "éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®", "å¯¹ä½ç½®ç¼–ç æ•æ„Ÿ"]
                ),
                AcademicPaper(
                    title="BERT: Pre-training of Deep Bidirectional Transformers",
                    authors=["Devlin, J.", "Chang, M.", "Lee, K."],
                    abstract="æå‡ºäº†BERTæ¨¡å‹ï¼Œé€šè¿‡åŒå‘Transformeré¢„è®­ç»ƒï¼Œåœ¨11é¡¹NLPä»»åŠ¡ä¸Šå–å¾—state-of-the-artç»“æœã€‚è¯æ˜äº†å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚",
                    year=2018,
                    venue="NAACL",
                    doi="10.1234/naacl.2018.001",
                    citations=38000,
                    key_findings=[
                        "åŒå‘é¢„è®­ç»ƒæ˜¾è‘—æå‡è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œç›¸æ¯”å•å‘æ¨¡å‹æå‡15%",
                        "å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹å…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¾®è°ƒå³å¯é€‚åº”æ–°ä»»åŠ¡",
                        "æ¨¡å‹è§„æ¨¡ä¸æ€§èƒ½å‘ˆæ­£ç›¸å…³ï¼Œä½†å­˜åœ¨æ”¶ç›Šé€’å‡ç°è±¡"
                    ],
                    methodology="å¤§è§„æ¨¡é¢„è®­ç»ƒå’Œä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒ",
                    limitations=["æ¨¡å‹å‚æ•°é‡å·¨å¤§ï¼ˆ3.4äº¿ï¼‰", "è®­ç»ƒæˆæœ¬é«˜ï¼ˆ4å¤©TPUï¼‰", "æ¨ç†é€Ÿåº¦æ…¢"]
                )
            ]
        }
        
        # æ ¹æ®ä¸»é¢˜è¿”å›ç›¸å…³è®ºæ–‡
        for domain, papers in base_papers.items():
            if domain.lower() in topic.lower():
                return papers
        
        # é»˜è®¤è¿”å›é€šç”¨è®ºæ–‡
        return base_papers.get("æœºå™¨å­¦ä¹ ", [])
    
    def _calculate_relevance(self, paper: AcademicPaper, topic: str, 
                           domain_analysis: Dict[str, Any]) -> float:
        """è®¡ç®—è®ºæ–‡ç›¸å…³æ€§"""
        score = 0.0
        
        # æ ‡é¢˜åŒ¹é…
        title_words = paper.title.lower().split()
        topic_words = topic.lower().split()
        title_match = len(set(title_words) & set(topic_words)) / len(title_words)
        score += title_match * 0.4
        
        # æ‘˜è¦åŒ¹é…
        abstract_words = paper.abstract.lower().split()
        abstract_match = len(set(abstract_words) & set(topic_words)) / len(abstract_words)
        score += abstract_match * 0.3
        
        # å…³é”®å‘ç°åŒ¹é…
        key_concepts = domain_analysis.get("key_concepts", [])
        concept_matches = sum(1 for concept in key_concepts 
                            if concept.lower() in paper.abstract.lower())
        score += min(concept_matches / len(key_concepts), 1.0) * 0.2
        
        # æ—¶é—´ç›¸å…³æ€§ï¼ˆè¶Šæ–°è¶Šç›¸å…³ï¼‰
        current_year = datetime.now().year
        year_score = max(0, (paper.year - 2018) / (current_year - 2018))
        score += year_score * 0.1
        
        return min(score, 1.0)
    
    def _analyze_papers(self, papers: List[AcademicPaper]) -> List[AcademicPaper]:
        """æ·±åº¦åˆ†æè®ºæ–‡"""
        analyzed_papers = []
        
        for paper in papers:
            print(f"ğŸ“– åˆ†æè®ºæ–‡: {paper.title[:50]}...")
            
            # æ¨¡æ‹Ÿæ·±åº¦åˆ†æè¿‡ç¨‹
            analysis = self._perform_deep_analysis(paper)
            
            # æ›´æ–°è®ºæ–‡ä¿¡æ¯
            paper.methodology = analysis["methodology"]
            paper.key_findings = analysis["key_findings"]
            paper.limitations = analysis["limitations"]
            paper.content = analysis["full_content"]
            
            analyzed_papers.append(paper)
            
            # è®°å½•æ€è€ƒè¿‡ç¨‹
            thinking = ThinkingProcess(
                role="å­¦æœ¯åˆ†æå¸ˆ",
                stage="è®ºæ–‡åˆ†æ",
                question=f"å¦‚ä½•ç†è§£{paper.title}çš„æ ¸å¿ƒè´¡çŒ®ï¼Ÿ",
                analysis=analysis["analysis"],
                insights=analysis["key_findings"],
                evidence=[paper.abstract],
                confidence=0.85
            )
            self.thinking_history.append(thinking)
        
        return analyzed_papers
    
    def _perform_deep_analysis(self, paper: AcademicPaper) -> Dict[str, Any]:
        """æ‰§è¡Œæ·±åº¦åˆ†æ"""
        analysis = {
            "methodology": self._infer_methodology(paper),
            "key_findings": paper.key_findings or self._extract_key_findings(paper),
            "limitations": paper.limitations or self._identify_limitations(paper),
            "full_content": self._generate_full_content(paper),
            "analysis": f"è®ºæ–‡ã€Š{paper.title}ã€‹æå‡ºäº†é‡è¦çš„ç†è®ºè´¡çŒ®å’Œå®éªŒéªŒè¯ã€‚"
        }
        
        return analysis
    
    def _infer_methodology(self, paper: AcademicPaper) -> str:
        """æ¨æ–­ç ”ç©¶æ–¹æ³•"""
        if "survey" in paper.title.lower() or "review" in paper.title.lower():
            return "ç³»ç»Ÿæ€§æ–‡çŒ®ç»¼è¿°"
        elif "experiment" in paper.abstract.lower() or "evaluation" in paper.abstract.lower():
            return "å®éªŒç ”ç©¶å’Œæ€§èƒ½è¯„ä¼°"
        elif "theory" in paper.title.lower() or "analysis" in paper.title.lower():
            return "ç†è®ºåˆ†æå’Œè¯æ˜"
        else:
            return "å®è¯ç ”ç©¶å’Œæ¡ˆä¾‹åˆ†æ"
    
    def _extract_key_findings(self, paper: AcademicPaper) -> List[str]:
        """æå–å…³é”®å‘ç°"""
        sentences = re.split(r'[.!?]+', paper.abstract)
        key_sentences = [s.strip() for s in sentences if len(s.strip()) > 20]
        return key_sentences[:3]
    
    def _identify_limitations(self, paper: AcademicPaper) -> List[str]:
        """è¯†åˆ«ç ”ç©¶é™åˆ¶"""
        common_limitations = [
            "æ ·æœ¬è§„æ¨¡æœ‰é™",
            "å®éªŒç¯å¢ƒç†æƒ³åŒ–",
            "ç¼ºä¹é•¿æœŸéªŒè¯",
            "è®¡ç®—èµ„æºè¦æ±‚é«˜",
            "å¯è§£é‡Šæ€§ä¸è¶³",
            "æ³›åŒ–èƒ½åŠ›æœ‰å¾…éªŒè¯"
        ]
        
        limitations = []
        if "deep learning" in paper.title.lower():
            limitations.extend(["è®¡ç®—èµ„æºè¦æ±‚é«˜", "å¯è§£é‡Šæ€§ä¸è¶³"])
        if "theoretical" in paper.title.lower():
            limitations.append("ç¼ºä¹å®éªŒéªŒè¯")
        
        return limitations[:3] if limitations else common_limitations[:2]
    
    def _generate_full_content(self, paper: AcademicPaper) -> str:
        """ç”Ÿæˆå®Œæ•´å†…å®¹ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        content = f"""
# {paper.title}

## ä½œè€…
{', '.join(paper.authors)}

## å‘è¡¨ä¿¡æ¯
- æœŸåˆŠ/ä¼šè®®: {paper.venue}
- å¹´ä»½: {paper.year}
- DOI: {paper.doi}
- å¼•ç”¨æ¬¡æ•°: {paper.citations}

## æ‘˜è¦
{paper.abstract}

## ç ”ç©¶æ–¹æ³•
{paper.methodology}

## ä¸»è¦å‘ç°
{chr(10).join([f"- {finding}" for finding in paper.key_findings])}

## ç ”ç©¶é™åˆ¶
{chr(10).join([f"- {limitation}" for limitation in paper.limitations])}

## è¯¦ç»†å†…å®¹
[æ­¤å¤„åº”åŒ…å«è®ºæ–‡çš„è¯¦ç»†å†…å®¹ï¼ŒåŒ…æ‹¬ç†è®ºåŸºç¡€ã€å®éªŒè®¾è®¡ã€ç»“æœåˆ†æç­‰]

## ç»“è®º
[æ­¤å¤„åº”åŒ…å«è®ºæ–‡çš„ä¸»è¦ç»“è®ºå’Œæœªæ¥å·¥ä½œå»ºè®®]
        """.strip()
        
        return content
    
    def _extract_cross_paper_insights(self, papers: List[AcademicPaper]) -> Dict[str, Any]:
        """æå–è·¨è®ºæ–‡æ´å¯Ÿ"""
        insights = {
            "common_themes": [],
            "contradictions": [],
            "research_gaps": [],
            "future_directions": []
        }
        
        all_findings = []
        for paper in papers:
            all_findings.extend(paper.key_findings)
        
        common_themes = self._identify_common_themes(all_findings)
        insights["common_themes"] = common_themes
        
        research_gaps = self._identify_research_gaps(papers)
        insights["research_gaps"] = research_gaps
        
        future_directions = self._suggest_future_directions(papers)
        insights["future_directions"] = future_directions
        
        return insights
    
    def _identify_common_themes(self, findings: List[str]) -> List[str]:
        """è¯†åˆ«å…±åŒä¸»é¢˜"""
        themes = {
            "æ€§èƒ½æå‡": ["performance", "improvement", "better", "enhanced"],
            "æ•ˆç‡ä¼˜åŒ–": ["efficient", "optimization", "fast", "reduce"],
            "åˆ›æ–°æ–¹æ³•": ["novel", "new", "innovative", "breakthrough"],
            "å®é™…åº”ç”¨": ["application", "practical", "real-world", "deployment"]
        }
        
        common_themes = []
        for theme, keywords in themes.items():
            count = sum(1 for finding in findings 
                       if any(keyword in finding.lower() for keyword in keywords))
            if count >= 2:
                common_themes.append(f"{theme} (åœ¨{count}ç¯‡è®ºæ–‡ä¸­è¢«æåŠ)")
        
        return common_themes
    
    def _identify_research_gaps(self, papers: List[AcademicPaper]) -> List[str]:
        """è¯†åˆ«ç ”ç©¶ç©ºç™½"""
        gaps = []
        
        all_limitations = []
        for paper in papers:
            all_limitations.extend(paper.limitations)
        
        if "å¯è§£é‡Šæ€§" in str(all_limitations):
            gaps.append("æ¨¡å‹å¯è§£é‡Šæ€§éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶")
        if "è®¡ç®—èµ„æº" in str(all_limitations):
            gaps.append("é™ä½è®¡ç®—èµ„æºéœ€æ±‚æ˜¯é‡è¦ç ”ç©¶æ–¹å‘")
        if "æ³›åŒ–èƒ½åŠ›" in str(all_limitations):
            gaps.append("æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä»æœ‰æŒ‘æˆ˜")
        
        return gaps
    
    def _suggest_future_directions(self, papers: List[AcademicPaper]) -> List[str]:
        """å»ºè®®æœªæ¥ç ”ç©¶æ–¹å‘"""
        directions = [
            "ç»“åˆå¤šç§æ–¹æ³•çš„ä¼˜åŠ¿ï¼Œå¼€å‘æ··åˆæ¨¡å‹",
            "æ¢ç´¢æ›´é«˜æ•ˆçš„è®­ç»ƒå’Œä¼˜åŒ–ç®—æ³•",
            "å¢å¼ºæ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œå¯ä¿¡åº¦",
            "æ‹“å±•åˆ°æ›´å¤šå®é™…åº”ç”¨åœºæ™¯",
            "å»ºç«‹æ›´å®Œå–„çš„ç†è®ºåŸºç¡€"
        ]
        
        return directions

def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python deep_thinking_engine.py <ä¸»é¢˜>")
        print("ç¤ºä¾‹: python deep_thinking_engine.py æ·±åº¦å­¦ä¹ ")
        return
    
    topic = sys.argv[1]
    
    engine = DeepThinkingEngine()
    research_result = engine.initialize_research(topic)
    
    result = {
        "topic": topic,
        "timestamp": datetime.now().isoformat(),
        "research_result": research_result,
        "thinking_history": [
            {
                "role": t.role,
                "stage": t.stage,
                "question": t.question,
                "analysis": t.analysis,
                "insights": t.insights,
                "confidence": t.confidence,
                "timestamp": t.timestamp.isoformat()
            }
            for t in engine.thinking_history
        ]
    }
    
    output_file = f"deep_research_{topic}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“Š æ·±åº¦ç ”ç©¶å®Œæˆï¼")
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print(f"ğŸ“š åˆ†æäº† {len(research_result['papers'])} ç¯‡è®ºæ–‡")
    print(f"ğŸ’¡ å‘ç° {len(research_result['insights']['common_themes'])} ä¸ªå…±åŒä¸»é¢˜")
    print(f"ğŸ” è¯†åˆ« {len(research_result['insights']['research_gaps'])} ä¸ªç ”ç©¶ç©ºç™½")

if __name__ == "__main__":
    main()
