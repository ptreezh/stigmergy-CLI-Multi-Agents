#!/usr/bin/env python3
"""
Wikiå†…å®¹è´¨é‡æ£€æŸ¥å™¨
è¯„ä¼°Wikiå†…å®¹çš„è´¨é‡ã€å®Œæ•´æ€§å’Œä¸“ä¸šæ€§
"""

import json
import re
import sys
from typing import Dict, List, Tuple, Any
from dataclasses import dataclass
from pathlib import Path

@dataclass
class QualityMetric:
    """è´¨é‡æŒ‡æ ‡æ•°æ®ç»“æ„"""
    name: str
    score: float
    max_score: float
    description: str
    suggestions: List[str]

class QualityChecker:
    """è´¨é‡æ£€æŸ¥å™¨ä¸»ç±»"""
    
    def __init__(self):
        self.metrics = []
        self.weights = {
            'accuracy': 0.25,
            'completeness': 0.20,
            'clarity': 0.20,
            'structure': 0.15,
            'references': 0.10,
            'objectivity': 0.10
        }
    
    def check_content(self, content: str, topic: str = "") -> Dict[str, Any]:
        """æ£€æŸ¥å†…å®¹è´¨é‡"""
        results = {
            'overall_score': 0.0,
            'metrics': {},
            'suggestions': [],
            'issues': [],
            'strengths': []
        }
        
        # æ‰§è¡Œå„é¡¹æ£€æŸ¥
        accuracy_score = self._check_accuracy(content)
        completeness_score = self._check_completeness(content, topic)
        clarity_score = self._check_clarity(content)
        structure_score = self._check_structure(content)
        references_score = self._check_references(content)
        objectivity_score = self._check_objectivity(content)
        
        # æ”¶é›†æŒ‡æ ‡
        metrics = {
            'accuracy': accuracy_score,
            'completeness': completeness_score,
            'clarity': clarity_score,
            'structure': structure_score,
            'references': references_score,
            'objectivity': objectivity_score
        }
        
        # è®¡ç®—æ€»åˆ†
        overall_score = sum(score * self.weights[metric] for metric, score in metrics.items())
        
        results['overall_score'] = overall_score
        results['metrics'] = metrics
        
        # ç”Ÿæˆå»ºè®®å’Œé—®é¢˜
        results['suggestions'] = self._generate_suggestions(metrics)
        results['issues'] = self._identify_issues(metrics)
        results['strengths'] = self._identify_strengths(metrics)
        
        return results
    
    def _check_accuracy(self, content: str) -> QualityMetric:
        """æ£€æŸ¥å‡†ç¡®æ€§"""
        score = 0.0
        suggestions = []
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜ç¡®çš„äº‹å®é™ˆè¿°
        factual_statements = len(re.findall(r'\d{4}å¹´|\d+%', content))
        if factual_statements > 0:
            score += 0.3
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¼•ç”¨æˆ–å‚è€ƒ
        has_references = bool(re.search(r'å‚è€ƒ|å¼•ç”¨|æ¥æº|æ®.*æŠ¥é“', content))
        if has_references:
            score += 0.4
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ¨¡ç³Šè¡¨è¿°
        vague_phrases = len(re.findall(r'å¯èƒ½|å¤§æ¦‚|æ®è¯´|ä¼¼ä¹|ä¹Ÿè®¸', content))
        if vague_phrases == 0:
            score += 0.3
        elif vague_phrases <= 2:
            score += 0.1
        
        if vague_phrases > 3:
            suggestions.append("å‡å°‘æ¨¡ç³Šè¡¨è¿°ï¼Œæä¾›æ›´ç¡®åˆ‡çš„ä¿¡æ¯")
        
        if not has_references:
            suggestions.append("æ·»åŠ å¼•ç”¨å’Œå‚è€ƒèµ„æ–™ä»¥æé«˜å¯ä¿¡åº¦")
        
        return QualityMetric(
            name="å‡†ç¡®æ€§",
            score=score,
            max_score=1.0,
            description="å†…å®¹çš„å‡†ç¡®æ€§å’Œå¯ä¿¡åº¦",
            suggestions=suggestions
        )
    
    def _check_completeness(self, content: str, topic: str) -> QualityMetric:
        """æ£€æŸ¥å®Œæ•´æ€§"""
        score = 0.0
        suggestions = []
        
        # æ£€æŸ¥å†…å®¹é•¿åº¦
        if len(content) > 500:
            score += 0.2
        elif len(content) > 200:
            score += 0.1
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å®šä¹‰éƒ¨åˆ†
        has_definition = bool(re.search(r'å®šä¹‰|æ˜¯æŒ‡|æ¦‚å¿µ|å«ä¹‰', content))
        if has_definition:
            score += 0.2
        
        # æ£€æŸ¥æ˜¯å¦æœ‰åº”ç”¨æˆ–æ¡ˆä¾‹
        has_applications = bool(re.search(r'åº”ç”¨|æ¡ˆä¾‹|å®ä¾‹|ä½¿ç”¨', content))
        if has_applications:
            score += 0.2
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æŠ€æœ¯ç»†èŠ‚
        has_technical = bool(re.search(r'æŠ€æœ¯|æ–¹æ³•|å®ç°|åŸç†', content))
        if has_technical:
            score += 0.2
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ€»ç»“
        has_summary = bool(re.search(r'æ€»ç»“|ç»“è®º|æ€»ä¹‹|ç»¼ä¸Š', content))
        if has_summary:
            score += 0.2
        
        if not has_definition:
            suggestions.append("æ·»åŠ æ ¸å¿ƒæ¦‚å¿µçš„å®šä¹‰")
        
        if not has_applications:
            suggestions.append("è¡¥å……å®é™…åº”ç”¨æ¡ˆä¾‹")
        
        if not has_technical and len(content) > 300:
            suggestions.append("æ·»åŠ æŠ€æœ¯ç»†èŠ‚æˆ–å®ç°æ–¹æ³•")
        
        return QualityMetric(
            name="å®Œæ•´æ€§",
            score=score,
            max_score=1.0,
            description="å†…å®¹çš„å®Œæ•´æ€§å’Œè¦†ç›–é¢",
            suggestions=suggestions
        )
    
    def _check_clarity(self, content: str) -> QualityMetric:
        """æ£€æŸ¥æ¸…æ™°åº¦"""
        score = 0.0
        suggestions = []
        
        # æ£€æŸ¥å¥å­é•¿åº¦
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', content)
        long_sentences = sum(1 for s in sentences if len(s) > 50)
        if long_sentences / max(len(sentences), 1) < 0.3:
            score += 0.3
        
        # æ£€æŸ¥æ®µè½ç»“æ„
        paragraphs = content.split('\n\n')
        if len(paragraphs) > 1:
            score += 0.2
        
        # æ£€æŸ¥ä¸“ä¸šæœ¯è¯­è§£é‡Š
        technical_terms = len(re.findall(r'[A-Z]{2,}|ä¸“ä¸š|æœ¯è¯­', content))
        if technical_terms > 0:
            # å‡è®¾æœ‰è§£é‡Šï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤æ‚çš„é€»è¾‘ï¼‰
            score += 0.2
        
        # æ£€æŸ¥é€»è¾‘è¿æ¥è¯
        connectors = len(re.findall(r'å› æ­¤|ç„¶è€Œ|æ‰€ä»¥|é¦–å…ˆ|å…¶æ¬¡|æœ€å', content))
        if connectors > 0:
            score += 0.3
        
        if long_sentences / max(len(sentences), 1) > 0.5:
            suggestions.append("ç¼©çŸ­è¿‡é•¿çš„å¥å­ä»¥æé«˜å¯è¯»æ€§")
        
        if len(paragraphs) == 1 and len(content) > 200:
            suggestions.append("å°†é•¿æ–‡æœ¬åˆ†æ®µä»¥æé«˜å¯è¯»æ€§")
        
        return QualityMetric(
            name="æ¸…æ™°åº¦",
            score=score,
            max_score=1.0,
            description="å†…å®¹çš„æ¸…æ™°åº¦å’Œå¯è¯»æ€§",
            suggestions=suggestions
        )
    
    def _check_structure(self, content: str) -> QualityMetric:
        """æ£€æŸ¥ç»“æ„"""
        score = 0.0
        suggestions = []
        
        # æ£€æŸ¥æ ‡é¢˜ç»“æ„
        headers = len(re.findall(r'^#+\s', content, re.MULTILINE))
        if headers > 0:
            score += 0.3
        
        # æ£€æŸ¥åˆ—è¡¨ç»“æ„
        lists = len(re.findall(r'^\s*[-*+]\s|^\s*\d+\.', content, re.MULTILINE))
        if lists > 0:
            score += 0.3
        
        # æ£€æŸ¥å±‚æ¬¡ç»“æ„
        if headers > 1:
            score += 0.2
        
        # æ£€æŸ¥å¼€å¤´å’Œç»“å°¾
        if content.startswith('#') or re.search(r'å¼•è¨€|æ¦‚è¿°|èƒŒæ™¯', content[:100]):
            score += 0.1
        
        if re.search(r'æ€»ç»“|ç»“è®º|å‚è€ƒ', content[-100:]):
            score += 0.1
        
        if headers == 0 and len(content) > 200:
            suggestions.append("æ·»åŠ æ ‡é¢˜å’Œå­æ ‡é¢˜æ¥ç»„ç»‡å†…å®¹")
        
        if lists == 0 and len(content) > 300:
            suggestions.append("ä½¿ç”¨åˆ—è¡¨æ¥å‘ˆç°è¦ç‚¹")
        
        return QualityMetric(
            name="ç»“æ„æ€§",
            score=score,
            max_score=1.0,
            description="å†…å®¹çš„ç»„ç»‡ç»“æ„å’Œå±‚æ¬¡",
            suggestions=suggestions
        )
    
    def _check_references(self, content: str) -> QualityMetric:
        """æ£€æŸ¥å‚è€ƒæ–‡çŒ®"""
        score = 0.0
        suggestions = []
        
        # æ£€æŸ¥é“¾æ¥
        links = len(re.findall(r'http[s]?://\S+', content))
        if links > 0:
            score += 0.4
        
        # æ£€æŸ¥å¼•ç”¨æ ¼å¼
        citations = len(re.findall(r'\[\d+\]|\([^)]+\d{4}\)', content))
        if citations > 0:
            score += 0.3
        
        # æ£€æŸ¥å‚è€ƒéƒ¨åˆ†
        has_reference_section = bool(re.search(r'å‚è€ƒ|å¼•ç”¨|æ–‡çŒ®|å‚è€ƒèµ„æ–™', content[-200:]))
        if has_reference_section:
            score += 0.3
        
        if links == 0 and len(content) > 200:
            suggestions.append("æ·»åŠ ç›¸å…³çš„å¤–éƒ¨é“¾æ¥")
        
        if citations == 0 and len(content) > 300:
            suggestions.append("æ·»åŠ é€‚å½“çš„å¼•ç”¨å’Œå‚è€ƒæ–‡çŒ®")
        
        return QualityMetric(
            name="å‚è€ƒæ–‡çŒ®",
            score=score,
            max_score=1.0,
            description="å¼•ç”¨å’Œå‚è€ƒèµ„æ–™çš„è´¨é‡",
            suggestions=suggestions
        )
    
    def _check_objectivity(self, content: str) -> QualityMetric:
        """æ£€æŸ¥å®¢è§‚æ€§"""
        score = 0.0
        suggestions = []
        
        # æ£€æŸ¥ä¸»è§‚è¡¨è¿°
        subjective_phrases = len(re.findall(r'æˆ‘è®¤ä¸º|åœ¨æˆ‘çœ‹æ¥|æ˜¾ç„¶|æ¯«æ— ç–‘é—®', content))
        if subjective_phrases == 0:
            score += 0.4
        elif subjective_phrases <= 2:
            score += 0.2
        
        # æ£€æŸ¥å¤šè§’åº¦å‘ˆç°
        perspective_markers = len(re.search(r'å¦ä¸€æ–¹é¢|åŒæ—¶|ç„¶è€Œ|ä».*è§’åº¦|ä¸åŒè§‚ç‚¹', content))
        if perspective_markers > 0:
            score += 0.3
        
        # æ£€æŸ¥å¹³è¡¡è¡¨è¿°
        balanced_phrases = len(re.search(r'æ—¢æœ‰.*ä¹Ÿæœ‰|ä¸ä»….*è€Œä¸”|ä¸€æ–¹é¢.*å¦ä¸€æ–¹é¢', content))
        if balanced_phrases > 0:
            score += 0.3
        
        if subjective_phrases > 3:
            suggestions.append("å‡å°‘ä¸»è§‚è¡¨è¿°ï¼Œä¿æŒå®¢è§‚ä¸­ç«‹")
        
        if perspective_markers == 0 and len(content) > 300:
            suggestions.append("è€ƒè™‘ä»å¤šä¸ªè§’åº¦å‘ˆç°è§‚ç‚¹")
        
        return QualityMetric(
            name="å®¢è§‚æ€§",
            score=score,
            max_score=1.0,
            description="å†…å®¹çš„å®¢è§‚æ€§å’Œä¸­ç«‹æ€§",
            suggestions=suggestions
        )
    
    def _generate_suggestions(self, metrics: Dict[str, QualityMetric]) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        suggestions = []
        
        for metric_name, metric in metrics.items():
            if metric.score < 0.6:
                suggestions.extend(metric.suggestions)
        
        # å»é‡
        return list(set(suggestions))
    
    def _identify_issues(self, metrics: Dict[str, QualityMetric]) -> List[str]:
        """è¯†åˆ«ä¸»è¦é—®é¢˜"""
        issues = []
        
        for metric_name, metric in metrics.items():
            if metric.score < 0.4:
                issues.append(f"{metric.name}éœ€è¦æ˜¾è‘—æ”¹è¿›")
            elif metric.score < 0.6:
                issues.append(f"{metric.name}æœ‰å¾…æå‡")
        
        return issues
    
    def _identify_strengths(self, metrics: Dict[str, QualityMetric]) -> List[str]:
        """è¯†åˆ«ä¼˜åŠ¿"""
        strengths = []
        
        for metric_name, metric in metrics.items():
            if metric.score >= 0.8:
                strengths.append(f"{metric.name}è¡¨ç°ä¼˜ç§€")
            elif metric.score >= 0.6:
                strengths.append(f"{metric.name}è¡¨ç°è‰¯å¥½")
        
        return strengths
    
    def generate_report(self, results: Dict[str, Any]) -> str:
        """ç”Ÿæˆè´¨é‡æŠ¥å‘Š"""
        report = []
        report.append("# Wikiå†…å®¹è´¨é‡æŠ¥å‘Š\n")
        
        # æ€»ä½“è¯„åˆ†
        overall_score = results['overall_score']
        report.append(f"## æ€»ä½“è¯„åˆ†: {overall_score:.2f}/1.00\n")
        
        if overall_score >= 0.8:
            report.append("è¯„ä»·: ä¼˜ç§€ âœ…")
        elif overall_score >= 0.6:
            report.append("è¯„ä»·: è‰¯å¥½ ğŸ‘")
        elif overall_score >= 0.4:
            report.append("è¯„ä»·: ä¸€èˆ¬ âš ï¸")
        else:
            report.append("è¯„ä»·: éœ€è¦æ”¹è¿› âŒ")
        
        report.append("")
        
        # è¯¦ç»†æŒ‡æ ‡
        report.append("## è¯¦ç»†æŒ‡æ ‡\n")
        for metric_name, metric in results['metrics'].items():
            report.append(f"### {metric.name}: {metric.score:.2f}/1.00")
            
            # è¿›åº¦æ¡
            bar_length = 20
            filled_length = int(metric.score * bar_length)
            bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
            report.append(f"`{bar}` {metric.score * 100:.0f}%")
            
            if metric.suggestions:
                report.append("**å»ºè®®:**")
                for suggestion in metric.suggestions:
                    report.append(f"- {suggestion}")
            
            report.append("")
        
        # ä¼˜åŠ¿æ€»ç»“
        if results['strengths']:
            report.append("## å†…å®¹ä¼˜åŠ¿ âœ¨\n")
            for strength in results['strengths']:
                report.append(f"- {strength}")
            report.append("")
        
        # é—®é¢˜æ€»ç»“
        if results['issues']:
            report.append("## ä¸»è¦é—®é¢˜ âš ï¸\n")
            for issue in results['issues']:
                report.append(f"- {issue}")
            report.append("")
        
        # æ”¹è¿›å»ºè®®
        if results['suggestions']:
            report.append("## æ”¹è¿›å»ºè®® ğŸ’¡\n")
            for suggestion in results['suggestions']:
                report.append(f"- {suggestion}")
            report.append("")
        
        return "\n".join(report)

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python quality_checker.py --content 'å†…å®¹æ–‡æœ¬'")
        print("       python quality_checker.py --file 'æ–‡ä»¶è·¯å¾„'")
        print("       python quality_checker.py --topic 'ä¸»é¢˜' --file 'æ–‡ä»¶è·¯å¾„'")
        sys.exit(1)
    
    checker = QualityChecker()
    
    # è§£æå‚æ•°
    content = ""
    topic = ""
    
    if sys.argv[1] == "--content":
        content = sys.argv[2]
    elif sys.argv[1] == "--file":
        filename = sys.argv[2]
        with open(filename, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸»é¢˜å‚æ•°
        if len(sys.argv) > 3 and sys.argv[3] == "--topic":
            topic = sys.argv[4]
    
    # æ‰§è¡Œè´¨é‡æ£€æŸ¥
    results = checker.check_content(content, topic)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = checker.generate_report(results)
    
    # è¾“å‡ºæŠ¥å‘Š
    print(report)
    
    # ä¿å­˜æŠ¥å‘Š
    with open("quality_report.md", 'w', encoding='utf-8') as f:
        f.write(report)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    with open("quality_results.json", 'w', encoding='utf-8') as f:
        # è½¬æ¢QualityMetricå¯¹è±¡ä¸ºå­—å…¸
        serializable_results = {
            'overall_score': results['overall_score'],
            'metrics': {
                name: {
                    'name': metric.name,
                    'score': metric.score,
                    'max_score': metric.max_score,
                    'description': metric.description,
                    'suggestions': metric.suggestions
                }
                for name, metric in results['metrics'].items()
            },
            'suggestions': results['suggestions'],
            'issues': results['issues'],
            'strengths': results['strengths']
        }
        json.dump(serializable_results, f, ensure_ascii=False, indent=2)
    
    print("\nè´¨é‡æŠ¥å‘Šå·²ä¿å­˜åˆ° quality_report.md")
    print("è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ° quality_results.json")

if __name__ == "__main__":
    main()